{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf88d83",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038def5e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Configuration\n",
    "import yaml\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29232b64",
   "metadata": {},
   "source": [
    "## 2. Setup Paths and Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b2f6f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Project paths\n",
    "project_root = Path(r'd:\\Projects\\AI-Virtual-TryOn')\n",
    "dataset_root = project_root / 'data' / 'zalando-hd-resized'\n",
    "processed_dir = project_root / 'data' / 'processed'\n",
    "output_dir = project_root / 'outputs' / 'dataset'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“ Project Root: {project_root}\")\n",
    "print(f\"ðŸ“ Dataset Root: {dataset_root}\")\n",
    "print(f\"ðŸ“ Processed Data: {processed_dir}\")\n",
    "print(f\"ðŸ“ Output Directory: {output_dir}\")\n",
    "\n",
    "# Load configurations\n",
    "with open(processed_dir / 'preprocessing_config.yaml', 'r') as f:\n",
    "    preprocess_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "with open(project_root / 'outputs' / 'cloth_agnostic' / 'cloth_agnostic_config.json', 'r') as f:\n",
    "    ca_config = json.load(f)\n",
    "\n",
    "print(\"\\nâœ… Loaded configurations\")\n",
    "\n",
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "# Load dataset catalogs\n",
    "train_catalog = pd.read_csv(processed_dir / 'train_catalog.csv')\n",
    "val_catalog = pd.read_csv(processed_dir / 'val_catalog.csv')\n",
    "test_catalog = pd.read_csv(processed_dir / 'test_catalog.csv')\n",
    "\n",
    "print(f\"\\nðŸ“Š Dataset: {len(train_catalog)} train, {len(val_catalog)} val, {len(test_catalog)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e13baf1",
   "metadata": {},
   "source": [
    "## 3. Define Helper Functions\n",
    "\n",
    "Reuse functions from previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7158d76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_pose_keypoints(json_path: str) -> np.ndarray:\n",
    "    \"\"\"Load pose keypoints from OpenPose JSON.\"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if 'people' in data and len(data['people']) > 0:\n",
    "        pose_keypoints = data['people'][0]['pose_keypoints_2d']\n",
    "        keypoints = np.array(pose_keypoints).reshape(-1, 3)\n",
    "        return keypoints\n",
    "    else:\n",
    "        return np.zeros((25, 3))\n",
    "\n",
    "\n",
    "def create_pose_heatmaps(keypoints: np.ndarray, \n",
    "                        image_size: Tuple[int, int],\n",
    "                        sigma: float = 3.0) -> np.ndarray:\n",
    "    \"\"\"Create Gaussian heatmaps from keypoints.\"\"\"\n",
    "    h, w = image_size\n",
    "    heatmaps = np.zeros((18, h, w), dtype=np.float32)  # 18 channels\n",
    "    \n",
    "    y_grid, x_grid = np.ogrid[:h, :w]\n",
    "    \n",
    "    for i in range(18):  # Only first 18 keypoints\n",
    "        x, y, conf = keypoints[i]\n",
    "        if conf > 0.1:\n",
    "            x, y = int(x), int(y)\n",
    "            if 0 <= x < w and 0 <= y < h:\n",
    "                heatmap = np.exp(-((x_grid - x)**2 + (y_grid - y)**2) / (2 * sigma**2))\n",
    "                heatmaps[i] = heatmap * conf\n",
    "    \n",
    "    return heatmaps\n",
    "\n",
    "\n",
    "def parsing_to_onehot(parsing: np.ndarray, num_classes: int = 20) -> np.ndarray:\n",
    "    \"\"\"Convert parsing mask to one-hot encoding.\"\"\"\n",
    "    h, w = parsing.shape\n",
    "    onehot = np.zeros((num_classes, h, w), dtype=np.float32)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        onehot[i][parsing == i] = 1.0\n",
    "    \n",
    "    return onehot\n",
    "\n",
    "\n",
    "def create_cloth_agnostic_mask(parsing: np.ndarray,\n",
    "                               garment_classes: List[int] = [5, 7]) -> np.ndarray:\n",
    "    \"\"\"Create cloth-agnostic mask by removing garment.\"\"\"\n",
    "    mask = np.ones_like(parsing, dtype=np.uint8)\n",
    "    \n",
    "    for cls_id in garment_classes:\n",
    "        mask[parsing == cls_id] = 0\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335e2c34",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cd540c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class VITONAugmentation:\n",
    "    \"\"\"\n",
    "    Data augmentation for virtual try-on.\n",
    "    Applies synchronized augmentation to all modalities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 is_train: bool = True,\n",
    "                 horizontal_flip_prob: float = 0.5,\n",
    "                 color_jitter_prob: float = 0.3):\n",
    "        self.is_train = is_train\n",
    "        self.horizontal_flip_prob = horizontal_flip_prob\n",
    "        self.color_jitter_prob = color_jitter_prob\n",
    "        \n",
    "        # Color jitter for RGB images only\n",
    "        self.color_jitter = T.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "            hue=0.1\n",
    "        )\n",
    "    \n",
    "    def __call__(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Apply augmentation to all data modalities.\n",
    "        \"\"\"\n",
    "        if not self.is_train:\n",
    "            return data\n",
    "        \n",
    "        # Horizontal flip (synchronized across all modalities)\n",
    "        if torch.rand(1).item() < self.horizontal_flip_prob:\n",
    "            data = self._horizontal_flip(data)\n",
    "        \n",
    "        # Color jitter (only for RGB images)\n",
    "        if torch.rand(1).item() < self.color_jitter_prob:\n",
    "            data = self._apply_color_jitter(data)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _horizontal_flip(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Apply horizontal flip to all modalities.\"\"\"\n",
    "        # Flip image tensors\n",
    "        for key in ['person_image', 'cloth_image', 'cloth_agnostic', \n",
    "                    'pose_map', 'parsing_onehot', 'ca_mask']:\n",
    "            if key in data:\n",
    "                data[key] = TF.hflip(data[key])\n",
    "        \n",
    "        # Swap left/right keypoints if needed\n",
    "        # This would require more complex logic for pose keypoints\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _apply_color_jitter(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Apply color jitter to RGB images only.\"\"\"\n",
    "        for key in ['person_image', 'cloth_image']:\n",
    "            if key in data:\n",
    "                data[key] = self.color_jitter(data[key])\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "print(\"âœ… Augmentation pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34821175",
   "metadata": {},
   "source": [
    "## 5. VITONDataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70414f54",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class VITONDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Virtual Try-On (VITON-HD).\n",
    "    \n",
    "    Returns:\n",
    "    - person_image: [3, H, W] normalized RGB\n",
    "    - cloth_image: [3, H, W] normalized RGB\n",
    "    - cloth_agnostic: [3, H, W] normalized RGB (with garment removed)\n",
    "    - pose_map: [18, H, W] pose heatmaps\n",
    "    - parsing_onehot: [20, H, W] parsing one-hot\n",
    "    - ca_mask: [1, H, W] cloth-agnostic mask\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 catalog: pd.DataFrame,\n",
    "                 image_size: Tuple[int, int] = (1024, 768),\n",
    "                 is_train: bool = True,\n",
    "                 augmentation: Optional[VITONAugmentation] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            catalog: DataFrame with file paths\n",
    "            image_size: (height, width)\n",
    "            is_train: Whether this is training set\n",
    "            augmentation: Augmentation pipeline\n",
    "        \"\"\"\n",
    "        self.catalog = catalog.reset_index(drop=True)\n",
    "        self.image_size = image_size\n",
    "        self.is_train = is_train\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "        # Normalization parameters\n",
    "        self.mean = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1)\n",
    "        self.std = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1)\n",
    "        \n",
    "        # Garment classes to remove for cloth-agnostic\n",
    "        self.upper_body_classes = [5, 7]  # Upper-clothes, Coat\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.catalog)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Load and preprocess a single sample.\n",
    "        \"\"\"\n",
    "        row = self.catalog.iloc[idx]\n",
    "        \n",
    "        # Load images\n",
    "        person_img = np.array(Image.open(row['person_image']))\n",
    "        cloth_img = np.array(Image.open(row['cloth_image']))\n",
    "        parsing = np.array(Image.open(row['parse_mask']))\n",
    "        keypoints = load_pose_keypoints(row['pose_json'])\n",
    "        \n",
    "        # Create cloth-agnostic RGB\n",
    "        ca_mask = create_cloth_agnostic_mask(parsing, self.upper_body_classes)\n",
    "        ca_rgb = person_img.copy()\n",
    "        ca_rgb[ca_mask == 0] = 128  # Gray fill\n",
    "        \n",
    "        # Create pose heatmaps\n",
    "        pose_heatmaps = create_pose_heatmaps(keypoints, self.image_size)\n",
    "        \n",
    "        # Create parsing one-hot\n",
    "        parsing_onehot = parsing_to_onehot(parsing, num_classes=20)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        person_tensor = torch.from_numpy(person_img).permute(2, 0, 1).float() / 255.0\n",
    "        cloth_tensor = torch.from_numpy(cloth_img).permute(2, 0, 1).float() / 255.0\n",
    "        ca_tensor = torch.from_numpy(ca_rgb).permute(2, 0, 1).float() / 255.0\n",
    "        pose_tensor = torch.from_numpy(pose_heatmaps).float()\n",
    "        parsing_tensor = torch.from_numpy(parsing_onehot).float()\n",
    "        ca_mask_tensor = torch.from_numpy(ca_mask).unsqueeze(0).float()\n",
    "        \n",
    "        # Normalize RGB images to [-1, 1]\n",
    "        person_tensor = (person_tensor - self.mean) / self.std\n",
    "        cloth_tensor = (cloth_tensor - self.mean) / self.std\n",
    "        ca_tensor = (ca_tensor - self.mean) / self.std\n",
    "        \n",
    "        # Create data dictionary\n",
    "        data = {\n",
    "            'person_image': person_tensor,        # [3, H, W]\n",
    "            'cloth_image': cloth_tensor,          # [3, H, W]\n",
    "            'cloth_agnostic': ca_tensor,          # [3, H, W]\n",
    "            'pose_map': pose_tensor,              # [18, H, W]\n",
    "            'parsing_onehot': parsing_tensor,     # [20, H, W]\n",
    "            'ca_mask': ca_mask_tensor,            # [1, H, W]\n",
    "            'id': row['id']\n",
    "        }\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if self.augmentation is not None:\n",
    "            data = self.augmentation(data)\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "print(\"âœ… VITONDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e1955",
   "metadata": {},
   "source": [
    "## 6. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e667e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create augmentation pipelines\n",
    "train_augmentation = VITONAugmentation(\n",
    "    is_train=True,\n",
    "    horizontal_flip_prob=0.5,\n",
    "    color_jitter_prob=0.3\n",
    ")\n",
    "\n",
    "val_augmentation = VITONAugmentation(\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VITONDataset(\n",
    "    catalog=train_catalog,\n",
    "    image_size=(1024, 768),\n",
    "    is_train=True,\n",
    "    augmentation=train_augmentation\n",
    ")\n",
    "\n",
    "val_dataset = VITONDataset(\n",
    "    catalog=val_catalog,\n",
    "    image_size=(1024, 768),\n",
    "    is_train=False,\n",
    "    augmentation=val_augmentation\n",
    ")\n",
    "\n",
    "test_dataset = VITONDataset(\n",
    "    catalog=test_catalog,\n",
    "    image_size=(1024, 768),\n",
    "    is_train=False,\n",
    "    augmentation=val_augmentation\n",
    ")\n",
    "\n",
    "print(\"âœ… Datasets created\")\n",
    "print(f\"   Train: {len(train_dataset):,} samples\")\n",
    "print(f\"   Val: {len(val_dataset):,} samples\")\n",
    "print(f\"   Test: {len(test_dataset):,} samples\")\n",
    "\n",
    "# Create DataLoaders\n",
    "# Note: num_workers=0 for Windows compatibility\n",
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Windows compatibility\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… DataLoaders created\")\n",
    "print(f\"   Train: {len(train_loader):,} batches\")\n",
    "print(f\"   Val: {len(val_loader):,} batches\")\n",
    "print(f\"   Test: {len(test_loader):,} batches\")\n",
    "print(f\"   Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e3b8a",
   "metadata": {},
   "source": [
    "## 7. Test Single Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abff697",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test loading a single sample\n",
    "sample = train_dataset[0]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ” SINGLE SAMPLE TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“Š Sample ID: {sample['id']}\")\n",
    "print(f\"\\nðŸ“Š Data shapes:\")\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"   {key:25s}: {tuple(value.shape)}\")\n",
    "        print(f\"      dtype: {value.dtype}, range: [{value.min():.3f}, {value.max():.3f}]\")\n",
    "\n",
    "# Calculate total channels\n",
    "total_channels = (\n",
    "    sample['cloth_agnostic'].shape[0] +  # 3\n",
    "    sample['pose_map'].shape[0] +         # 18\n",
    "    sample['parsing_onehot'].shape[0]     # 20\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Total model input channels: {total_channels}\")\n",
    "print(f\"   - Cloth-agnostic RGB: {sample['cloth_agnostic'].shape[0]}\")\n",
    "print(f\"   - Pose map: {sample['pose_map'].shape[0]}\")\n",
    "print(f\"   - Parsing one-hot: {sample['parsing_onehot'].shape[0]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nâœ… Single sample loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af93a269",
   "metadata": {},
   "source": [
    "## 8. Test Batch Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd57784",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load a batch\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“¦ BATCH LOADING TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(f\"\\nðŸ“Š Batch shapes:\")\n",
    "for key, value in batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"   {key:25s}: {tuple(value.shape)}\")\n",
    "        print(f\"      dtype: {value.dtype}, device: {value.device}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Memory usage:\")\n",
    "total_memory = 0\n",
    "for key, value in batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        mem = value.element_size() * value.nelement() / 1024**2  # MB\n",
    "        total_memory += mem\n",
    "        print(f\"   {key:25s}: {mem:.2f} MB\")\n",
    "\n",
    "print(f\"   {'Total':25s}: {total_memory:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nâœ… Batch loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e91118d",
   "metadata": {},
   "source": [
    "## 9. Visualize Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d94d4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def denormalize(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Denormalize from [-1, 1] to [0, 1].\"\"\"\n",
    "    return tensor * 0.5 + 0.5\n",
    "\n",
    "\n",
    "# Visualize batch\n",
    "fig, axes = plt.subplots(4, 5, figsize=(20, 16))\n",
    "\n",
    "for i in range(4):  # Show 4 samples from batch\n",
    "    # Person image\n",
    "    person = denormalize(batch['person_image'][i]).permute(1, 2, 0).cpu().numpy()\n",
    "    axes[i, 0].imshow(np.clip(person, 0, 1))\n",
    "    axes[i, 0].set_title(f\"Person\\n{batch['id'][i]}\" if i == 0 else batch['id'][i], fontsize=10)\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Cloth image\n",
    "    cloth = denormalize(batch['cloth_image'][i]).permute(1, 2, 0).cpu().numpy()\n",
    "    axes[i, 1].imshow(np.clip(cloth, 0, 1))\n",
    "    axes[i, 1].set_title('Cloth' if i == 0 else '', fontsize=10)\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Cloth-agnostic\n",
    "    ca = denormalize(batch['cloth_agnostic'][i]).permute(1, 2, 0).cpu().numpy()\n",
    "    axes[i, 2].imshow(np.clip(ca, 0, 1))\n",
    "    axes[i, 2].set_title('Cloth-Agnostic' if i == 0 else '', fontsize=10)\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    # Pose map (max across channels)\n",
    "    pose = torch.max(batch['pose_map'][i], dim=0)[0].cpu().numpy()\n",
    "    axes[i, 3].imshow(pose, cmap='hot')\n",
    "    axes[i, 3].set_title('Pose Map' if i == 0 else '', fontsize=10)\n",
    "    axes[i, 3].axis('off')\n",
    "    \n",
    "    # Parsing (argmax to get class labels)\n",
    "    parsing = torch.argmax(batch['parsing_onehot'][i], dim=0).cpu().numpy()\n",
    "    axes[i, 4].imshow(parsing, cmap='tab20')\n",
    "    axes[i, 4].set_title('Parsing' if i == 0 else '', fontsize=10)\n",
    "    axes[i, 4].axis('off')\n",
    "\n",
    "plt.suptitle(f'Batch Visualization (Batch size: {batch_size})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'batch_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Batch visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876adbe",
   "metadata": {},
   "source": [
    "## 10. Timing Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f337dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"â±ï¸ DATALOADER TIMING BENCHMARK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Benchmark loading speed\n",
    "n_batches = 20\n",
    "times = []\n",
    "\n",
    "print(f\"\\nðŸ”„ Loading {n_batches} batches...\\n\")\n",
    "\n",
    "for i, batch in enumerate(tqdm(train_loader, total=n_batches, desc=\"Timing\")):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Simulate moving to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        for key in batch:\n",
    "            if isinstance(batch[key], torch.Tensor):\n",
    "                batch[key] = batch[key].to(device)\n",
    "    \n",
    "    times.append(time.time() - start)\n",
    "    \n",
    "    if i >= n_batches - 1:\n",
    "        break\n",
    "\n",
    "times = np.array(times)\n",
    "\n",
    "print(f\"\\nðŸ“Š Timing Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   Mean time per batch: {times.mean()*1000:.2f} ms\")\n",
    "print(f\"   Std time per batch: {times.std()*1000:.2f} ms\")\n",
    "print(f\"   Min time per batch: {times.min()*1000:.2f} ms\")\n",
    "print(f\"   Max time per batch: {times.max()*1000:.2f} ms\")\n",
    "\n",
    "# Calculate samples per second\n",
    "samples_per_second = batch_size / times.mean()\n",
    "print(f\"\\nâš¡ Throughput: {samples_per_second:.2f} samples/second\")\n",
    "\n",
    "# Estimate epoch time\n",
    "epoch_time = len(train_loader) * times.mean() / 60  # minutes\n",
    "print(f\"ðŸ“Š Estimated epoch time: {epoch_time:.2f} minutes\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nâœ… Timing benchmark complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f087e9dd",
   "metadata": {},
   "source": [
    "## 11. Memory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e2ef82",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ’¾ MEMORY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Single sample memory\n",
    "sample = train_dataset[0]\n",
    "sample_memory = 0\n",
    "\n",
    "print(f\"\\nðŸ“Š Single Sample Memory:\")\n",
    "print(\"-\" * 50)\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        mem = value.element_size() * value.nelement() / 1024**2\n",
    "        sample_memory += mem\n",
    "        print(f\"   {key:25s}: {mem:.2f} MB\")\n",
    "\n",
    "print(f\"   {'Total':25s}: {sample_memory:.2f} MB\")\n",
    "\n",
    "# Batch memory\n",
    "batch = next(iter(train_loader))\n",
    "batch_memory = 0\n",
    "\n",
    "print(f\"\\nðŸ“Š Batch Memory (batch_size={batch_size}):\")\n",
    "print(\"-\" * 50)\n",
    "for key, value in batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        mem = value.element_size() * value.nelement() / 1024**2\n",
    "        batch_memory += mem\n",
    "        print(f\"   {key:25s}: {mem:.2f} MB\")\n",
    "\n",
    "print(f\"   {'Total':25s}: {batch_memory:.2f} MB\")\n",
    "\n",
    "# GPU memory if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Move batch to GPU\n",
    "    for key in batch:\n",
    "        if isinstance(batch[key], torch.Tensor):\n",
    "            batch[key] = batch[key].to(device)\n",
    "    \n",
    "    gpu_memory = torch.cuda.memory_allocated() / 1024**2\n",
    "    gpu_reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "    \n",
    "    print(f\"\\nðŸ“Š GPU Memory:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"   Allocated: {gpu_memory:.2f} MB\")\n",
    "    print(f\"   Reserved: {gpu_reserved:.2f} MB\")\n",
    "    print(f\"   Total GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nâœ… Memory analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29868933",
   "metadata": {},
   "source": [
    "## 12. Save Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672eb7c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create dataset configuration\n",
    "dataset_config = {\n",
    "    'dataset': {\n",
    "        'train_samples': len(train_dataset),\n",
    "        'val_samples': len(val_dataset),\n",
    "        'test_samples': len(test_dataset),\n",
    "        'total_samples': len(train_dataset) + len(val_dataset) + len(test_dataset)\n",
    "    },\n",
    "    'dataloader': {\n",
    "        'batch_size': batch_size,\n",
    "        'num_workers': 0,\n",
    "        'pin_memory': torch.cuda.is_available(),\n",
    "        'train_batches': len(train_loader),\n",
    "        'val_batches': len(val_loader),\n",
    "        'test_batches': len(test_loader)\n",
    "    },\n",
    "    'data_format': {\n",
    "        'image_size': list(train_dataset.image_size),\n",
    "        'channels': {\n",
    "            'person_image': 3,\n",
    "            'cloth_image': 3,\n",
    "            'cloth_agnostic': 3,\n",
    "            'pose_map': 18,\n",
    "            'parsing_onehot': 20,\n",
    "            'ca_mask': 1,\n",
    "            'total_input': 41\n",
    "        },\n",
    "        'normalization': {\n",
    "            'mean': [0.5, 0.5, 0.5],\n",
    "            'std': [0.5, 0.5, 0.5],\n",
    "            'range': [-1, 1]\n",
    "        }\n",
    "    },\n",
    "    'augmentation': {\n",
    "        'horizontal_flip_prob': 0.5,\n",
    "        'color_jitter_prob': 0.3\n",
    "    },\n",
    "    'performance': {\n",
    "        'mean_batch_time_ms': float(times.mean() * 1000),\n",
    "        'throughput_samples_per_sec': float(samples_per_second),\n",
    "        'estimated_epoch_time_min': float(epoch_time),\n",
    "        'single_sample_memory_mb': float(sample_memory),\n",
    "        'batch_memory_mb': float(batch_memory)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = output_dir / 'dataset_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(dataset_config, f, indent=2)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ’¾ DATASET CONFIGURATION SAVED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“„ Config saved to: {config_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"   - Total samples: {dataset_config['dataset']['total_samples']:,}\")\n",
    "print(f\"   - Batch size: {dataset_config['dataloader']['batch_size']}\")\n",
    "print(f\"   - Input channels: {dataset_config['data_format']['channels']['total_input']}\")\n",
    "print(f\"   - Throughput: {dataset_config['performance']['throughput_samples_per_sec']:.2f} samples/sec\")\n",
    "print(f\"   - Epoch time: {dataset_config['performance']['estimated_epoch_time_min']:.2f} min\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d9f505",
   "metadata": {},
   "source": [
    "## 13. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc2118",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸŽ‰ PYTORCH DATASET & DATALOADER COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nâœ… Completed Tasks:\")\n",
    "print(\"   1. âœ“ Implemented VITONDataset class\")\n",
    "print(\"   2. âœ“ Created data augmentation pipeline\")\n",
    "print(\"   3. âœ“ Built training, validation, and test DataLoaders\")\n",
    "print(\"   4. âœ“ Tested single sample and batch loading\")\n",
    "print(\"   5. âœ“ Benchmarked loading performance\")\n",
    "print(\"   6. âœ“ Analyzed memory usage\")\n",
    "print(\"   7. âœ“ Saved dataset configuration\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Key Metrics:\")\n",
    "print(f\"   - Train samples: {len(train_dataset):,}\")\n",
    "print(f\"   - Val samples: {len(val_dataset):,}\")\n",
    "print(f\"   - Test samples: {len(test_dataset):,}\")\n",
    "print(f\"   - Batch size: {batch_size}\")\n",
    "print(f\"   - Input channels: 41 (3 + 18 + 20)\")\n",
    "print(f\"   - Throughput: {samples_per_second:.2f} samples/sec\")\n",
    "print(f\"   - Epoch time: {epoch_time:.2f} minutes\")\n",
    "\n",
    "print(f\"\\nðŸ“ Generated Files:\")\n",
    "print(f\"   - batch_visualization.png\")\n",
    "print(f\"   - dataset_config.json\")\n",
    "\n",
    "print(\"\\nðŸš€ Ready for Next Steps:\")\n",
    "print(\"   1. Begin model architecture development\")\n",
    "print(\"   2. Implement Generator network\")\n",
    "print(\"   3. Implement Discriminator network\")\n",
    "print(\"   4. Create training loop with losses\")\n",
    "print(\"   5. Start model training\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Components Created:\")\n",
    "print(\"   - VITONDataset: Complete PyTorch Dataset\")\n",
    "print(\"     * Load all modalities (person, cloth, parsing, pose)\")\n",
    "print(\"     * Generate cloth-agnostic representation\")\n",
    "print(\"     * Create multi-channel input (41 channels)\")\n",
    "print(\"     * Support data augmentation\")\n",
    "print(\"   - VITONAugmentation: Synchronized augmentation\")\n",
    "print(\"     * Horizontal flip (50% probability)\")\n",
    "print(\"     * Color jitter (30% probability)\")\n",
    "print(\"   - DataLoaders: Optimized batch loading\")\n",
    "print(\"     * Windows compatible (num_workers=0)\")\n",
    "print(\"     * GPU memory pinning enabled\")\n",
    "print(\"     * Efficient batch processing\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"\\nâœ… DATASET MODULE READY FOR TRAINING!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea01bd8",
   "metadata": {},
   "source": [
    "# Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48a437",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Ensure `torchsummary` is installed and importable (fallback to `torchinfo`)\n",
    "import importlib, subprocess, sys\n",
    "\n",
    "\n",
    "def ensure_package(pkg_name: str, import_name: str = None):\n",
    "    name = import_name or pkg_name\n",
    "    try:\n",
    "        return importlib.import_module(name)\n",
    "    except Exception:\n",
    "        print(f\"âš ï¸ {name} not found â€” installing {pkg_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg_name])\n",
    "        importlib.invalidate_caches()\n",
    "        module = importlib.import_module(name)\n",
    "        print(f\"âœ… Installed and imported {name}\")\n",
    "        return module\n",
    "\n",
    "# Primary: torchsummary\n",
    "try:\n",
    "    ts = ensure_package(\"torchsummary\", \"torchsummary\")\n",
    "    from torchsummary import summary\n",
    "    print(\"âœ… `torchsummary.summary` is available\")\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ Failed to import `torchsummary` (will try fallback):\", e)\n",
    "    # Fallback: torchinfo\n",
    "    try:\n",
    "        ti = ensure_package(\"torchinfo\", \"torchinfo\")\n",
    "        from torchinfo import summary\n",
    "        print(\"âœ… Using fallback `torchinfo.summary`\")\n",
    "    except Exception as e2:\n",
    "        print(\"âŒ Failed to install fallback 'torchinfo':\", e2)\n",
    "        print(\"Please install `torchsummary` or `torchinfo` in your environment and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ca060",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376662ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torchsummary import summary\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299526b1",
   "metadata": {},
   "source": [
    "## 2. Setup Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd216912",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Project paths\n",
    "project_root = Path(r'd:\\Projects\\AI-Virtual-TryOn')\n",
    "output_dir = project_root / 'outputs' / 'model_architecture'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load dataset configuration\n",
    "with open(project_root / 'outputs' / 'dataset' / 'dataset_config.json', 'r') as f:\n",
    "    dataset_config = json.load(f)\n",
    "\n",
    "print(f\"ðŸ“ Project Root: {project_root}\")\n",
    "print(f\"ðŸ“ Output Directory: {output_dir}\")\n",
    "print(f\"\\nâœ… Loaded dataset configuration\")\n",
    "\n",
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "# Model configuration\n",
    "model_config = {\n",
    "    'input_channels': 41,  # 3 CA + 18 Pose + 20 Parsing\n",
    "    'output_channels': 3,  # RGB\n",
    "    'image_size': (1024, 768),\n",
    "    'ngf': 64,  # Generator base channels\n",
    "    'ndf': 64,  # Discriminator base channels\n",
    "    'n_downsampling': 4,\n",
    "    'n_blocks': 9,  # Residual blocks at bottleneck\n",
    "    'use_dropout': True,\n",
    "    'use_attention': True\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Configuration:\")\n",
    "for key, value in model_config.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e0c47a",
   "metadata": {},
   "source": [
    "## 3. Building Blocks\n",
    "\n",
    "Define reusable components for the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732215c5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with two convolutional layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, use_dropout: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        \n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(0.5))\n",
    "        \n",
    "        layers += [\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        ]\n",
    "        \n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention module for capturing long-range dependencies.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query = nn.Conv2d(channels, channels // 8, kernel_size=1)\n",
    "        self.key = nn.Conv2d(channels, channels // 8, kernel_size=1)\n",
    "        self.value = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch, channels, height, width = x.size()\n",
    "        \n",
    "        # Query, Key, Value\n",
    "        q = self.query(x).view(batch, -1, height * width).permute(0, 2, 1)\n",
    "        k = self.key(x).view(batch, -1, height * width)\n",
    "        v = self.value(x).view(batch, -1, height * width)\n",
    "        \n",
    "        # Attention map\n",
    "        attention = F.softmax(torch.bmm(q, k), dim=-1)\n",
    "        \n",
    "        # Apply attention\n",
    "        out = torch.bmm(v, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch, channels, height, width)\n",
    "        \n",
    "        return self.gamma * out + x\n",
    "\n",
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Downsampling block for encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, \n",
    "                 normalize: bool = True, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, \n",
    "                     stride=2, padding=1, bias=False)\n",
    "        ]\n",
    "        \n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        \n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling block for decoder with skip connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4,\n",
    "                              stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        \n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, skip: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        x = self.block(x)\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"âœ… Building blocks defined:\")\n",
    "print(\"   - ResidualBlock\")\n",
    "print(\"   - SelfAttention\")\n",
    "print(\"   - DownsampleBlock\")\n",
    "print(\"   - UpsampleBlock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e9ce6",
   "metadata": {},
   "source": [
    "## 4. Generator Network\n",
    "\n",
    "U-Net architecture with attention and residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea3d54",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net based Generator for Virtual Try-On.\n",
    "    \n",
    "    Input: [B, 41, H, W] (cloth-agnostic + pose + parsing)\n",
    "    Output: [B, 3, H, W] (RGB image)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels: int = 41,\n",
    "                 out_channels: int = 3,\n",
    "                 ngf: int = 64,\n",
    "                 n_downsampling: int = 4,\n",
    "                 n_blocks: int = 9,\n",
    "                 use_dropout: bool = True,\n",
    "                 use_attention: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_downsampling = n_downsampling\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ngf, kernel_size=7, padding=3, bias=False),\n",
    "            nn.InstanceNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.encoder = nn.ModuleList()\n",
    "        mult = 1\n",
    "        for i in range(n_downsampling):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** (i + 1), 8)\n",
    "            self.encoder.append(\n",
    "                DownsampleBlock(\n",
    "                    ngf * mult_prev,\n",
    "                    ngf * mult,\n",
    "                    normalize=True,\n",
    "                    dropout=0.0\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Bottleneck (residual blocks)\n",
    "        bottleneck_channels = ngf * mult\n",
    "        self.bottleneck = nn.ModuleList()\n",
    "        for _ in range(n_blocks):\n",
    "            self.bottleneck.append(\n",
    "                ResidualBlock(bottleneck_channels, use_dropout=use_dropout)\n",
    "            )\n",
    "        \n",
    "        # Self-attention at bottleneck\n",
    "        self.attention = SelfAttention(bottleneck_channels) if use_attention else None\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(n_downsampling):\n",
    "            mult_prev = min(2 ** (n_downsampling - i), 8)\n",
    "            mult = min(2 ** (n_downsampling - i - 1), 8)\n",
    "            \n",
    "            # Account for skip connections (double the input channels)\n",
    "            in_ch = ngf * mult_prev\n",
    "            out_ch = ngf * mult\n",
    "            \n",
    "            self.decoder.append(\n",
    "                UpsampleBlock(\n",
    "                    in_ch * 2,  # *2 for skip connection\n",
    "                    out_ch,\n",
    "                    dropout=0.5 if i < 3 and use_dropout else 0.0\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Final convolution\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(ngf * 2, ngf, kernel_size=3, padding=1, bias=False),  # *2 for skip\n",
    "            nn.InstanceNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ngf, out_channels, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through Generator.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [B, 41, H, W]\n",
    "        \n",
    "        Returns:\n",
    "            Generated image [B, 3, H, W]\n",
    "        \"\"\"\n",
    "        # Initial convolution\n",
    "        x = self.initial(x)\n",
    "        \n",
    "        # Encoder with skip connections\n",
    "        skip_connections = [x]\n",
    "        for encoder_block in self.encoder:\n",
    "            x = encoder_block(x)\n",
    "            skip_connections.append(x)\n",
    "        \n",
    "        # Bottleneck\n",
    "        for res_block in self.bottleneck:\n",
    "            x = res_block(x)\n",
    "        \n",
    "        # Attention\n",
    "        if self.attention is not None:\n",
    "            x = self.attention(x)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        skip_connections = skip_connections[::-1]  # Reverse order\n",
    "        for i, decoder_block in enumerate(self.decoder):\n",
    "            skip = skip_connections[i]\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = decoder_block(x)\n",
    "        \n",
    "        # Final convolution with last skip connection\n",
    "        x = torch.cat([x, skip_connections[-1]], dim=1)\n",
    "        x = self.final(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"âœ… Generator network defined\")\n",
    "print(\"   Architecture: U-Net with attention\")\n",
    "print(\"   Input: [B, 41, H, W]\")\n",
    "print(\"   Output: [B, 3, H, W]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041de6b",
   "metadata": {},
   "source": [
    "## 5. Discriminator Network\n",
    "\n",
    "PatchGAN discriminator with spectral normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb537e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchGAN Discriminator for Virtual Try-On.\n",
    "    \n",
    "    Classifies 70x70 patches as real or fake.\n",
    "    Uses spectral normalization for training stability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels: int = 6,  # 3 (image) + 3 (condition)\n",
    "                 ndf: int = 64,\n",
    "                 n_layers: int = 3,\n",
    "                 use_spectral_norm: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        norm_layer = spectral_norm if use_spectral_norm else lambda x: x\n",
    "        \n",
    "        # Initial layer (no normalization)\n",
    "        layers = [\n",
    "            norm_layer(nn.Conv2d(in_channels, ndf, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        # Intermediate layers\n",
    "        mult = 1\n",
    "        for n in range(1, n_layers):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** n, 8)\n",
    "            layers += [\n",
    "                norm_layer(nn.Conv2d(ndf * mult_prev, ndf * mult, \n",
    "                                    kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "                nn.InstanceNorm2d(ndf * mult),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ]\n",
    "        \n",
    "        # Final layers\n",
    "        mult_prev = mult\n",
    "        mult = min(2 ** n_layers, 8)\n",
    "        layers += [\n",
    "            norm_layer(nn.Conv2d(ndf * mult_prev, ndf * mult,\n",
    "                                kernel_size=4, stride=1, padding=1, bias=False)),\n",
    "            nn.InstanceNorm2d(ndf * mult),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(\n",
    "            norm_layer(nn.Conv2d(ndf * mult, 1, kernel_size=4, stride=1, padding=1))\n",
    "        )\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, condition: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through Discriminator.\n",
    "        \n",
    "        Args:\n",
    "            x: Input image [B, 3, H, W]\n",
    "            condition: Condition image [B, 3, H, W] (e.g., cloth-agnostic)\n",
    "        \n",
    "        Returns:\n",
    "            Patch predictions [B, 1, H', W']\n",
    "        \"\"\"\n",
    "        # Concatenate image and condition\n",
    "        x = torch.cat([x, condition], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "print(\"âœ… Discriminator network defined\")\n",
    "print(\"   Architecture: PatchGAN with spectral normalization\")\n",
    "print(\"   Input: [B, 6, H, W] (image + condition)\")\n",
    "print(\"   Output: [B, 1, H', W'] (patch predictions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552120bf",
   "metadata": {},
   "source": [
    "## 6. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7067371",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Generator\n",
    "generator = Generator(\n",
    "    in_channels=model_config['input_channels'],\n",
    "    out_channels=model_config['output_channels'],\n",
    "    ngf=model_config['ngf'],\n",
    "    n_downsampling=model_config['n_downsampling'],\n",
    "    n_blocks=model_config['n_blocks'],\n",
    "    use_dropout=model_config['use_dropout'],\n",
    "    use_attention=model_config['use_attention']\n",
    ").to(device)\n",
    "\n",
    "# Initialize Discriminator\n",
    "discriminator = Discriminator(\n",
    "    in_channels=6,  # RGB image + RGB condition\n",
    "    ndf=model_config['ndf'],\n",
    "    n_layers=3,\n",
    "    use_spectral_norm=True\n",
    ").to(device)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸŽ¯ MODELS INITIALIZED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nâœ… Generator created and moved to {device}\")\n",
    "print(f\"âœ… Discriminator created and moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe09429",
   "metadata": {},
   "source": [
    "## 7. Count Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c0e9b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Count total and trainable parameters.\n",
    "    \"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "# Count parameters\n",
    "gen_total, gen_trainable = count_parameters(generator)\n",
    "disc_total, disc_trainable = count_parameters(discriminator)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š PARAMETER COUNT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ”· Generator:\")\n",
    "print(f\"   Total parameters: {gen_total:,}\")\n",
    "print(f\"   Trainable parameters: {gen_trainable:,}\")\n",
    "print(f\"   Memory: {gen_total * 4 / 1024**2:.2f} MB (float32)\")\n",
    "\n",
    "print(f\"\\nðŸ”¶ Discriminator:\")\n",
    "print(f\"   Total parameters: {disc_total:,}\")\n",
    "print(f\"   Trainable parameters: {disc_trainable:,}\")\n",
    "print(f\"   Memory: {disc_total * 4 / 1024**2:.2f} MB (float32)\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total:\")\n",
    "print(f\"   Combined parameters: {gen_total + disc_total:,}\")\n",
    "print(f\"   Combined memory: {(gen_total + disc_total) * 4 / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f410604",
   "metadata": {},
   "source": [
    "## 8. Test Forward Pass - Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71928d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ§ª TESTING GENERATOR FORWARD PASS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create dummy input\n",
    "batch_size = 2\n",
    "dummy_input = torch.randn(batch_size, 41, 1024, 768).to(device)\n",
    "\n",
    "print(f\"\\nðŸ“¥ Input shape: {tuple(dummy_input.shape)}\")\n",
    "print(f\"   Channels breakdown:\")\n",
    "print(f\"   - Cloth-agnostic RGB: 3\")\n",
    "print(f\"   - Pose heatmaps: 18\")\n",
    "print(f\"   - Parsing one-hot: 20\")\n",
    "print(f\"   Total: 41 channels\")\n",
    "\n",
    "# Forward pass\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    output = generator(dummy_input)\n",
    "\n",
    "print(f\"\\nðŸ“¤ Output shape: {tuple(output.shape)}\")\n",
    "print(f\"   Output range: [{output.min().item():.3f}, {output.max().item():.3f}]\")\n",
    "print(f\"   Expected range: [-1, 1] (Tanh activation)\")\n",
    "\n",
    "# Check output\n",
    "assert output.shape == (batch_size, 3, 1024, 768), \"Output shape mismatch!\"\n",
    "assert output.min() >= -1.0 and output.max() <= 1.0, \"Output range incorrect!\"\n",
    "\n",
    "print(f\"\\nâœ… Generator forward pass successful!\")\n",
    "print(f\"   Input: {tuple(dummy_input.shape)}\")\n",
    "print(f\"   Output: {tuple(output.shape)}\")\n",
    "\n",
    "# Memory usage\n",
    "if torch.cuda.is_available():\n",
    "    mem_allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "    mem_reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "    print(f\"\\nðŸ’¾ GPU Memory:\")\n",
    "    print(f\"   Allocated: {mem_allocated:.2f} MB\")\n",
    "    print(f\"   Reserved: {mem_reserved:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1b777",
   "metadata": {},
   "source": [
    "## 9. Test Forward Pass - Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06031e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ§ª TESTING DISCRIMINATOR FORWARD PASS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create dummy inputs\n",
    "dummy_image = torch.randn(batch_size, 3, 1024, 768).to(device)\n",
    "dummy_condition = torch.randn(batch_size, 3, 1024, 768).to(device)\n",
    "\n",
    "print(f\"\\nðŸ“¥ Input shapes:\")\n",
    "print(f\"   Image: {tuple(dummy_image.shape)}\")\n",
    "print(f\"   Condition: {tuple(dummy_condition.shape)}\")\n",
    "\n",
    "# Forward pass\n",
    "discriminator.eval()\n",
    "with torch.no_grad():\n",
    "    disc_output = discriminator(dummy_image, dummy_condition)\n",
    "\n",
    "print(f\"\\nðŸ“¤ Output shape: {tuple(disc_output.shape)}\")\n",
    "print(f\"   Output range: [{disc_output.min().item():.3f}, {disc_output.max().item():.3f}]\")\n",
    "\n",
    "# Calculate receptive field size\n",
    "patch_h, patch_w = disc_output.shape[2], disc_output.shape[3]\n",
    "print(f\"\\nðŸ“Š Patch predictions:\")\n",
    "print(f\"   Number of patches: {patch_h} Ã— {patch_w} = {patch_h * patch_w}\")\n",
    "print(f\"   Each patch covers ~70Ã—70 pixels\")\n",
    "\n",
    "print(f\"\\nâœ… Discriminator forward pass successful!\")\n",
    "print(f\"   Image: {tuple(dummy_image.shape)}\")\n",
    "print(f\"   Condition: {tuple(dummy_condition.shape)}\")\n",
    "print(f\"   Output: {tuple(disc_output.shape)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8294a996",
   "metadata": {},
   "source": [
    "## 10. Test Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43740e8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ”„ TESTING FULL PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create dummy input\n",
    "print(\"\\nðŸ“¥ Creating dummy inputs...\")\n",
    "multi_channel_input = torch.randn(batch_size, 41, 1024, 768).to(device)\n",
    "real_image = torch.randn(batch_size, 3, 1024, 768).to(device)\n",
    "condition = torch.randn(batch_size, 3, 1024, 768).to(device)\n",
    "\n",
    "# Generator forward pass\n",
    "print(\"\\nðŸ”· Generator: Generating fake images...\")\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    fake_image = generator(multi_channel_input)\n",
    "\n",
    "print(f\"   Input: {tuple(multi_channel_input.shape)}\")\n",
    "print(f\"   Output (fake): {tuple(fake_image.shape)}\")\n",
    "\n",
    "# Discriminator forward pass on fake\n",
    "print(\"\\nðŸ”¶ Discriminator: Evaluating fake images...\")\n",
    "discriminator.eval()\n",
    "with torch.no_grad():\n",
    "    pred_fake = discriminator(fake_image, condition)\n",
    "\n",
    "print(f\"   Fake prediction: {tuple(pred_fake.shape)}\")\n",
    "print(f\"   Mean score: {pred_fake.mean().item():.3f}\")\n",
    "\n",
    "# Discriminator forward pass on real\n",
    "print(\"\\nðŸ”¶ Discriminator: Evaluating real images...\")\n",
    "with torch.no_grad():\n",
    "    pred_real = discriminator(real_image, condition)\n",
    "\n",
    "print(f\"   Real prediction: {tuple(pred_real.shape)}\")\n",
    "print(f\"   Mean score: {pred_real.mean().item():.3f}\")\n",
    "\n",
    "print(\"\\nâœ… Full pipeline test successful!\")\n",
    "print(\"\\nðŸ“Š Pipeline Summary:\")\n",
    "print(f\"   1. Multi-channel input [{batch_size}, 41, 1024, 768]\")\n",
    "print(f\"   2. Generator â†’ Fake image [{batch_size}, 3, 1024, 768]\")\n",
    "print(f\"   3. Discriminator(fake) â†’ Patches [{batch_size}, 1, {patch_h}, {patch_w}]\")\n",
    "print(f\"   4. Discriminator(real) â†’ Patches [{batch_size}, 1, {patch_h}, {patch_w}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf330c15",
   "metadata": {},
   "source": [
    "## 11. Model Architecture Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e48c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_architecture():\n",
    "    \"\"\"\n",
    "    Create visual diagram of model architecture.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 12))\n",
    "    \n",
    "    # Generator U-Net visualization\n",
    "    ax1.set_title('Generator Architecture (U-Net)', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_xlim(0, 10)\n",
    "    ax1.set_ylim(0, 10)\n",
    "    \n",
    "    # Encoder path\n",
    "    encoder_colors = ['#e8f4f8', '#b8dfe6', '#88cad4', '#58b5c2']\n",
    "    encoder_positions = [(1, 8), (2, 7), (3, 6), (4, 5)]\n",
    "    encoder_sizes = [(0.8, 0.8), (0.7, 0.7), (0.6, 0.6), (0.5, 0.5)]\n",
    "    \n",
    "    # Input\n",
    "    ax1.add_patch(Rectangle((0.5, 8.5), 0.9, 0.9, \n",
    "                            facecolor='#c8e6c9', edgecolor='black', linewidth=2))\n",
    "    ax1.text(0.95, 9.0, 'Input\\n41ch', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Encoder blocks\n",
    "    for i, (pos, size, color) in enumerate(zip(encoder_positions, encoder_sizes, encoder_colors)):\n",
    "        channels = 64 * min(2**i, 8)\n",
    "        ax1.add_patch(Rectangle((pos[0]-size[0]/2, pos[1]-size[1]/2), size[0], size[1],\n",
    "                                facecolor=color, edgecolor='black', linewidth=2))\n",
    "        ax1.text(pos[0], pos[1], f'Down\\n{channels}ch', ha='center', va='center', \n",
    "                fontsize=9, fontweight='bold')\n",
    "        # Arrow\n",
    "        if i > 0:\n",
    "            ax1.arrow(encoder_positions[i-1][0], encoder_positions[i-1][1]-0.5,\n",
    "                     pos[0]-encoder_positions[i-1][0], pos[1]-encoder_positions[i-1][1]+0.5,\n",
    "                     head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "    \n",
    "    # Bottleneck\n",
    "    ax1.add_patch(Rectangle((4.25, 4.25), 0.5, 0.5,\n",
    "                            facecolor='#ff9999', edgecolor='black', linewidth=2))\n",
    "    ax1.text(4.5, 4.5, 'Bottleneck\\n512ch\\n9 blocks\\nAttention', \n",
    "            ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "    \n",
    "    # Decoder path\n",
    "    decoder_positions = [(6, 5), (7, 6), (8, 7), (9, 8)]\n",
    "    decoder_sizes = [(0.5, 0.5), (0.6, 0.6), (0.7, 0.7), (0.8, 0.8)]\n",
    "    \n",
    "    for i, (pos, size, color) in enumerate(zip(decoder_positions, decoder_sizes, \n",
    "                                               encoder_colors[::-1])):\n",
    "        channels = 64 * min(2**(3-i), 8)\n",
    "        ax1.add_patch(Rectangle((pos[0]-size[0]/2, pos[1]-size[1]/2), size[0], size[1],\n",
    "                                facecolor=color, edgecolor='black', linewidth=2))\n",
    "        ax1.text(pos[0], pos[1], f'Up\\n{channels}ch', ha='center', va='center',\n",
    "                fontsize=9, fontweight='bold')\n",
    "        # Arrow\n",
    "        if i > 0:\n",
    "            ax1.arrow(decoder_positions[i-1][0], decoder_positions[i-1][1]+0.5,\n",
    "                     pos[0]-decoder_positions[i-1][0], pos[1]-decoder_positions[i-1][1]-0.5,\n",
    "                     head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "    \n",
    "    # Output\n",
    "    ax1.add_patch(Rectangle((9.1, 8.5), 0.9, 0.9,\n",
    "                            facecolor='#fff9c4', edgecolor='black', linewidth=2))\n",
    "    ax1.text(9.55, 9.0, 'Output\\n3ch', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Skip connections (dashed lines)\n",
    "    for enc_pos, dec_pos in zip(encoder_positions, decoder_positions[::-1]):\n",
    "        ax1.plot([enc_pos[0], dec_pos[0]], [enc_pos[1], dec_pos[1]],\n",
    "                'r--', linewidth=1.5, alpha=0.6, label='Skip' if enc_pos == encoder_positions[0] else '')\n",
    "    \n",
    "    ax1.legend(loc='lower left', fontsize=10)\n",
    "    \n",
    "    # Discriminator visualization\n",
    "    ax2.set_title('Discriminator Architecture (PatchGAN)', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_xlim(0, 10)\n",
    "    ax2.set_ylim(0, 10)\n",
    "    \n",
    "    # Input images\n",
    "    ax2.add_patch(Rectangle((0.5, 8), 1.2, 1.5,\n",
    "                            facecolor='#c8e6c9', edgecolor='black', linewidth=2))\n",
    "    ax2.text(1.1, 8.75, 'Image\\n3ch', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax2.add_patch(Rectangle((2.3, 8), 1.2, 1.5,\n",
    "                            facecolor='#c8e6c9', edgecolor='black', linewidth=2))\n",
    "    ax2.text(2.9, 8.75, 'Condition\\n3ch', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Concatenate\n",
    "    ax2.add_patch(Rectangle((4.3, 8), 1.2, 1.5,\n",
    "                            facecolor='#ffe0b2', edgecolor='black', linewidth=2))\n",
    "    ax2.text(4.9, 8.75, 'Concat\\n6ch', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Conv layers\n",
    "    disc_colors = ['#b8dfe6', '#88cad4', '#58b5c2', '#2891a0']\n",
    "    disc_positions = [(2, 6), (4, 5), (6, 4), (8, 3)]\n",
    "    disc_channels = [64, 128, 256, 512]\n",
    "    \n",
    "    for i, (pos, color, ch) in enumerate(zip(disc_positions, disc_colors, disc_channels)):\n",
    "        size = 1.2 - i * 0.15\n",
    "        ax2.add_patch(Rectangle((pos[0]-size/2, pos[1]-size/2), size, size,\n",
    "                                facecolor=color, edgecolor='black', linewidth=2))\n",
    "        ax2.text(pos[0], pos[1], f'Conv\\n{ch}ch', ha='center', va='center',\n",
    "                fontsize=9, fontweight='bold')\n",
    "        # Arrow\n",
    "        if i > 0:\n",
    "            ax2.arrow(disc_positions[i-1][0], disc_positions[i-1][1]-0.7,\n",
    "                     pos[0]-disc_positions[i-1][0], pos[1]-disc_positions[i-1][1]+0.7,\n",
    "                     head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "    \n",
    "    # Output\n",
    "    ax2.add_patch(Rectangle((8.5, 1), 1.0, 1.0,\n",
    "                            facecolor='#fff9c4', edgecolor='black', linewidth=2))\n",
    "    ax2.text(9.0, 1.5, f'Patches\\n{patch_h}Ã—{patch_w}', \n",
    "            ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'model_architecture.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Architecture visualization saved\")\n",
    "\n",
    "\n",
    "visualize_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4652b398",
   "metadata": {},
   "source": [
    "## 12. Save Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dbddf4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create detailed model configuration\n",
    "architecture_config = {\n",
    "    'generator': {\n",
    "        'type': 'U-Net',\n",
    "        'input_channels': model_config['input_channels'],\n",
    "        'output_channels': model_config['output_channels'],\n",
    "        'base_channels': model_config['ngf'],\n",
    "        'n_downsampling': model_config['n_downsampling'],\n",
    "        'n_blocks': model_config['n_blocks'],\n",
    "        'use_dropout': model_config['use_dropout'],\n",
    "        'use_attention': model_config['use_attention'],\n",
    "        'total_parameters': gen_total,\n",
    "        'trainable_parameters': gen_trainable,\n",
    "        'memory_mb': gen_total * 4 / 1024**2\n",
    "    },\n",
    "    'discriminator': {\n",
    "        'type': 'PatchGAN',\n",
    "        'input_channels': 6,\n",
    "        'base_channels': model_config['ndf'],\n",
    "        'n_layers': 3,\n",
    "        'use_spectral_norm': True,\n",
    "        'patch_size': '70x70',\n",
    "        'output_patches': f'{patch_h}x{patch_w}',\n",
    "        'total_parameters': disc_total,\n",
    "        'trainable_parameters': disc_trainable,\n",
    "        'memory_mb': disc_total * 4 / 1024**2\n",
    "    },\n",
    "    'total': {\n",
    "        'combined_parameters': gen_total + disc_total,\n",
    "        'combined_memory_mb': (gen_total + disc_total) * 4 / 1024**2\n",
    "    },\n",
    "    'image_size': model_config['image_size'],\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = output_dir / 'model_architecture_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(architecture_config, f, indent=2)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ’¾ MODEL CONFIGURATION SAVED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“„ Config saved to: {config_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"   - Generator parameters: {gen_total:,}\")\n",
    "print(f\"   - Discriminator parameters: {disc_total:,}\")\n",
    "print(f\"   - Total parameters: {gen_total + disc_total:,}\")\n",
    "print(f\"   - Total memory: {(gen_total + disc_total) * 4 / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf84470",
   "metadata": {},
   "source": [
    "## 13. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6125734",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸŽ‰ MODEL ARCHITECTURE COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nâœ… Completed Tasks:\")\n",
    "print(\"   1. âœ“ Implemented building blocks (ResidualBlock, Attention, etc.)\")\n",
    "print(\"   2. âœ“ Created Generator (U-Net with attention)\")\n",
    "print(\"   3. âœ“ Created Discriminator (PatchGAN with spectral norm)\")\n",
    "print(\"   4. âœ“ Initialized both models on GPU\")\n",
    "print(\"   5. âœ“ Counted parameters and memory\")\n",
    "print(\"   6. âœ“ Tested Generator forward pass\")\n",
    "print(\"   7. âœ“ Tested Discriminator forward pass\")\n",
    "print(\"   8. âœ“ Tested full pipeline\")\n",
    "print(\"   9. âœ“ Visualized architecture\")\n",
    "print(\"   10. âœ“ Saved model configuration\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Statistics:\")\n",
    "print(f\"   ðŸ”· Generator:\")\n",
    "print(f\"      - Architecture: U-Net with self-attention\")\n",
    "print(f\"      - Input: [B, 41, 1024, 768]\")\n",
    "print(f\"      - Output: [B, 3, 1024, 768]\")\n",
    "print(f\"      - Parameters: {gen_total:,}\")\n",
    "print(f\"      - Memory: {gen_total * 4 / 1024**2:.2f} MB\")\n",
    "print(f\"   ðŸ”¶ Discriminator:\")\n",
    "print(f\"      - Architecture: PatchGAN with spectral normalization\")\n",
    "print(f\"      - Input: [B, 6, 1024, 768] (image + condition)\")\n",
    "print(f\"      - Output: [B, 1, {patch_h}, {patch_w}] (patch predictions)\")\n",
    "print(f\"      - Parameters: {disc_total:,}\")\n",
    "print(f\"      - Memory: {disc_total * 4 / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nðŸ“ Generated Files:\")\n",
    "print(f\"   - model_architecture.png\")\n",
    "print(f\"   - model_architecture_config.json\")\n",
    "\n",
    "print(\"\\nðŸš€ Ready for Next Steps:\")\n",
    "print(\"   1. Define loss functions (GAN, Perceptual, L1)\")\n",
    "print(\"   2. Implement training loop\")\n",
    "print(\"   3. Add checkpointing and logging\")\n",
    "print(\"   4. Start model training\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Features:\")\n",
    "print(\"   - U-Net encoder-decoder architecture\")\n",
    "print(\"   - Skip connections for feature preservation\")\n",
    "print(\"   - Self-attention at bottleneck\")\n",
    "print(\"   - Residual blocks for stability\")\n",
    "print(\"   - PatchGAN discriminator (70Ã—70)\")\n",
    "print(\"   - Spectral normalization for training stability\")\n",
    "print(\"   - Instance normalization throughout\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"\\nâœ… MODEL ARCHITECTURE READY FOR TRAINING!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774a2d83",
   "metadata": {},
   "source": [
    "# Loss Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34803636",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d4122",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models import VGG19_Weights\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c622ab8",
   "metadata": {},
   "source": [
    "## 2. Setup Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8700ba5c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Project paths\n",
    "project_root = Path(r'd:\\Projects\\AI-Virtual-TryOn')\n",
    "output_dir = project_root / 'outputs' / 'loss_functions'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load model configuration\n",
    "with open(project_root / 'outputs' / 'model_architecture' / 'model_architecture_config.json', 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "print(f\"ðŸ“ Project Root: {project_root}\")\n",
    "print(f\"ðŸ“ Output Directory: {output_dir}\")\n",
    "print(f\"\\nâœ… Loaded model configuration\")\n",
    "\n",
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "# Loss weights configuration\n",
    "loss_config = {\n",
    "    'lambda_gan': 1.0,        # GAN loss weight\n",
    "    'lambda_perceptual': 10.0, # Perceptual loss weight\n",
    "    'lambda_l1': 10.0,        # L1 reconstruction weight\n",
    "    'lambda_fm': 10.0,        # Feature matching weight\n",
    "    'vgg_layers': ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1']\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸ“Š Loss Configuration:\")\n",
    "for key, value in loss_config.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c92b52c",
   "metadata": {},
   "source": [
    "## 3. VGG19 Perceptual Loss Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65bc1c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Perceptual loss using VGG19 features.\n",
    "    Computes L1 distance between features from multiple layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 layers: List[str] = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1'],\n",
    "                 weights: Optional[List[float]] = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained VGG19\n",
    "        vgg = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features\n",
    "        \n",
    "        # Freeze VGG parameters\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Layer mapping\n",
    "        self.layer_name_mapping = {\n",
    "            'relu1_1': '1',\n",
    "            'relu1_2': '3',\n",
    "            'relu2_1': '6',\n",
    "            'relu2_2': '8',\n",
    "            'relu3_1': '11',\n",
    "            'relu3_2': '13',\n",
    "            'relu3_3': '15',\n",
    "            'relu3_4': '17',\n",
    "            'relu4_1': '20',\n",
    "            'relu4_2': '22',\n",
    "            'relu4_3': '24',\n",
    "            'relu4_4': '26',\n",
    "            'relu5_1': '29',\n",
    "            'relu5_2': '31',\n",
    "            'relu5_3': '33',\n",
    "            'relu5_4': '35',\n",
    "        }\n",
    "        \n",
    "        # Build feature extractors\n",
    "        self.features = nn.ModuleDict()\n",
    "        for layer_name in layers:\n",
    "            layer_idx = int(self.layer_name_mapping[layer_name])\n",
    "            self.features[layer_name] = nn.Sequential(*[vgg[i] for i in range(layer_idx + 1)])\n",
    "        \n",
    "        # Layer weights (equal if not specified)\n",
    "        self.weights = weights if weights is not None else [1.0] * len(layers)\n",
    "        \n",
    "        # Normalization for ImageNet\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "    \n",
    "    def normalize(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize from [-1, 1] to ImageNet normalization.\n",
    "        \"\"\"\n",
    "        # Convert from [-1, 1] to [0, 1]\n",
    "        x = (x + 1) / 2\n",
    "        # Apply ImageNet normalization\n",
    "        x = (x - self.mean) / self.std\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute perceptual loss between x and y.\n",
    "        \n",
    "        Args:\n",
    "            x: Generated image [B, 3, H, W]\n",
    "            y: Target image [B, 3, H, W]\n",
    "        \n",
    "        Returns:\n",
    "            Perceptual loss (scalar)\n",
    "        \"\"\"\n",
    "        # Normalize inputs\n",
    "        x = self.normalize(x)\n",
    "        y = self.normalize(y)\n",
    "        \n",
    "        loss = 0.0\n",
    "        \n",
    "        # Extract features and compute loss\n",
    "        for (layer_name, feature_extractor), weight in zip(self.features.items(), self.weights):\n",
    "            x_feat = feature_extractor(x)\n",
    "            y_feat = feature_extractor(y)\n",
    "            \n",
    "            # L1 distance between features\n",
    "            loss += weight * F.l1_loss(x_feat, y_feat)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "print(\"âœ… VGGPerceptualLoss defined\")\n",
    "print(\"   - Extracts features from VGG19\")\n",
    "print(\"   - Computes L1 distance across multiple layers\")\n",
    "print(\"   - Uses ImageNet pretrained weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df56d3",
   "metadata": {},
   "source": [
    "## 4. GAN Loss (Adversarial Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b411511",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    GAN loss (adversarial loss).\n",
    "    Supports multiple GAN objectives: vanilla, lsgan, hinge.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gan_mode: str = 'lsgan', target_real_label: float = 1.0, \n",
    "                 target_fake_label: float = 0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gan_mode: Type of GAN loss ('vanilla', 'lsgan', 'hinge')\n",
    "            target_real_label: Label for real images\n",
    "            target_fake_label: Label for fake images\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gan_mode = gan_mode\n",
    "        self.real_label = target_real_label\n",
    "        self.fake_label = target_fake_label\n",
    "        \n",
    "        if gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'hinge':\n",
    "            # Hinge loss computed manually\n",
    "            self.loss = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported GAN mode: {gan_mode}\")\n",
    "    \n",
    "    def get_target_tensor(self, prediction: torch.Tensor, target_is_real: bool) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create target tensor with same size as prediction.\n",
    "        \"\"\"\n",
    "        if target_is_real:\n",
    "            target = torch.ones_like(prediction) * self.real_label\n",
    "        else:\n",
    "            target = torch.ones_like(prediction) * self.fake_label\n",
    "        return target\n",
    "    \n",
    "    def forward(self, prediction: torch.Tensor, target_is_real: bool) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute GAN loss.\n",
    "        \n",
    "        Args:\n",
    "            prediction: Discriminator output [B, 1, H, W]\n",
    "            target_is_real: Whether target should be real (True) or fake (False)\n",
    "        \n",
    "        Returns:\n",
    "            GAN loss (scalar)\n",
    "        \"\"\"\n",
    "        if self.gan_mode == 'hinge':\n",
    "            if target_is_real:\n",
    "                # Discriminator loss for real: -min(0, -1 + D(x))\n",
    "                loss = F.relu(1.0 - prediction).mean()\n",
    "            else:\n",
    "                # Discriminator loss for fake: -min(0, -1 - D(G(z)))\n",
    "                loss = F.relu(1.0 + prediction).mean()\n",
    "        else:\n",
    "            target = self.get_target_tensor(prediction, target_is_real)\n",
    "            loss = self.loss(prediction, target)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "print(\"âœ… GANLoss defined\")\n",
    "print(\"   - Supports vanilla, LSGAN, and hinge loss\")\n",
    "print(\"   - Handles both real and fake predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0868a",
   "metadata": {},
   "source": [
    "## 5. Feature Matching Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11acd1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class FeatureMatchingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature matching loss.\n",
    "    Matches intermediate discriminator features between real and fake images.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, fake_features: List[torch.Tensor], \n",
    "                real_features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute feature matching loss.\n",
    "        \n",
    "        Args:\n",
    "            fake_features: List of feature maps from discriminator (fake images)\n",
    "            real_features: List of feature maps from discriminator (real images)\n",
    "        \n",
    "        Returns:\n",
    "            Feature matching loss (scalar)\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "        \n",
    "        for fake_feat, real_feat in zip(fake_features, real_features):\n",
    "            loss += F.l1_loss(fake_feat, real_feat.detach())\n",
    "        \n",
    "        return loss / len(fake_features)\n",
    "\n",
    "\n",
    "print(\"âœ… FeatureMatchingLoss defined\")\n",
    "print(\"   - Matches discriminator intermediate features\")\n",
    "print(\"   - Helps stabilize GAN training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e54558",
   "metadata": {},
   "source": [
    "## 6. Combined Loss Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a41c5bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class VITONLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined loss for Virtual Try-On.\n",
    "    Combines GAN, perceptual, L1, and feature matching losses.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 lambda_gan: float = 1.0,\n",
    "                 lambda_perceptual: float = 10.0,\n",
    "                 lambda_l1: float = 10.0,\n",
    "                 lambda_fm: float = 10.0,\n",
    "                 vgg_layers: List[str] = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1'],\n",
    "                 gan_mode: str = 'lsgan'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Loss weights\n",
    "        self.lambda_gan = lambda_gan\n",
    "        self.lambda_perceptual = lambda_perceptual\n",
    "        self.lambda_l1 = lambda_l1\n",
    "        self.lambda_fm = lambda_fm\n",
    "        \n",
    "        # Loss functions\n",
    "        self.gan_loss = GANLoss(gan_mode=gan_mode)\n",
    "        self.perceptual_loss = VGGPerceptualLoss(layers=vgg_layers)\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.fm_loss = FeatureMatchingLoss()\n",
    "    \n",
    "    def compute_generator_loss(self,\n",
    "                              fake_image: torch.Tensor,\n",
    "                              real_image: torch.Tensor,\n",
    "                              disc_fake: torch.Tensor,\n",
    "                              fake_features: Optional[List[torch.Tensor]] = None,\n",
    "                              real_features: Optional[List[torch.Tensor]] = None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute generator losses.\n",
    "        \n",
    "        Args:\n",
    "            fake_image: Generated image [B, 3, H, W]\n",
    "            real_image: Target real image [B, 3, H, W]\n",
    "            disc_fake: Discriminator output for fake image [B, 1, H', W']\n",
    "            fake_features: Discriminator features for fake (optional)\n",
    "            real_features: Discriminator features for real (optional)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with individual and total losses\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        \n",
    "        # GAN loss (fool discriminator)\n",
    "        losses['gan'] = self.gan_loss(disc_fake, target_is_real=True) * self.lambda_gan\n",
    "        \n",
    "        # Perceptual loss\n",
    "        losses['perceptual'] = self.perceptual_loss(fake_image, real_image) * self.lambda_perceptual\n",
    "        \n",
    "        # L1 reconstruction loss\n",
    "        losses['l1'] = self.l1_loss(fake_image, real_image) * self.lambda_l1\n",
    "        \n",
    "        # Feature matching loss (if features provided)\n",
    "        if fake_features is not None and real_features is not None:\n",
    "            losses['fm'] = self.fm_loss(fake_features, real_features) * self.lambda_fm\n",
    "        else:\n",
    "            losses['fm'] = torch.tensor(0.0, device=fake_image.device)\n",
    "        \n",
    "        # Total loss\n",
    "        losses['total'] = losses['gan'] + losses['perceptual'] + losses['l1'] + losses['fm']\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def compute_discriminator_loss(self,\n",
    "                                   disc_real: torch.Tensor,\n",
    "                                   disc_fake: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute discriminator losses.\n",
    "        \n",
    "        Args:\n",
    "            disc_real: Discriminator output for real image [B, 1, H', W']\n",
    "            disc_fake: Discriminator output for fake image [B, 1, H', W']\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with individual and total losses\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        \n",
    "        # Real loss\n",
    "        losses['real'] = self.gan_loss(disc_real, target_is_real=True)\n",
    "        \n",
    "        # Fake loss\n",
    "        losses['fake'] = self.gan_loss(disc_fake, target_is_real=False)\n",
    "        \n",
    "        # Total loss (average of real and fake)\n",
    "        losses['total'] = (losses['real'] + losses['fake']) * 0.5\n",
    "        \n",
    "        return losses\n",
    "\n",
    "\n",
    "print(\"âœ… VITONLoss defined\")\n",
    "print(\"   - Combines GAN, perceptual, L1, and feature matching losses\")\n",
    "print(\"   - Separate methods for generator and discriminator\")\n",
    "print(\"   - Returns detailed loss breakdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997cd7c",
   "metadata": {},
   "source": [
    "## 7. Initialize Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b469c73",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize combined loss\n",
    "criterion = VITONLoss(\n",
    "    lambda_gan=loss_config['lambda_gan'],\n",
    "    lambda_perceptual=loss_config['lambda_perceptual'],\n",
    "    lambda_l1=loss_config['lambda_l1'],\n",
    "    lambda_fm=loss_config['lambda_fm'],\n",
    "    vgg_layers=loss_config['vgg_layers'],\n",
    "    gan_mode='lsgan'\n",
    ").to(device)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸŽ¯ LOSS FUNCTIONS INITIALIZED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nâœ… VITONLoss created and moved to {device}\")\n",
    "print(f\"\\nðŸ“Š Loss weights:\")\n",
    "print(f\"   - GAN: {loss_config['lambda_gan']}\")\n",
    "print(f\"   - Perceptual: {loss_config['lambda_perceptual']}\")\n",
    "print(f\"   - L1: {loss_config['lambda_l1']}\")\n",
    "print(f\"   - Feature Matching: {loss_config['lambda_fm']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b1ca90",
   "metadata": {},
   "source": [
    "## 8. Test Loss Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b1b2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ§ª TESTING LOSS COMPUTATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create dummy data\n",
    "batch_size = 2\n",
    "fake_image = torch.randn(batch_size, 3, 1024, 768).to(device)\n",
    "real_image = torch.randn(batch_size, 3, 1024, 768).to(device)\n",
    "disc_fake = torch.randn(batch_size, 1, 126, 94).to(device)\n",
    "disc_real = torch.randn(batch_size, 1, 126, 94).to(device)\n",
    "\n",
    "print(f\"\\nðŸ“¥ Input shapes:\")\n",
    "print(f\"   Fake image: {tuple(fake_image.shape)}\")\n",
    "print(f\"   Real image: {tuple(real_image.shape)}\")\n",
    "print(f\"   Disc fake: {tuple(disc_fake.shape)}\")\n",
    "print(f\"   Disc real: {tuple(disc_real.shape)}\")\n",
    "\n",
    "# Test generator loss\n",
    "print(f\"\\nðŸ”· Testing Generator Loss...\")\n",
    "gen_losses = criterion.compute_generator_loss(\n",
    "    fake_image=fake_image,\n",
    "    real_image=real_image,\n",
    "    disc_fake=disc_fake\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Generator Losses:\")\n",
    "for name, loss in gen_losses.items():\n",
    "    print(f\"   {name:15s}: {loss.item():.4f}\")\n",
    "\n",
    "# Test discriminator loss\n",
    "print(f\"\\nðŸ”¶ Testing Discriminator Loss...\")\n",
    "disc_losses = criterion.compute_discriminator_loss(\n",
    "    disc_real=disc_real,\n",
    "    disc_fake=disc_fake\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Discriminator Losses:\")\n",
    "for name, loss in disc_losses.items():\n",
    "    print(f\"   {name:15s}: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nâœ… Loss computation test successful!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b915dc",
   "metadata": {},
   "source": [
    "## 9. Test Individual Loss Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c24e40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ”¬ TESTING INDIVIDUAL LOSS COMPONENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test GAN loss\n",
    "print(\"\\nðŸ”· GAN Loss:\")\n",
    "gan_loss_real = criterion.gan_loss(disc_real, target_is_real=True)\n",
    "gan_loss_fake = criterion.gan_loss(disc_fake, target_is_real=False)\n",
    "print(f\"   Real: {gan_loss_real.item():.4f}\")\n",
    "print(f\"   Fake: {gan_loss_fake.item():.4f}\")\n",
    "\n",
    "# Test perceptual loss\n",
    "print(\"\\nðŸ”· Perceptual Loss:\")\n",
    "perceptual_loss = criterion.perceptual_loss(fake_image, real_image)\n",
    "print(f\"   Loss: {perceptual_loss.item():.4f}\")\n",
    "\n",
    "# Test L1 loss\n",
    "print(\"\\nðŸ”· L1 Loss:\")\n",
    "l1_loss = criterion.l1_loss(fake_image, real_image)\n",
    "print(f\"   Loss: {l1_loss.item():.4f}\")\n",
    "\n",
    "# Test with different similarity levels\n",
    "print(\"\\nðŸ”¬ Testing with different image similarities:\")\n",
    "identical = real_image.clone()\n",
    "similar = real_image + torch.randn_like(real_image) * 0.1\n",
    "different = torch.randn_like(real_image)\n",
    "\n",
    "print(\"\\n   Identical images:\")\n",
    "print(f\"      Perceptual: {criterion.perceptual_loss(identical, real_image).item():.4f}\")\n",
    "print(f\"      L1: {criterion.l1_loss(identical, real_image).item():.4f}\")\n",
    "\n",
    "print(\"\\n   Similar images (noise=0.1):\")\n",
    "print(f\"      Perceptual: {criterion.perceptual_loss(similar, real_image).item():.4f}\")\n",
    "print(f\"      L1: {criterion.l1_loss(similar, real_image).item():.4f}\")\n",
    "\n",
    "print(\"\\n   Different images (random):\")\n",
    "print(f\"      Perceptual: {criterion.perceptual_loss(different, real_image).item():.4f}\")\n",
    "print(f\"      L1: {criterion.l1_loss(different, real_image).item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nâœ… Individual loss tests successful!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58147b21",
   "metadata": {},
   "source": [
    "## 10. Visualize Loss Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9602ab4b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_loss_comparison():\n",
    "    \"\"\"\n",
    "    Visualize how different losses respond to image similarity.\n",
    "    \"\"\"\n",
    "    # Create images with varying levels of similarity\n",
    "    base_image = torch.randn(1, 3, 256, 256).to(device)\n",
    "    noise_levels = np.linspace(0, 2, 20)\n",
    "    \n",
    "    perceptual_losses = []\n",
    "    l1_losses = []\n",
    "    \n",
    "    for noise in noise_levels:\n",
    "        noisy_image = base_image + torch.randn_like(base_image) * noise\n",
    "        noisy_image = torch.clamp(noisy_image, -1, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            perceptual = criterion.perceptual_loss(noisy_image, base_image).item()\n",
    "            l1 = criterion.l1_loss(noisy_image, base_image).item()\n",
    "        \n",
    "        perceptual_losses.append(perceptual)\n",
    "        l1_losses.append(l1)\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(noise_levels, perceptual_losses, 'b-', linewidth=2, label='Perceptual Loss')\n",
    "    ax1.plot(noise_levels, l1_losses, 'r-', linewidth=2, label='L1 Loss')\n",
    "    ax1.set_xlabel('Noise Level', fontsize=12)\n",
    "    ax1.set_ylabel('Loss Value', fontsize=12)\n",
    "    ax1.set_title('Loss vs Image Similarity', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Normalized comparison\n",
    "    perceptual_norm = np.array(perceptual_losses) / max(perceptual_losses)\n",
    "    l1_norm = np.array(l1_losses) / max(l1_losses)\n",
    "    \n",
    "    ax2.plot(noise_levels, perceptual_norm, 'b-', linewidth=2, label='Perceptual (normalized)')\n",
    "    ax2.plot(noise_levels, l1_norm, 'r-', linewidth=2, label='L1 (normalized)')\n",
    "    ax2.set_xlabel('Noise Level', fontsize=12)\n",
    "    ax2.set_ylabel('Normalized Loss', fontsize=12)\n",
    "    ax2.set_title('Normalized Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'loss_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Loss comparison visualization saved\")\n",
    "\n",
    "\n",
    "visualize_loss_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df28a44",
   "metadata": {},
   "source": [
    "## 11. Loss Weights Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83412b68",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_loss_weights():\n",
    "    \"\"\"\n",
    "    Analyze the impact of different loss weights.\n",
    "    \"\"\"\n",
    "    # Sample data\n",
    "    fake = torch.randn(1, 3, 256, 256).to(device)\n",
    "    real = torch.randn(1, 3, 256, 256).to(device)\n",
    "    disc = torch.randn(1, 1, 31, 23).to(device)\n",
    "    \n",
    "    # Test different weight configurations\n",
    "    weight_configs = [\n",
    "        {'lambda_gan': 1, 'lambda_perceptual': 0, 'lambda_l1': 0, 'lambda_fm': 0},\n",
    "        {'lambda_gan': 0, 'lambda_perceptual': 1, 'lambda_l1': 0, 'lambda_fm': 0},\n",
    "        {'lambda_gan': 0, 'lambda_perceptual': 0, 'lambda_l1': 1, 'lambda_fm': 0},\n",
    "        {'lambda_gan': 1, 'lambda_perceptual': 10, 'lambda_l1': 10, 'lambda_fm': 10},\n",
    "    ]\n",
    "    \n",
    "    config_names = ['GAN only', 'Perceptual only', 'L1 only', 'Combined (balanced)']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for config, name in zip(weight_configs, config_names):\n",
    "        test_criterion = VITONLoss(**config, vgg_layers=loss_config['vgg_layers']).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            losses = test_criterion.compute_generator_loss(fake, real, disc)\n",
    "        \n",
    "        results.append({\n",
    "            'config': name,\n",
    "            'total': losses['total'].item(),\n",
    "            'gan': losses['gan'].item(),\n",
    "            'perceptual': losses['perceptual'].item(),\n",
    "            'l1': losses['l1'].item(),\n",
    "            'fm': losses['fm'].item()\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ“Š LOSS WEIGHTS SENSITIVITY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{df.to_string(index=False)}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(config_names))\n",
    "    width = 0.15\n",
    "    \n",
    "    ax.bar(x - 2*width, df['gan'], width, label='GAN', color='#ff9999')\n",
    "    ax.bar(x - width, df['perceptual'], width, label='Perceptual', color='#66b3ff')\n",
    "    ax.bar(x, df['l1'], width, label='L1', color='#99ff99')\n",
    "    ax.bar(x + width, df['fm'], width, label='Feature Matching', color='#ffcc99')\n",
    "    ax.bar(x + 2*width, df['total'], width, label='Total', color='#ff99cc', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Configuration', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Loss Value', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Loss Components Across Different Weight Configurations', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(config_names, rotation=15, ha='right')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'loss_weights_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… Loss weights analysis complete\")\n",
    "\n",
    "\n",
    "analyze_loss_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e88e18",
   "metadata": {},
   "source": [
    "## 12. Save Loss Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a63bd9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create detailed loss configuration\n",
    "loss_full_config = {\n",
    "    'loss_weights': {\n",
    "        'lambda_gan': loss_config['lambda_gan'],\n",
    "        'lambda_perceptual': loss_config['lambda_perceptual'],\n",
    "        'lambda_l1': loss_config['lambda_l1'],\n",
    "        'lambda_fm': loss_config['lambda_fm']\n",
    "    },\n",
    "    'gan_loss': {\n",
    "        'type': 'lsgan',\n",
    "        'target_real': 1.0,\n",
    "        'target_fake': 0.0\n",
    "    },\n",
    "    'perceptual_loss': {\n",
    "        'network': 'VGG19',\n",
    "        'layers': loss_config['vgg_layers'],\n",
    "        'pretrained': 'ImageNet'\n",
    "    },\n",
    "    'reconstruction_loss': {\n",
    "        'type': 'L1',\n",
    "        'pixel_wise': True\n",
    "    },\n",
    "    'feature_matching': {\n",
    "        'enabled': True,\n",
    "        'discriminator_features': True\n",
    "    },\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = output_dir / 'loss_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(loss_full_config, f, indent=2)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ’¾ LOSS CONFIGURATION SAVED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“„ Config saved to: {config_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"   - GAN weight: {loss_config['lambda_gan']}\")\n",
    "print(f\"   - Perceptual weight: {loss_config['lambda_perceptual']}\")\n",
    "print(f\"   - L1 weight: {loss_config['lambda_l1']}\")\n",
    "print(f\"   - Feature matching weight: {loss_config['lambda_fm']}\")\n",
    "print(f\"   - VGG layers: {len(loss_config['vgg_layers'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c46c6",
   "metadata": {},
   "source": [
    "## 13. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eddf84a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸŽ‰ LOSS FUNCTIONS COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nâœ… Completed Tasks:\")\n",
    "print(\"   1. âœ“ Implemented VGG19 perceptual loss\")\n",
    "print(\"   2. âœ“ Implemented GAN loss (LSGAN)\")\n",
    "print(\"   3. âœ“ Implemented feature matching loss\")\n",
    "print(\"   4. âœ“ Created combined VITONLoss module\")\n",
    "print(\"   5. âœ“ Tested all loss computations\")\n",
    "print(\"   6. âœ“ Tested individual loss components\")\n",
    "print(\"   7. âœ“ Visualized loss comparisons\")\n",
    "print(\"   8. âœ“ Analyzed loss weight sensitivity\")\n",
    "print(\"   9. âœ“ Saved loss configuration\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Loss Components:\")\n",
    "print(f\"   ðŸ”· GAN Loss:\")\n",
    "print(f\"      - Type: LSGAN (Mean Squared Error)\")\n",
    "print(f\"      - Weight: {loss_config['lambda_gan']}\")\n",
    "print(f\"      - Purpose: Adversarial training for realistic generation\")\n",
    "print(f\"   ðŸ”· Perceptual Loss:\")\n",
    "print(f\"      - Network: VGG19 (ImageNet pretrained)\")\n",
    "print(f\"      - Layers: {len(loss_config['vgg_layers'])} layers\")\n",
    "print(f\"      - Weight: {loss_config['lambda_perceptual']}\")\n",
    "print(f\"      - Purpose: Feature-level similarity\")\n",
    "print(f\"   ðŸ”· L1 Loss:\")\n",
    "print(f\"      - Type: Mean Absolute Error\")\n",
    "print(f\"      - Weight: {loss_config['lambda_l1']}\")\n",
    "print(f\"      - Purpose: Pixel-wise reconstruction\")\n",
    "print(f\"   ðŸ”· Feature Matching:\")\n",
    "print(f\"      - Type: Discriminator feature matching\")\n",
    "print(f\"      - Weight: {loss_config['lambda_fm']}\")\n",
    "print(f\"      - Purpose: Training stability\")\n",
    "\n",
    "print(f\"\\nðŸ“ Generated Files:\")\n",
    "print(f\"   - loss_comparison.png\")\n",
    "print(f\"   - loss_weights_analysis.png\")\n",
    "print(f\"   - loss_config.json\")\n",
    "\n",
    "print(\"\\nðŸš€ Ready for Next Steps:\")\n",
    "print(\"   1. Implement training loop\")\n",
    "print(\"   2. Add checkpointing and logging\")\n",
    "print(\"   3. Implement learning rate scheduling\")\n",
    "print(\"   4. Start model training\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Features:\")\n",
    "print(\"   - Multi-scale perceptual loss (5 VGG layers)\")\n",
    "print(\"   - Stable GAN training with LSGAN\")\n",
    "print(\"   - Feature matching for better convergence\")\n",
    "print(\"   - Balanced loss weights for quality\")\n",
    "print(\"   - Comprehensive loss breakdown for monitoring\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"\\nâœ… LOSS FUNCTIONS READY FOR TRAINING!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
