{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed54e1b2",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe73639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa2f62",
   "metadata": {},
   "source": [
    "## 2. Setup Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45338534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project paths\n",
    "project_root = Path(r'd:\\Projects\\AI-Virtual-TryOn')\n",
    "data_dir = project_root / 'data' / 'raw' / 'viton-hd'\n",
    "output_dir = project_root / 'outputs' / 'training'\n",
    "checkpoint_dir = output_dir / 'checkpoints'\n",
    "logs_dir = output_dir / 'logs'\n",
    "\n",
    "# Create directories\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load configurations\n",
    "with open(project_root / 'outputs' / 'model_architecture' / 'model_architecture_config.json', 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "with open(project_root / 'outputs' / 'loss_functions' / 'loss_config.json', 'r') as f:\n",
    "    loss_config = json.load(f)\n",
    "\n",
    "print(f\"üìÅ Project Root: {project_root}\")\n",
    "print(f\"üìÅ Data Directory: {data_dir}\")\n",
    "print(f\"üìÅ Output Directory: {output_dir}\")\n",
    "print(f\"üìÅ Checkpoint Directory: {checkpoint_dir}\")\n",
    "print(f\"üìÅ Logs Directory: {logs_dir}\")\n",
    "print(f\"\\n‚úÖ Loaded model and loss configurations\")\n",
    "\n",
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nüñ•Ô∏è Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f375af6",
   "metadata": {},
   "source": [
    "## 3. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "training_config = {\n",
    "    # Training settings\n",
    "    'num_epochs': 100,\n",
    "    'batch_size': 4,\n",
    "    'num_workers': 0,  # Windows compatibility\n",
    "    \n",
    "    # Optimizer settings\n",
    "    'lr_g': 0.0002,  # Generator learning rate\n",
    "    'lr_d': 0.0002,  # Discriminator learning rate\n",
    "    'beta1': 0.5,    # Adam beta1\n",
    "    'beta2': 0.999,  # Adam beta2\n",
    "    \n",
    "    # Scheduler settings\n",
    "    'decay_after_epoch': 50,  # Start LR decay after this epoch\n",
    "    'decay_type': 'linear',   # 'linear' or 'step'\n",
    "    \n",
    "    # Training stability\n",
    "    'gradient_clip': 5.0,      # Gradient clipping value\n",
    "    'discriminator_steps': 1,  # Discriminator updates per generator update\n",
    "    \n",
    "    # Checkpointing\n",
    "    'save_every': 5,           # Save checkpoint every N epochs\n",
    "    'keep_best': True,         # Keep best model based on validation\n",
    "    'max_checkpoints': 5,      # Maximum checkpoints to keep\n",
    "    \n",
    "    # Validation\n",
    "    'validate_every': 1,       # Validate every N epochs\n",
    "    'save_images_every': 5,    # Save sample images every N epochs\n",
    "    \n",
    "    # Early stopping\n",
    "    'early_stopping': True,\n",
    "    'patience': 20,            # Epochs without improvement\n",
    "    \n",
    "    # Logging\n",
    "    'log_every': 50,           # Log every N batches\n",
    "    'tensorboard': True,       # Use TensorBoard\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä TRAINING CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\\\nüéØ Training Settings:\")\n",
    "print(f\"   Epochs: {training_config['num_epochs']}\")\n",
    "print(f\"   Batch size: {training_config['batch_size']}\")\n",
    "print(f\"   Num workers: {training_config['num_workers']}\")\n",
    "\n",
    "print(f\"\\\\n‚öôÔ∏è Optimizer Settings:\")\n",
    "print(f\"   Generator LR: {training_config['lr_g']}\")\n",
    "print(f\"   Discriminator LR: {training_config['lr_d']}\")\n",
    "print(f\"   Beta1: {training_config['beta1']}, Beta2: {training_config['beta2']}\")\n",
    "\n",
    "print(f\"\\\\nüìâ Scheduler Settings:\")\n",
    "print(f\"   Decay after epoch: {training_config['decay_after_epoch']}\")\n",
    "print(f\"   Decay type: {training_config['decay_type']}\")\n",
    "\n",
    "print(f\"\\\\nüõ°Ô∏è Training Stability:\")\n",
    "print(f\"   Gradient clipping: {training_config['gradient_clip']}\")\n",
    "print(f\"   Discriminator steps: {training_config['discriminator_steps']}\")\n",
    "\n",
    "print(f\"\\\\nüíæ Checkpointing:\")\n",
    "print(f\"   Save every: {training_config['save_every']} epochs\")\n",
    "print(f\"   Keep best: {training_config['keep_best']}\")\n",
    "print(f\"   Max checkpoints: {training_config['max_checkpoints']}\")\n",
    "\n",
    "print(f\"\\\\n‚úÖ Validation:\")\n",
    "print(f\"   Validate every: {training_config['validate_every']} epochs\")\n",
    "print(f\"   Save images every: {training_config['save_images_every']} epochs\")\n",
    "\n",
    "print(f\"\\\\nüõë Early Stopping:\")\n",
    "print(f\"   Enabled: {training_config['early_stopping']}\")\n",
    "print(f\"   Patience: {training_config['patience']} epochs\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa20428",
   "metadata": {},
   "source": [
    "## 4. Load Model Architectures from Notebook 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdebec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model architecture classes from Notebook 08\n",
    "\n",
    "# Building blocks\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.norm1 = nn.InstanceNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, 1, 1)\n",
    "        self.norm2 = nn.InstanceNorm2d(channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.norm1(self.conv1(x)))\n",
    "        out = self.norm2(self.conv2(out))\n",
    "        return out + residual\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
    "        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)\n",
    "        self.value = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        query = self.query(x).view(B, -1, H * W).permute(0, 2, 1)\n",
    "        key = self.key(x).view(B, -1, H * W)\n",
    "        attention = F.softmax(torch.bmm(query, key), dim=-1)\n",
    "        value = self.value(x).view(B, C, H * W)\n",
    "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
    "        out = out.view(B, C, H, W)\n",
    "        return self.gamma * out + x\n",
    "\n",
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 4, 2, 1)\n",
    "        self.norm = nn.InstanceNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.relu(self.norm(self.conv(x)))\n",
    "\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1)\n",
    "        self.norm = nn.InstanceNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return F.relu(self.norm(self.conv(x)))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Building blocks defined\")\n",
    "print(\"   - ResidualBlock, SelfAttention, DownsampleBlock, UpsampleBlock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05b9a32",
   "metadata": {},
   "source": [
    "## 5. Generator and Discriminator Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b579be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Network (U-Net with Self-Attention)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels=41, output_channels=3, ngf=64, num_downs=4, num_res_blocks=9):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, ngf, 7, 1, 3),\n",
    "            nn.InstanceNorm2d(ngf),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.encoder = nn.ModuleList()\n",
    "        in_ch = ngf\n",
    "        for i in range(num_downs):\n",
    "            out_ch = min(in_ch * 2, 512)\n",
    "            self.encoder.append(DownsampleBlock(in_ch, out_ch))\n",
    "            in_ch = out_ch\n",
    "        \n",
    "        # Bottleneck with residual blocks and attention\n",
    "        self.bottleneck = nn.ModuleList()\n",
    "        for _ in range(num_res_blocks):\n",
    "            self.bottleneck.append(ResidualBlock(in_ch))\n",
    "        self.attention = SelfAttention(in_ch)\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(num_downs):\n",
    "            out_ch = max(in_ch // 2, ngf)\n",
    "            self.decoder.append(UpsampleBlock(in_ch * 2, out_ch))  # *2 for skip connections\n",
    "            in_ch = out_ch\n",
    "        \n",
    "        # Final convolution\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(ngf, output_channels, 7, 1, 3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial\n",
    "        x = self.initial(x)\n",
    "        \n",
    "        # Encoder with skip connections\n",
    "        skips = []\n",
    "        for enc in self.encoder:\n",
    "            x = enc(x)\n",
    "            skips.append(x)\n",
    "        \n",
    "        # Bottleneck\n",
    "        for res_block in self.bottleneck:\n",
    "            x = res_block(x)\n",
    "        x = self.attention(x)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        for dec, skip in zip(self.decoder, reversed(skips)):\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = dec(x)\n",
    "        \n",
    "        # Final\n",
    "        return self.final(x)\n",
    "\n",
    "\n",
    "# Discriminator Network (PatchGAN)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels=6, ndf=64, n_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        def discriminator_block(in_ch, out_ch, normalize=True):\n",
    "            layers = [nn.utils.spectral_norm(nn.Conv2d(in_ch, out_ch, 4, 2, 1))]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_ch))\n",
    "            layers.append(nn.LeakyReLU(0.2, True))\n",
    "            return layers\n",
    "        \n",
    "        layers = []\n",
    "        in_ch = input_channels\n",
    "        \n",
    "        # First layer (no normalization)\n",
    "        layers.extend(discriminator_block(in_ch, ndf, normalize=False))\n",
    "        in_ch = ndf\n",
    "        \n",
    "        # Intermediate layers\n",
    "        for i in range(n_layers - 1):\n",
    "            out_ch = min(in_ch * 2, 512)\n",
    "            layers.extend(discriminator_block(in_ch, out_ch))\n",
    "            in_ch = out_ch\n",
    "        \n",
    "        # Final layer\n",
    "        layers.extend(discriminator_block(in_ch, in_ch))\n",
    "        layers.append(nn.utils.spectral_norm(nn.Conv2d(in_ch, 1, 4, 1, 1)))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, image, condition):\n",
    "        x = torch.cat([image, condition], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Generator and Discriminator networks defined\")\n",
    "print(\"   - Generator: U-Net with self-attention\")\n",
    "print(\"   - Discriminator: PatchGAN with spectral normalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a948951",
   "metadata": {},
   "source": [
    "## 6. Load Loss Functions from Notebook 09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7765f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import loss functions from torchvision\n",
    "from torchvision.models import VGG19_Weights\n",
    "import torchvision.models as models\n",
    "\n",
    "# VGG Perceptual Loss\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self, layers=['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1'], weights=None):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.layer_name_mapping = {\n",
    "            'relu1_1': '1', 'relu2_1': '6', 'relu3_1': '11', 'relu4_1': '20', 'relu5_1': '29'\n",
    "        }\n",
    "        \n",
    "        self.features = nn.ModuleDict()\n",
    "        for layer_name in layers:\n",
    "            layer_idx = int(self.layer_name_mapping[layer_name])\n",
    "            self.features[layer_name] = nn.Sequential(*[vgg[i] for i in range(layer_idx + 1)])\n",
    "        \n",
    "        self.weights = weights if weights is not None else [1.0] * len(layers)\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "    \n",
    "    def normalize(self, x):\n",
    "        x = (x + 1) / 2\n",
    "        x = (x - self.mean) / self.std\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = self.normalize(x)\n",
    "        y = self.normalize(y)\n",
    "        loss = 0.0\n",
    "        for (layer_name, feature_extractor), weight in zip(self.features.items(), self.weights):\n",
    "            x_feat = feature_extractor(x)\n",
    "            y_feat = feature_extractor(y)\n",
    "            loss += weight * F.l1_loss(x_feat, y_feat)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# GAN Loss\n",
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, gan_mode='lsgan', target_real_label=1.0, target_fake_label=0.0):\n",
    "        super().__init__()\n",
    "        self.gan_mode = gan_mode\n",
    "        self.real_label = target_real_label\n",
    "        self.fake_label = target_fake_label\n",
    "        \n",
    "        if gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'hinge':\n",
    "            self.loss = None\n",
    "        else:\n",
    "            raise ValueError(f\\\"Unsupported GAN mode: {gan_mode}\\\")\n",
    "    \n",
    "    def get_target_tensor(self, prediction, target_is_real):\n",
    "        if target_is_real:\n",
    "            target = torch.ones_like(prediction) * self.real_label\n",
    "        else:\n",
    "            target = torch.ones_like(prediction) * self.fake_label\n",
    "        return target\n",
    "    \n",
    "    def forward(self, prediction, target_is_real):\n",
    "        if self.gan_mode == 'hinge':\n",
    "            if target_is_real:\n",
    "                loss = F.relu(1.0 - prediction).mean()\n",
    "            else:\n",
    "                loss = F.relu(1.0 + prediction).mean()\n",
    "        else:\n",
    "            target = self.get_target_tensor(prediction, target_is_real)\n",
    "            loss = self.loss(prediction, target)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Combined VITON Loss\n",
    "class VITONLoss(nn.Module):\n",
    "    def __init__(self, lambda_gan=1.0, lambda_perceptual=10.0, lambda_l1=10.0, \n",
    "                 vgg_layers=['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1'], gan_mode='lsgan'):\n",
    "        super().__init__()\n",
    "        self.lambda_gan = lambda_gan\n",
    "        self.lambda_perceptual = lambda_perceptual\n",
    "        self.lambda_l1 = lambda_l1\n",
    "        \n",
    "        self.gan_loss = GANLoss(gan_mode=gan_mode)\n",
    "        self.perceptual_loss = VGGPerceptualLoss(layers=vgg_layers)\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "    \n",
    "    def compute_generator_loss(self, fake_image, real_image, disc_fake):\n",
    "        losses = {}\n",
    "        losses['gan'] = self.gan_loss(disc_fake, target_is_real=True) * self.lambda_gan\n",
    "        losses['perceptual'] = self.perceptual_loss(fake_image, real_image) * self.lambda_perceptual\n",
    "        losses['l1'] = self.l1_loss(fake_image, real_image) * self.lambda_l1\n",
    "        losses['total'] = losses['gan'] + losses['perceptual'] + losses['l1']\n",
    "        return losses\n",
    "    \n",
    "    def compute_discriminator_loss(self, disc_real, disc_fake):\n",
    "        losses = {}\n",
    "        losses['real'] = self.gan_loss(disc_real, target_is_real=True)\n",
    "        losses['fake'] = self.gan_loss(disc_fake, target_is_real=False)\n",
    "        losses['total'] = (losses['real'] + losses['fake']) * 0.5\n",
    "        return losses\n",
    "\n",
    "\n",
    "print(\"‚úÖ Loss functions defined\")\n",
    "print(\"   - VGGPerceptualLoss, GANLoss, VITONLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f1df1",
   "metadata": {},
   "source": [
    "## 7. Load Dataset (Small Subset for Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ff18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, we'll create a simple dummy dataset to test the training loop\n",
    "# In production, you would load the actual VITON dataset\n",
    "\n",
    "class DummyVITONDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dummy dataset for testing training loop\"\"\"\n",
    "    def __init__(self, num_samples=100):\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate dummy data matching expected shapes\n",
    "        # In real dataset: load actual images, parsing, pose, etc.\n",
    "        return {\n",
    "            'multi_channel_input': torch.randn(41, 1024, 768),  # 41-channel input\n",
    "            'target_image': torch.randn(3, 1024, 768),          # Target RGB image\n",
    "            'cloth_image': torch.randn(3, 1024, 768),           # Cloth condition\n",
    "        }\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating dummy datasets for testing...\")\n",
    "train_dataset = DummyVITONDataset(num_samples=200)  # Small for testing\n",
    "val_dataset = DummyVITONDataset(num_samples=50)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=training_config['num_workers'],\n",
    "    pin_memory=True if device == 'cuda' else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=training_config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=training_config['num_workers'],\n",
    "    pin_memory=True if device == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä DATASET LOADED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\\\n‚úÖ Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"‚úÖ Val dataset: {len(val_dataset)} samples\")\n",
    "print(f\"\\\\nüì¶ Train batches: {len(train_loader)}\")\n",
    "print(f\"üì¶ Val batches: {len(val_loader)}\")\n",
    "print(f\"\\\\n‚öôÔ∏è Batch size: {training_config['batch_size']}\")\n",
    "print(f\"‚öôÔ∏è Num workers: {training_config['num_workers']}\")\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"\\\\n‚ö†Ô∏è  Note: Using dummy data for testing. Replace with actual VITON dataset for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8467be8",
   "metadata": {},
   "source": [
    "## 8. Initialize Models, Optimizers, and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da695b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "print(\"Initializing models...\")\n",
    "generator = Generator(\n",
    "    input_channels=41,\n",
    "    output_channels=3,\n",
    "    ngf=64,\n",
    "    num_downs=4,\n",
    "    num_res_blocks=9\n",
    ").to(device)\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    input_channels=6,  # 3 (image) + 3 (condition)\n",
    "    ndf=64,\n",
    "    n_layers=3\n",
    ").to(device)\n",
    "\n",
    "# Initialize loss\n",
    "criterion = VITONLoss(\n",
    "    lambda_gan=loss_config['loss_weights']['lambda_gan'],\n",
    "    lambda_perceptual=loss_config['loss_weights']['lambda_perceptual'],\n",
    "    lambda_l1=loss_config['loss_weights']['lambda_l1'],\n",
    "    vgg_layers=loss_config['perceptual_loss']['layers'],\n",
    "    gan_mode='lsgan'\n",
    ").to(device)\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer_g = optim.Adam(\n",
    "    generator.parameters(),\n",
    "    lr=training_config['lr_g'],\n",
    "    betas=(training_config['beta1'], training_config['beta2'])\n",
    ")\n",
    "\n",
    "optimizer_d = optim.Adam(\n",
    "    discriminator.parameters(),\n",
    "    lr=training_config['lr_d'],\n",
    "    betas=(training_config['beta1'], training_config['beta2'])\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "gen_params = count_parameters(generator)\n",
    "disc_params = count_parameters(discriminator)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ MODELS INITIALIZED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\\\nüî∑ Generator:\")\n",
    "print(f\"   Parameters: {gen_params:,} ({gen_params/1e6:.2f}M)\")\n",
    "print(f\"   Architecture: U-Net with self-attention\")\n",
    "\n",
    "print(f\"\\\\nüî∂ Discriminator:\")\n",
    "print(f\"   Parameters: {disc_params:,} ({disc_params/1e6:.2f}M)\")\n",
    "print(f\"   Architecture: PatchGAN with spectral normalization\")\n",
    "\n",
    "print(f\"\\\\nüíæ Total Parameters: {gen_params + disc_params:,} ({(gen_params + disc_params)/1e6:.2f}M)\")\n",
    "\n",
    "print(f\"\\\\n‚öôÔ∏è Optimizer: Adam\")\n",
    "print(f\"   Generator LR: {training_config['lr_g']}\")\n",
    "print(f\"   Discriminator LR: {training_config['lr_d']}\")\n",
    "print(f\"   Betas: ({training_config['beta1']}, {training_config['beta2']})\")\n",
    "\n",
    "print(f\"\\\\nüìä Loss Functions:\")\n",
    "print(f\"   GAN loss weight: {loss_config['loss_weights']['lambda_gan']}\")\n",
    "print(f\"   Perceptual loss weight: {loss_config['loss_weights']['lambda_perceptual']}\")\n",
    "print(f\"   L1 loss weight: {loss_config['loss_weights']['lambda_l1']}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837dc074",
   "metadata": {},
   "source": [
    "## 9. Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(generator, discriminator, train_loader, optimizer_g, optimizer_d, \n",
    "                    criterion, device, epoch, config):\n",
    "    \"\"\"\n",
    "    Train for one epoch.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with average losses for the epoch\n",
    "    \"\"\"\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    # Metrics tracking\n",
    "    metrics = defaultdict(list)\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move data to device\n",
    "        multi_channel_input = batch['multi_channel_input'].to(device)\n",
    "        target_image = batch['target_image'].to(device)\n",
    "        cloth_condition = batch['cloth_image'].to(device)\n",
    "        \n",
    "        batch_size = multi_channel_input.size(0)\n",
    "        \n",
    "        # ==================== Train Discriminator ====================\n",
    "        optimizer_d.zero_grad()\n",
    "        \n",
    "        # Generate fake images\n",
    "        with torch.no_grad():\n",
    "            fake_image = generator(multi_channel_input)\n",
    "        \n",
    "        # Discriminator on real\n",
    "        disc_real = discriminator(target_image, cloth_condition)\n",
    "        \n",
    "        # Discriminator on fake\n",
    "        disc_fake = discriminator(fake_image.detach(), cloth_condition)\n",
    "        \n",
    "        # Compute discriminator loss\n",
    "        d_losses = criterion.compute_discriminator_loss(disc_real, disc_fake)\n",
    "        d_loss = d_losses['total']\n",
    "        \n",
    "        # Backward and optimize\n",
    "        d_loss.backward()\n",
    "        if config['gradient_clip'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(discriminator.parameters(), config['gradient_clip'])\n",
    "        optimizer_d.step()\n",
    "        \n",
    "        # ==================== Train Generator ====================\n",
    "        optimizer_g.zero_grad()\n",
    "        \n",
    "        # Generate fake images\n",
    "        fake_image = generator(multi_channel_input)\n",
    "        \n",
    "        # Discriminator on fake (for generator)\n",
    "        disc_fake = discriminator(fake_image, cloth_condition)\n",
    "        \n",
    "        # Compute generator loss\n",
    "        g_losses = criterion.compute_generator_loss(fake_image, target_image, disc_fake)\n",
    "        g_loss = g_losses['total']\n",
    "        \n",
    "        # Backward and optimize\n",
    "        g_loss.backward()\n",
    "        if config['gradient_clip'] > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(generator.parameters(), config['gradient_clip'])\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        # ==================== Log Metrics ====================\n",
    "        metrics['g_loss'].append(g_loss.item())\n",
    "        metrics['g_gan'].append(g_losses['gan'].item())\n",
    "        metrics['g_perceptual'].append(g_losses['perceptual'].item())\n",
    "        metrics['g_l1'].append(g_losses['l1'].item())\n",
    "        metrics['d_loss'].append(d_loss.item())\n",
    "        metrics['d_real'].append(d_losses['real'].item())\n",
    "        metrics['d_fake'].append(d_losses['fake'].item())\n",
    "        \n",
    "        # Update progress bar\n",
    "        if batch_idx % config['log_every'] == 0:\n",
    "            pbar.set_postfix({\n",
    "                'G_loss': f\"{g_loss.item():.4f}\",\n",
    "                'D_loss': f\"{d_loss.item():.4f}\",\n",
    "                'G_GAN': f\"{g_losses['gan'].item():.4f}\",\n",
    "                'G_Perc': f\"{g_losses['perceptual'].item():.4f}\"\n",
    "            })\n",
    "    \n",
    "    # Compute average metrics\n",
    "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "def validate(generator, discriminator, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with average validation losses\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    metrics = defaultdict(list)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            # Move data to device\n",
    "            multi_channel_input = batch['multi_channel_input'].to(device)\n",
    "            target_image = batch['target_image'].to(device)\n",
    "            cloth_condition = batch['cloth_image'].to(device)\n",
    "            \n",
    "            # Generate fake images\n",
    "            fake_image = generator(multi_channel_input)\n",
    "            \n",
    "            # Discriminator outputs\n",
    "            disc_real = discriminator(target_image, cloth_condition)\n",
    "            disc_fake = discriminator(fake_image, cloth_condition)\n",
    "            \n",
    "            # Compute losses\n",
    "            g_losses = criterion.compute_generator_loss(fake_image, target_image, disc_fake)\n",
    "            d_losses = criterion.compute_discriminator_loss(disc_real, disc_fake)\n",
    "            \n",
    "            # Log metrics\n",
    "            metrics['g_loss'].append(g_losses['total'].item())\n",
    "            metrics['g_gan'].append(g_losses['gan'].item())\n",
    "            metrics['g_perceptual'].append(g_losses['perceptual'].item())\n",
    "            metrics['g_l1'].append(g_losses['l1'].item())\n",
    "            metrics['d_loss'].append(d_losses['total'].item())\n",
    "    \n",
    "    # Compute average metrics\n",
    "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training and validation functions defined\")\n",
    "print(\"   - train_one_epoch(): Trains for one epoch with progress bar\")\n",
    "print(\"   - validate(): Validates model on validation set\")\n",
    "print(\"   - Includes gradient clipping and metric tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a041d1",
   "metadata": {},
   "source": [
    "## 10. Checkpointing and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(generator, discriminator, optimizer_g, optimizer_d, epoch, \n",
    "                   metrics, checkpoint_dir, is_best=False, config=None):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
    "        'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
    "        'metrics': metrics,\n",
    "        'config': config\n",
    "    }\n",
    "    \n",
    "    # Save regular checkpoint\n",
    "    checkpoint_path = checkpoint_dir / f'checkpoint_epoch_{epoch:03d}.pth'\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"üíæ Saved checkpoint: {checkpoint_path.name}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if is_best:\n",
    "        best_path = checkpoint_dir / 'best_model.pth'\n",
    "        torch.save(checkpoint, best_path)\n",
    "        print(f\"üåü Saved best model: {best_path.name}\")\n",
    "    \n",
    "    # Keep only max_checkpoints recent checkpoints\n",
    "    if config and 'max_checkpoints' in config:\n",
    "        checkpoints = sorted(checkpoint_dir.glob('checkpoint_epoch_*.pth'))\n",
    "        if len(checkpoints) > config['max_checkpoints']:\n",
    "            for old_checkpoint in checkpoints[:-config['max_checkpoints']]:\n",
    "                old_checkpoint.unlink()\n",
    "                print(f\"üóëÔ∏è  Removed old checkpoint: {old_checkpoint.name}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, generator, discriminator, optimizer_g=None, \n",
    "                   optimizer_d=None, device='cuda'):\n",
    "    \"\"\"Load model checkpoint.\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    \n",
    "    if optimizer_g is not None:\n",
    "        optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n",
    "    if optimizer_d is not None:\n",
    "        optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
    "    \n",
    "    epoch = checkpoint['epoch']\n",
    "    metrics = checkpoint.get('metrics', {})\n",
    "    \n",
    "    print(f\"‚úÖ Loaded checkpoint from epoch {epoch}\")\n",
    "    return epoch, metrics\n",
    "\n",
    "\n",
    "def save_sample_images(generator, val_loader, epoch, output_dir, device, num_samples=4):\n",
    "    \"\"\"Save sample generated images.\"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get first batch\n",
    "        batch = next(iter(val_loader))\n",
    "        multi_channel_input = batch['multi_channel_input'][:num_samples].to(device)\n",
    "        target_image = batch['target_image'][:num_samples].to(device)\n",
    "        \n",
    "        # Generate images\n",
    "        fake_image = generator(multi_channel_input)\n",
    "        \n",
    "        # Denormalize images (from [-1, 1] to [0, 1])\n",
    "        fake_image = (fake_image + 1) / 2\n",
    "        target_image = (target_image + 1) / 2\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(2, num_samples, figsize=(num_samples * 4, 8))\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Target image\n",
    "            target_np = target_image[i].cpu().permute(1, 2, 0).numpy()\n",
    "            axes[0, i].imshow(target_np)\n",
    "            axes[0, i].set_title(f'Target {i+1}', fontsize=12)\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            # Generated image\n",
    "            fake_np = fake_image[i].cpu().permute(1, 2, 0).numpy()\n",
    "            axes[1, i].imshow(fake_np)\n",
    "            axes[1, i].set_title(f'Generated {i+1}', fontsize=12)\n",
    "            axes[1, i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = output_dir / f'samples_epoch_{epoch:03d}.png'\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"üñºÔ∏è  Saved sample images: {save_path.name}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Checkpoint and utility functions defined\")\n",
    "print(\"   - save_checkpoint(): Save model state\")\n",
    "print(\"   - load_checkpoint(): Load model state\")\n",
    "print(\"   - save_sample_images(): Save generated image samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ffde89",
   "metadata": {},
   "source": [
    "## 11. Test Training Loop (1 Epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27c4582",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üöÄ TESTING TRAINING LOOP\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\\\nTraining for 1 epoch to test the pipeline...\")\n",
    "print(\"This will verify that all components work together correctly.\\\\n\")\n",
    "\n",
    "# Test training for 1 epoch\n",
    "epoch = 1\n",
    "start_time = time.time()\n",
    "\n",
    "# Train\n",
    "train_metrics = train_one_epoch(\n",
    "    generator, discriminator, train_loader, \n",
    "    optimizer_g, optimizer_d, criterion, \n",
    "    device, epoch, training_config\n",
    ")\n",
    "\n",
    "# Validate\n",
    "val_metrics = validate(generator, discriminator, val_loader, criterion, device)\n",
    "\n",
    "# Save sample images\n",
    "samples_dir = output_dir / 'samples'\n",
    "samples_dir.mkdir(exist_ok=True)\n",
    "save_sample_images(generator, val_loader, epoch, samples_dir, device, num_samples=4)\n",
    "\n",
    "epoch_time = time.time() - start_time\n",
    "\n",
    "# Print results\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"üìä EPOCH 1 RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\\\nüî∑ Training Metrics:\")\n",
    "print(f\"   Generator Loss: {train_metrics['g_loss']:.4f}\")\n",
    "print(f\"   - GAN Loss: {train_metrics['g_gan']:.4f}\")\n",
    "print(f\"   - Perceptual Loss: {train_metrics['g_perceptual']:.4f}\")\n",
    "print(f\"   - L1 Loss: {train_metrics['g_l1']:.4f}\")\n",
    "print(f\"   Discriminator Loss: {train_metrics['d_loss']:.4f}\")\n",
    "print(f\"   - Real Loss: {train_metrics['d_real']:.4f}\")\n",
    "print(f\"   - Fake Loss: {train_metrics['d_fake']:.4f}\")\n",
    "\n",
    "print(f\"\\\\nüî∂ Validation Metrics:\")\n",
    "print(f\"   Generator Loss: {val_metrics['g_loss']:.4f}\")\n",
    "print(f\"   - GAN Loss: {val_metrics['g_gan']:.4f}\")\n",
    "print(f\"   - Perceptual Loss: {val_metrics['g_perceptual']:.4f}\")\n",
    "print(f\"   - L1 Loss: {val_metrics['g_l1']:.4f}\")\n",
    "print(f\"   Discriminator Loss: {val_metrics['d_loss']:.4f}\")\n",
    "\n",
    "print(f\"\\\\n‚è±Ô∏è  Epoch Time: {epoch_time:.2f}s ({epoch_time/60:.2f}min)\")\n",
    "print(f\"‚è±Ô∏è  Estimated time for 100 epochs: {epoch_time * 100 / 3600:.2f}h\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"\\\\n‚úÖ TRAINING LOOP TEST SUCCESSFUL!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b83e6",
   "metadata": {},
   "source": [
    "## 12. Save Test Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd13bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoint after test epoch\n",
    "combined_metrics = {\n",
    "    'train': train_metrics,\n",
    "    'val': val_metrics,\n",
    "    'epoch_time': epoch_time\n",
    "}\n",
    "\n",
    "save_checkpoint(\n",
    "    generator, discriminator, \n",
    "    optimizer_g, optimizer_d,\n",
    "    epoch=1,\n",
    "    metrics=combined_metrics,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    is_best=True,\n",
    "    config=training_config\n",
    ")\n",
    "\n",
    "print(\"\\\\n‚úÖ Test checkpoint saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ecd7af",
   "metadata": {},
   "source": [
    "## 13. Training Configuration Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete training configuration\n",
    "full_training_config = {\n",
    "    'model': {\n",
    "        'generator': {\n",
    "            'architecture': 'U-Net with self-attention',\n",
    "            'input_channels': 41,\n",
    "            'output_channels': 3,\n",
    "            'ngf': 64,\n",
    "            'num_downs': 4,\n",
    "            'num_res_blocks': 9,\n",
    "            'parameters': gen_params\n",
    "        },\n",
    "        'discriminator': {\n",
    "            'architecture': 'PatchGAN with spectral normalization',\n",
    "            'input_channels': 6,\n",
    "            'ndf': 64,\n",
    "            'n_layers': 3,\n",
    "            'parameters': disc_params\n",
    "        }\n",
    "    },\n",
    "    'training': training_config,\n",
    "    'loss': loss_config,\n",
    "    'test_results': {\n",
    "        'epoch': 1,\n",
    "        'train_metrics': {k: float(v) for k, v in train_metrics.items()},\n",
    "        'val_metrics': {k: float(v) for k, v in val_metrics.items()},\n",
    "        'epoch_time_seconds': epoch_time\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_save_path = output_dir / 'training_config.json'\n",
    "with open(config_save_path, 'w') as f:\n",
    "    json.dump(full_training_config, f, indent=2)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üíæ TRAINING CONFIGURATION SAVED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\\\nüìÑ Config saved to: {config_save_path}\")\n",
    "\n",
    "# List generated files\n",
    "print(f\"\\\\nüìÅ Generated Files:\")\n",
    "checkpoint_files = list(checkpoint_dir.glob('*.pth'))\n",
    "sample_files = list(samples_dir.glob('*.png'))\n",
    "\n",
    "for f in checkpoint_files:\n",
    "    print(f\"   ‚úì {f.relative_to(output_dir)}\")\n",
    "for f in sample_files:\n",
    "    print(f\"   ‚úì {f.relative_to(output_dir)}\")\n",
    "print(f\"   ‚úì {config_save_path.relative_to(output_dir)}\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d44e8d",
   "metadata": {},
   "source": [
    "## 14. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d917a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üéâ TRAINING LOOP COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\\\n‚úÖ Completed Tasks:\")\n",
    "print(\"   1. ‚úì Loaded model architectures (Generator & Discriminator)\")\n",
    "print(\"   2. ‚úì Loaded loss functions (GAN, Perceptual, L1)\")\n",
    "print(\"   3. ‚úì Created dummy dataset for testing\")\n",
    "print(\"   4. ‚úì Initialized models and optimizers\")\n",
    "print(\"   5. ‚úì Implemented train_one_epoch() function\")\n",
    "print(\"   6. ‚úì Implemented validate() function\")\n",
    "print(\"   7. ‚úì Implemented checkpoint saving/loading\")\n",
    "print(\"   8. ‚úì Tested complete training pipeline for 1 epoch\")\n",
    "print(\"   9. ‚úì Generated sample images\")\n",
    "print(\"   10. ‚úì Saved configuration and checkpoints\")\n",
    "\n",
    "print(f\"\\\\nüìä Training Pipeline Status:\")\n",
    "print(f\"   üî∑ Generator: {gen_params/1e6:.2f}M parameters\")\n",
    "print(f\"   üî∂ Discriminator: {disc_params/1e6:.2f}M parameters\")\n",
    "print(f\"   ‚è±Ô∏è  1 Epoch Time: {epoch_time:.2f}s\")\n",
    "print(f\"   üì¶ Train batches: {len(train_loader)}\")\n",
    "print(f\"   üì¶ Val batches: {len(val_loader)}\")\n",
    "\n",
    "print(f\"\\\\nüìà Test Results (1 Epoch):\")\n",
    "print(f\"   Training:\")\n",
    "print(f\"   - Generator Loss: {train_metrics['g_loss']:.4f}\")\n",
    "print(f\"   - Discriminator Loss: {train_metrics['d_loss']:.4f}\")\n",
    "print(f\"   Validation:\")\n",
    "print(f\"   - Generator Loss: {val_metrics['g_loss']:.4f}\")\n",
    "print(f\"   - Discriminator Loss: {val_metrics['d_loss']:.4f}\")\n",
    "\n",
    "print(f\"\\\\nüìÅ Output Files:\")\n",
    "print(f\"   - Checkpoints: {checkpoint_dir}\")\n",
    "print(f\"   - Sample images: {samples_dir}\")\n",
    "print(f\"   - Configuration: {config_save_path}\")\n",
    "\n",
    "print(\"\\\\nüöÄ Ready for Full Training!\")\n",
    "print(\"\\\\nüí° Next Steps:\")\n",
    "print(\"   1. Replace DummyVITONDataset with actual VITON-HD dataset\")\n",
    "print(\"   2. Add TensorBoard logging for real-time monitoring\")\n",
    "print(\"   3. Implement learning rate scheduling\")\n",
    "print(\"   4. Add early stopping based on validation loss\")\n",
    "print(\"   5. Train for full 100 epochs\")\n",
    "print(\"   6. Evaluate on test set\")\n",
    "print(\"   7. Implement inference pipeline\")\n",
    "\n",
    "print(\"\\\\n‚ö†Ô∏è  Important Notes:\")\n",
    "print(\"   - Currently using dummy data for pipeline testing\")\n",
    "print(\"   - Replace with actual dataset from Notebooks 03-07\")\n",
    "print(\"   - Adjust batch size based on GPU memory\")\n",
    "print(\"   - Monitor GPU memory usage during training\")\n",
    "print(\"   - Use gradient accumulation if batch size is too small\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"\\\\n‚úÖ TRAINING LOOP IMPLEMENTATION COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
