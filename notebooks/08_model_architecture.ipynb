{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece62349",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e974c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torchsummary import summary\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502f785b",
   "metadata": {},
   "source": [
    "## 2. Setup Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d3a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project paths\n",
    "project_root = Path(r'd:\\Projects\\AI-Virtual-TryOn')\n",
    "output_dir = project_root / 'outputs' / 'model_architecture'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load dataset configuration\n",
    "with open(project_root / 'outputs' / 'dataset' / 'dataset_config.json', 'r') as f:\n",
    "    dataset_config = json.load(f)\n",
    "\n",
    "print(f\"üìÅ Project Root: {project_root}\")\n",
    "print(f\"üìÅ Output Directory: {output_dir}\")\n",
    "print(f\"\\n‚úÖ Loaded dataset configuration\")\n",
    "\n",
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nüñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "# Model configuration\n",
    "model_config = {\n",
    "    'input_channels': 41,  # 3 CA + 18 Pose + 20 Parsing\n",
    "    'output_channels': 3,  # RGB\n",
    "    'image_size': (1024, 768),\n",
    "    'ngf': 64,  # Generator base channels\n",
    "    'ndf': 64,  # Discriminator base channels\n",
    "    'n_downsampling': 4,\n",
    "    'n_blocks': 9,  # Residual blocks at bottleneck\n",
    "    'use_dropout': True,\n",
    "    'use_attention': True\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Model Configuration:\")\n",
    "for key, value in model_config.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a273f1a",
   "metadata": {},
   "source": [
    "## 3. Building Blocks\n",
    "\n",
    "Define reusable components for the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11649117",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with two convolutional layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int, use_dropout: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        \n",
    "        if use_dropout:\n",
    "            layers.append(nn.Dropout(0.5))\n",
    "        \n",
    "        layers += [\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        ]\n",
    "        \n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention module for capturing long-range dependencies.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.query = nn.Conv2d(channels, channels // 8, kernel_size=1)\n",
    "        self.key = nn.Conv2d(channels, channels // 8, kernel_size=1)\n",
    "        self.value = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch, channels, height, width = x.size()\n",
    "        \n",
    "        # Query, Key, Value\n",
    "        q = self.query(x).view(batch, -1, height * width).permute(0, 2, 1)\n",
    "        k = self.key(x).view(batch, -1, height * width)\n",
    "        v = self.value(x).view(batch, -1, height * width)\n",
    "        \n",
    "        # Attention map\n",
    "        attention = F.softmax(torch.bmm(q, k), dim=-1)\n",
    "        \n",
    "        # Apply attention\n",
    "        out = torch.bmm(v, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch, channels, height, width)\n",
    "        \n",
    "        return self.gamma * out + x\n",
    "\n",
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Downsampling block for encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, \n",
    "                 normalize: bool = True, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=4, \n",
    "                     stride=2, padding=1, bias=False)\n",
    "        ]\n",
    "        \n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        \n",
    "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling block for decoder with skip connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4,\n",
    "                              stride=2, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        \n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, skip: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        x = self.block(x)\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"‚úÖ Building blocks defined:\")\n",
    "print(\"   - ResidualBlock\")\n",
    "print(\"   - SelfAttention\")\n",
    "print(\"   - DownsampleBlock\")\n",
    "print(\"   - UpsampleBlock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af404ce",
   "metadata": {},
   "source": [
    "## 4. Generator Network\n",
    "\n",
    "U-Net architecture with attention and residual connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd42e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net based Generator for Virtual Try-On.\n",
    "    \n",
    "    Input: [B, 41, H, W] (cloth-agnostic + pose + parsing)\n",
    "    Output: [B, 3, H, W] (RGB image)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels: int = 41,\n",
    "                 out_channels: int = 3,\n",
    "                 ngf: int = 64,\n",
    "                 n_downsampling: int = 4,\n",
    "                 n_blocks: int = 9,\n",
    "                 use_dropout: bool = True,\n",
    "                 use_attention: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_downsampling = n_downsampling\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, ngf, kernel_size=7, padding=3, bias=False),\n",
    "            nn.InstanceNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Encoder (downsampling)\n",
    "        self.encoder = nn.ModuleList()\n",
    "        mult = 1\n",
    "        for i in range(n_downsampling):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** (i + 1), 8)\n",
    "            self.encoder.append(\n",
    "                DownsampleBlock(\n",
    "                    ngf * mult_prev,\n",
    "                    ngf * mult,\n",
    "                    normalize=True,\n",
    "                    dropout=0.0\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Bottleneck (residual blocks)\n",
    "        bottleneck_channels = ngf * mult\n",
    "        self.bottleneck = nn.ModuleList()\n",
    "        for _ in range(n_blocks):\n",
    "            self.bottleneck.append(\n",
    "                ResidualBlock(bottleneck_channels, use_dropout=use_dropout)\n",
    "            )\n",
    "        \n",
    "        # Self-attention at bottleneck\n",
    "        self.attention = SelfAttention(bottleneck_channels) if use_attention else None\n",
    "        \n",
    "        # Decoder (upsampling)\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for i in range(n_downsampling):\n",
    "            mult_prev = min(2 ** (n_downsampling - i), 8)\n",
    "            mult = min(2 ** (n_downsampling - i - 1), 8)\n",
    "            \n",
    "            # Account for skip connections (double the input channels)\n",
    "            in_ch = ngf * mult_prev\n",
    "            out_ch = ngf * mult\n",
    "            \n",
    "            self.decoder.append(\n",
    "                UpsampleBlock(\n",
    "                    in_ch * 2,  # *2 for skip connection\n",
    "                    out_ch,\n",
    "                    dropout=0.5 if i < 3 and use_dropout else 0.0\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Final convolution\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(ngf * 2, ngf, kernel_size=3, padding=1, bias=False),  # *2 for skip\n",
    "            nn.InstanceNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ngf, out_channels, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through Generator.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [B, 41, H, W]\n",
    "        \n",
    "        Returns:\n",
    "            Generated image [B, 3, H, W]\n",
    "        \"\"\"\n",
    "        # Initial convolution\n",
    "        x = self.initial(x)\n",
    "        \n",
    "        # Encoder with skip connections\n",
    "        skip_connections = [x]\n",
    "        for encoder_block in self.encoder:\n",
    "            x = encoder_block(x)\n",
    "            skip_connections.append(x)\n",
    "        \n",
    "        # Bottleneck\n",
    "        for res_block in self.bottleneck:\n",
    "            x = res_block(x)\n",
    "        \n",
    "        # Attention\n",
    "        if self.attention is not None:\n",
    "            x = self.attention(x)\n",
    "        \n",
    "        # Decoder with skip connections\n",
    "        skip_connections = skip_connections[::-1]  # Reverse order\n",
    "        for i, decoder_block in enumerate(self.decoder):\n",
    "            skip = skip_connections[i]\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = decoder_block(x)\n",
    "        \n",
    "        # Final convolution with last skip connection\n",
    "        x = torch.cat([x, skip_connections[-1]], dim=1)\n",
    "        x = self.final(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"‚úÖ Generator network defined\")\n",
    "print(\"   Architecture: U-Net with attention\")\n",
    "print(\"   Input: [B, 41, H, W]\")\n",
    "print(\"   Output: [B, 3, H, W]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b0c5c",
   "metadata": {},
   "source": [
    "## 5. Discriminator Network\n",
    "\n",
    "PatchGAN discriminator with spectral normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb0b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchGAN Discriminator for Virtual Try-On.\n",
    "    \n",
    "    Classifies 70x70 patches as real or fake.\n",
    "    Uses spectral normalization for training stability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 in_channels: int = 6,  # 3 (image) + 3 (condition)\n",
    "                 ndf: int = 64,\n",
    "                 n_layers: int = 3,\n",
    "                 use_spectral_norm: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        norm_layer = spectral_norm if use_spectral_norm else lambda x: x\n",
    "        \n",
    "        # Initial layer (no normalization)\n",
    "        layers = [\n",
    "            norm_layer(nn.Conv2d(in_channels, ndf, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        # Intermediate layers\n",
    "        mult = 1\n",
    "        for n in range(1, n_layers):\n",
    "            mult_prev = mult\n",
    "            mult = min(2 ** n, 8)\n",
    "            layers += [\n",
    "                norm_layer(nn.Conv2d(ndf * mult_prev, ndf * mult, \n",
    "                                    kernel_size=4, stride=2, padding=1, bias=False)),\n",
    "                nn.InstanceNorm2d(ndf * mult),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ]\n",
    "        \n",
    "        # Final layers\n",
    "        mult_prev = mult\n",
    "        mult = min(2 ** n_layers, 8)\n",
    "        layers += [\n",
    "            norm_layer(nn.Conv2d(ndf * mult_prev, ndf * mult,\n",
    "                                kernel_size=4, stride=1, padding=1, bias=False)),\n",
    "            nn.InstanceNorm2d(ndf * mult),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(\n",
    "            norm_layer(nn.Conv2d(ndf * mult, 1, kernel_size=4, stride=1, padding=1))\n",
    "        )\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, condition: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through Discriminator.\n",
    "        \n",
    "        Args:\n",
    "            x: Input image [B, 3, H, W]\n",
    "            condition: Condition image [B, 3, H, W] (e.g., cloth-agnostic)\n",
    "        \n",
    "        Returns:\n",
    "            Patch predictions [B, 1, H', W']\n",
    "        \"\"\"\n",
    "        # Concatenate image and condition\n",
    "        x = torch.cat([x, condition], dim=1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Discriminator network defined\")\n",
    "print(\"   Architecture: PatchGAN with spectral normalization\")\n",
    "print(\"   Input: [B, 6, H, W] (image + condition)\")\n",
    "print(\"   Output: [B, 1, H', W'] (patch predictions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f6b9e",
   "metadata": {},
   "source": [
    "## 6. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e83c78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Generator\n",
    "generator = Generator(\n",
    "    in_channels=model_config['input_channels'],\n",
    "    out_channels=model_config['output_channels'],\n",
    "    ngf=model_config['ngf'],\n",
    "    n_downsampling=model_config['n_downsampling'],\n",
    "    n_blocks=model_config['n_blocks'],\n",
    "    use_dropout=model_config['use_dropout'],\n",
    "    use_attention=model_config['use_attention']\n",
    ").to(device)\n",
    "\n",
    "# Initialize Discriminator\n",
    "discriminator = Discriminator(\n",
    "    in_channels=6,  # RGB image + RGB condition\n",
    "    ndf=model_config['ndf'],\n",
    "    n_layers=3,\n",
    "    use_spectral_norm=True\n",
    ").to(device)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ MODELS INITIALIZED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n‚úÖ Generator created and moved to {device}\")\n",
    "print(f\"‚úÖ Discriminator created and moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00cd91",
   "metadata": {},
   "source": [
    "## 7. Count Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fba615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Count total and trainable parameters.\n",
    "    \"\"\"\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "\n",
    "# Count parameters\n",
    "gen_total, gen_trainable = count_parameters(generator)\n",
    "disc_total, disc_trainable = count_parameters(discriminator)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìä PARAMETER COUNT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüî∑ Generator:\")\n",
    "print(f\"   Total parameters: {gen_total:,}\")\n",
    "print(f\"   Trainable parameters: {gen_trainable:,}\")\n",
    "print(f\"   Memory: {gen_total * 4 / 1024**2:.2f} MB (float32)\")\n",
    "\n",
    "print(f\"\\nüî∂ Discriminator:\")\n",
    "print(f\"   Total parameters: {disc_total:,}\")\n",
    "print(f\"   Trainable parameters: {disc_trainable:,}\")\n",
    "print(f\"   Memory: {disc_total * 4 / 1024**2:.2f} MB (float32)\")\n",
    "\n",
    "print(f\"\\nüìä Total:\")\n",
    "print(f\"   Combined parameters: {gen_total + disc_total:,}\")\n",
    "print(f\"   Combined memory: {(gen_total + disc_total) * 4 / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d980f21e",
   "metadata": {},
   "source": [
    "## 8. Test Forward Pass - Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060fa3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üß™ TESTING GENERATOR FORWARD PASS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create dummy input\n",
    "batch_size = 2\n",
    "dummy_input = torch.randn(batch_size, 41, 1024, 768).to(device)\n",
    "\n",
    "print(f\"\\nüì• Input shape: {tuple(dummy_input.shape)}\")\n",
    "print(f\"   Channels breakdown:\")\n",
    "print(f\"   - Cloth-agnostic RGB: 3\")\n",
    "print(f\"   - Pose heatmaps: 18\")\n",
    "print(f\"   - Parsing one-hot: 20\")\n",
    "print(f\"   Total: 41 channels\")\n",
    "\n",
    "# Forward pass\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    output = generator(dummy_input)\n",
    "\n",
    "print(f\"\\nüì§ Output shape: {tuple(output.shape)}\")\n",
    "print(f\"   Output range: [{output.min().item():.3f}, {output.max().item():.3f}]\")\n",
    "print(f\"   Expected range: [-1, 1] (Tanh activation)\")\n",
    "\n",
    "# Check output\n",
    "assert output.shape == (batch_size, 3, 1024, 768), \"Output shape mismatch!\"\n",
    "assert output.min() >= -1.0 and output.max() <= 1.0, \"Output range incorrect!\"\n",
    "\n",
    "print(f\"\\n‚úÖ Generator forward pass successful!\")\n",
    "print(f\"   Input: {tuple(dummy_input.shape)}\")\n",
    "print(f\"   Output: {tuple(output.shape)}\")\n",
    "\n",
    "# Memory usage\n",
    "if torch.cuda.is_available():\n",
    "    mem_allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "    mem_reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "    print(f\"\\nüíæ GPU Memory:\")\n",
    "    print(f\"   Allocated: {mem_allocated:.2f} MB\")\n",
    "    print(f\"   Reserved: {mem_reserved:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97c2dd6",
   "metadata": {},
   "source": [
    "## 9. Test Forward Pass - Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üß™ TESTING DISCRIMINATOR FORWARD PASS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create dummy inputs\n",
    "dummy_image = torch.randn(batch_size, 3, 1024, 768).to(device)\n",
    "dummy_condition = torch.randn(batch_size, 3, 1024, 768).to(device)\n",
    "\n",
    "print(f\"\\nüì• Input shapes:\")\n",
    "print(f\"   Image: {tuple(dummy_image.shape)}\")\n",
    "print(f\"   Condition: {tuple(dummy_condition.shape)}\")\n",
    "\n",
    "# Forward pass\n",
    "discriminator.eval()\n",
    "with torch.no_grad():\n",
    "    disc_output = discriminator(dummy_image, dummy_condition)\n",
    "\n",
    "print(f\"\\nüì§ Output shape: {tuple(disc_output.shape)}\")\n",
    "print(f\"   Output range: [{disc_output.min().item():.3f}, {disc_output.max().item():.3f}]\")\n",
    "\n",
    "# Calculate receptive field size\n",
    "patch_h, patch_w = disc_output.shape[2], disc_output.shape[3]\n",
    "print(f\"\\nüìä Patch predictions:\")\n",
    "print(f\"   Number of patches: {patch_h} √ó {patch_w} = {patch_h * patch_w}\")\n",
    "print(f\"   Each patch covers ~70√ó70 pixels\")\n",
    "\n",
    "print(f\"\\n‚úÖ Discriminator forward pass successful!\")\n",
    "print(f\"   Image: {tuple(dummy_image.shape)}\")\n",
    "print(f\"   Condition: {tuple(dummy_condition.shape)}\")\n",
    "print(f\"   Output: {tuple(disc_output.shape)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc113d",
   "metadata": {},
   "source": [
    "## 10. Test Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6250f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üîÑ TESTING FULL PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create dummy input\n",
    "print(\"\\nüì• Creating dummy inputs...\")\n",
    "multi_channel_input = torch.randn(batch_size, 41, 1024, 768).to(device)\n",
    "real_image = torch.randn(batch_size, 3, 1024, 768).to(device)\n",
    "condition = torch.randn(batch_size, 3, 1024, 768).to(device)\n",
    "\n",
    "# Generator forward pass\n",
    "print(\"\\nüî∑ Generator: Generating fake images...\")\n",
    "generator.eval()\n",
    "with torch.no_grad():\n",
    "    fake_image = generator(multi_channel_input)\n",
    "\n",
    "print(f\"   Input: {tuple(multi_channel_input.shape)}\")\n",
    "print(f\"   Output (fake): {tuple(fake_image.shape)}\")\n",
    "\n",
    "# Discriminator forward pass on fake\n",
    "print(\"\\nüî∂ Discriminator: Evaluating fake images...\")\n",
    "discriminator.eval()\n",
    "with torch.no_grad():\n",
    "    pred_fake = discriminator(fake_image, condition)\n",
    "\n",
    "print(f\"   Fake prediction: {tuple(pred_fake.shape)}\")\n",
    "print(f\"   Mean score: {pred_fake.mean().item():.3f}\")\n",
    "\n",
    "# Discriminator forward pass on real\n",
    "print(\"\\nüî∂ Discriminator: Evaluating real images...\")\n",
    "with torch.no_grad():\n",
    "    pred_real = discriminator(real_image, condition)\n",
    "\n",
    "print(f\"   Real prediction: {tuple(pred_real.shape)}\")\n",
    "print(f\"   Mean score: {pred_real.mean().item():.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Full pipeline test successful!\")\n",
    "print(\"\\nüìä Pipeline Summary:\")\n",
    "print(f\"   1. Multi-channel input [{batch_size}, 41, 1024, 768]\")\n",
    "print(f\"   2. Generator ‚Üí Fake image [{batch_size}, 3, 1024, 768]\")\n",
    "print(f\"   3. Discriminator(fake) ‚Üí Patches [{batch_size}, 1, {patch_h}, {patch_w}]\")\n",
    "print(f\"   4. Discriminator(real) ‚Üí Patches [{batch_size}, 1, {patch_h}, {patch_w}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac04a68",
   "metadata": {},
   "source": [
    "## 11. Model Architecture Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfaa571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_architecture():\n",
    "    \"\"\"\n",
    "    Create visual diagram of model architecture.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 12))\n",
    "    \n",
    "    # Generator U-Net visualization\n",
    "    ax1.set_title('Generator Architecture (U-Net)', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_xlim(0, 10)\n",
    "    ax1.set_ylim(0, 10)\n",
    "    \n",
    "    # Encoder path\n",
    "    encoder_colors = ['#e8f4f8', '#b8dfe6', '#88cad4', '#58b5c2']\n",
    "    encoder_positions = [(1, 8), (2, 7), (3, 6), (4, 5)]\n",
    "    encoder_sizes = [(0.8, 0.8), (0.7, 0.7), (0.6, 0.6), (0.5, 0.5)]\n",
    "    \n",
    "    # Input\n",
    "    ax1.add_patch(Rectangle((0.5, 8.5), 0.9, 0.9, \n",
    "                            facecolor='#c8e6c9', edgecolor='black', linewidth=2))\n",
    "    ax1.text(0.95, 9.0, 'Input\\n41ch', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Encoder blocks\n",
    "    for i, (pos, size, color) in enumerate(zip(encoder_positions, encoder_sizes, encoder_colors)):\n",
    "        channels = 64 * min(2**i, 8)\n",
    "        ax1.add_patch(Rectangle((pos[0]-size[0]/2, pos[1]-size[1]/2), size[0], size[1],\n",
    "                                facecolor=color, edgecolor='black', linewidth=2))\n",
    "        ax1.text(pos[0], pos[1], f'Down\\n{channels}ch', ha='center', va='center', \n",
    "                fontsize=9, fontweight='bold')\n",
    "        # Arrow\n",
    "        if i > 0:\n",
    "            ax1.arrow(encoder_positions[i-1][0], encoder_positions[i-1][1]-0.5,\n",
    "                     pos[0]-encoder_positions[i-1][0], pos[1]-encoder_positions[i-1][1]+0.5,\n",
    "                     head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "    \n",
    "    # Bottleneck\n",
    "    ax1.add_patch(Rectangle((4.25, 4.25), 0.5, 0.5,\n",
    "                            facecolor='#ff9999', edgecolor='black', linewidth=2))\n",
    "    ax1.text(4.5, 4.5, 'Bottleneck\\n512ch\\n9 blocks\\nAttention', \n",
    "            ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "    \n",
    "    # Decoder path\n",
    "    decoder_positions = [(6, 5), (7, 6), (8, 7), (9, 8)]\n",
    "    decoder_sizes = [(0.5, 0.5), (0.6, 0.6), (0.7, 0.7), (0.8, 0.8)]\n",
    "    \n",
    "    for i, (pos, size, color) in enumerate(zip(decoder_positions, decoder_sizes, \n",
    "                                               encoder_colors[::-1])):\n",
    "        channels = 64 * min(2**(3-i), 8)\n",
    "        ax1.add_patch(Rectangle((pos[0]-size[0]/2, pos[1]-size[1]/2), size[0], size[1],\n",
    "                                facecolor=color, edgecolor='black', linewidth=2))\n",
    "        ax1.text(pos[0], pos[1], f'Up\\n{channels}ch', ha='center', va='center',\n",
    "                fontsize=9, fontweight='bold')\n",
    "        # Arrow\n",
    "        if i > 0:\n",
    "            ax1.arrow(decoder_positions[i-1][0], decoder_positions[i-1][1]+0.5,\n",
    "                     pos[0]-decoder_positions[i-1][0], pos[1]-decoder_positions[i-1][1]-0.5,\n",
    "                     head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "    \n",
    "    # Output\n",
    "    ax1.add_patch(Rectangle((9.1, 8.5), 0.9, 0.9,\n",
    "                            facecolor='#fff9c4', edgecolor='black', linewidth=2))\n",
    "    ax1.text(9.55, 9.0, 'Output\\n3ch', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Skip connections (dashed lines)\n",
    "    for enc_pos, dec_pos in zip(encoder_positions, decoder_positions[::-1]):\n",
    "        ax1.plot([enc_pos[0], dec_pos[0]], [enc_pos[1], dec_pos[1]],\n",
    "                'r--', linewidth=1.5, alpha=0.6, label='Skip' if enc_pos == encoder_positions[0] else '')\n",
    "    \n",
    "    ax1.legend(loc='lower left', fontsize=10)\n",
    "    \n",
    "    # Discriminator visualization\n",
    "    ax2.set_title('Discriminator Architecture (PatchGAN)', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_xlim(0, 10)\n",
    "    ax2.set_ylim(0, 10)\n",
    "    \n",
    "    # Input images\n",
    "    ax2.add_patch(Rectangle((0.5, 8), 1.2, 1.5,\n",
    "                            facecolor='#c8e6c9', edgecolor='black', linewidth=2))\n",
    "    ax2.text(1.1, 8.75, 'Image\\n3ch', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax2.add_patch(Rectangle((2.3, 8), 1.2, 1.5,\n",
    "                            facecolor='#c8e6c9', edgecolor='black', linewidth=2))\n",
    "    ax2.text(2.9, 8.75, 'Condition\\n3ch', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Concatenate\n",
    "    ax2.add_patch(Rectangle((4.3, 8), 1.2, 1.5,\n",
    "                            facecolor='#ffe0b2', edgecolor='black', linewidth=2))\n",
    "    ax2.text(4.9, 8.75, 'Concat\\n6ch', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Conv layers\n",
    "    disc_colors = ['#b8dfe6', '#88cad4', '#58b5c2', '#2891a0']\n",
    "    disc_positions = [(2, 6), (4, 5), (6, 4), (8, 3)]\n",
    "    disc_channels = [64, 128, 256, 512]\n",
    "    \n",
    "    for i, (pos, color, ch) in enumerate(zip(disc_positions, disc_colors, disc_channels)):\n",
    "        size = 1.2 - i * 0.15\n",
    "        ax2.add_patch(Rectangle((pos[0]-size/2, pos[1]-size/2), size, size,\n",
    "                                facecolor=color, edgecolor='black', linewidth=2))\n",
    "        ax2.text(pos[0], pos[1], f'Conv\\n{ch}ch', ha='center', va='center',\n",
    "                fontsize=9, fontweight='bold')\n",
    "        # Arrow\n",
    "        if i > 0:\n",
    "            ax2.arrow(disc_positions[i-1][0], disc_positions[i-1][1]-0.7,\n",
    "                     pos[0]-disc_positions[i-1][0], pos[1]-disc_positions[i-1][1]+0.7,\n",
    "                     head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "    \n",
    "    # Output\n",
    "    ax2.add_patch(Rectangle((8.5, 1), 1.0, 1.0,\n",
    "                            facecolor='#fff9c4', edgecolor='black', linewidth=2))\n",
    "    ax2.text(9.0, 1.5, f'Patches\\n{patch_h}√ó{patch_w}', \n",
    "            ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'model_architecture.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Architecture visualization saved\")\n",
    "\n",
    "\n",
    "visualize_architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75006f6b",
   "metadata": {},
   "source": [
    "## 12. Save Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8526aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed model configuration\n",
    "architecture_config = {\n",
    "    'generator': {\n",
    "        'type': 'U-Net',\n",
    "        'input_channels': model_config['input_channels'],\n",
    "        'output_channels': model_config['output_channels'],\n",
    "        'base_channels': model_config['ngf'],\n",
    "        'n_downsampling': model_config['n_downsampling'],\n",
    "        'n_blocks': model_config['n_blocks'],\n",
    "        'use_dropout': model_config['use_dropout'],\n",
    "        'use_attention': model_config['use_attention'],\n",
    "        'total_parameters': gen_total,\n",
    "        'trainable_parameters': gen_trainable,\n",
    "        'memory_mb': gen_total * 4 / 1024**2\n",
    "    },\n",
    "    'discriminator': {\n",
    "        'type': 'PatchGAN',\n",
    "        'input_channels': 6,\n",
    "        'base_channels': model_config['ndf'],\n",
    "        'n_layers': 3,\n",
    "        'use_spectral_norm': True,\n",
    "        'patch_size': '70x70',\n",
    "        'output_patches': f'{patch_h}x{patch_w}',\n",
    "        'total_parameters': disc_total,\n",
    "        'trainable_parameters': disc_trainable,\n",
    "        'memory_mb': disc_total * 4 / 1024**2\n",
    "    },\n",
    "    'total': {\n",
    "        'combined_parameters': gen_total + disc_total,\n",
    "        'combined_memory_mb': (gen_total + disc_total) * 4 / 1024**2\n",
    "    },\n",
    "    'image_size': model_config['image_size'],\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = output_dir / 'model_architecture_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(architecture_config, f, indent=2)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üíæ MODEL CONFIGURATION SAVED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìÑ Config saved to: {config_path}\")\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   - Generator parameters: {gen_total:,}\")\n",
    "print(f\"   - Discriminator parameters: {disc_total:,}\")\n",
    "print(f\"   - Total parameters: {gen_total + disc_total:,}\")\n",
    "print(f\"   - Total memory: {(gen_total + disc_total) * 4 / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21c5b61",
   "metadata": {},
   "source": [
    "## 13. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a65d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üéâ MODEL ARCHITECTURE COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ Completed Tasks:\")\n",
    "print(\"   1. ‚úì Implemented building blocks (ResidualBlock, Attention, etc.)\")\n",
    "print(\"   2. ‚úì Created Generator (U-Net with attention)\")\n",
    "print(\"   3. ‚úì Created Discriminator (PatchGAN with spectral norm)\")\n",
    "print(\"   4. ‚úì Initialized both models on GPU\")\n",
    "print(\"   5. ‚úì Counted parameters and memory\")\n",
    "print(\"   6. ‚úì Tested Generator forward pass\")\n",
    "print(\"   7. ‚úì Tested Discriminator forward pass\")\n",
    "print(\"   8. ‚úì Tested full pipeline\")\n",
    "print(\"   9. ‚úì Visualized architecture\")\n",
    "print(\"   10. ‚úì Saved model configuration\")\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   üî∑ Generator:\")\n",
    "print(f\"      - Architecture: U-Net with self-attention\")\n",
    "print(f\"      - Input: [B, 41, 1024, 768]\")\n",
    "print(f\"      - Output: [B, 3, 1024, 768]\")\n",
    "print(f\"      - Parameters: {gen_total:,}\")\n",
    "print(f\"      - Memory: {gen_total * 4 / 1024**2:.2f} MB\")\n",
    "print(f\"   üî∂ Discriminator:\")\n",
    "print(f\"      - Architecture: PatchGAN with spectral normalization\")\n",
    "print(f\"      - Input: [B, 6, 1024, 768] (image + condition)\")\n",
    "print(f\"      - Output: [B, 1, {patch_h}, {patch_w}] (patch predictions)\")\n",
    "print(f\"      - Parameters: {disc_total:,}\")\n",
    "print(f\"      - Memory: {disc_total * 4 / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "print(f\"   - model_architecture.png\")\n",
    "print(f\"   - model_architecture_config.json\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for Next Steps:\")\n",
    "print(\"   1. Define loss functions (GAN, Perceptual, L1)\")\n",
    "print(\"   2. Implement training loop\")\n",
    "print(\"   3. Add checkpointing and logging\")\n",
    "print(\"   4. Start model training\")\n",
    "\n",
    "print(\"\\nüí° Key Features:\")\n",
    "print(\"   - U-Net encoder-decoder architecture\")\n",
    "print(\"   - Skip connections for feature preservation\")\n",
    "print(\"   - Self-attention at bottleneck\")\n",
    "print(\"   - Residual blocks for stability\")\n",
    "print(\"   - PatchGAN discriminator (70√ó70)\")\n",
    "print(\"   - Spectral normalization for training stability\")\n",
    "print(\"   - Instance normalization throughout\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ MODEL ARCHITECTURE READY FOR TRAINING!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
