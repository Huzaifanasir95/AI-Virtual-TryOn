{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Notebook 01: Environment Setup & Configuration\n",
    "\n",
    "**AI Virtual Try-On System - Hybrid Generative AI Approach**\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Notebook Overview\n",
    "\n",
    "This notebook covers the complete environment setup for the AI Virtual Try-On project:\n",
    "\n",
    "1. **System Requirements Check** - Verify GPU, CUDA, and system specifications\n",
    "2. **Python Environment Setup** - Install all required dependencies\n",
    "3. **Directory Structure Verification** - Ensure all folders are created\n",
    "4. **GPU Configuration** - Configure PyTorch for GPU acceleration\n",
    "5. **Download Pretrained Models** - Download base models (Stable Diffusion, ControlNet)\n",
    "6. **Test Installation** - Verify all components are working\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è System Requirements\n",
    "\n",
    "**Minimum Requirements:**\n",
    "- GPU: NVIDIA GPU with 12GB+ VRAM (RTX 3060 or better)\n",
    "- RAM: 16GB minimum, 32GB recommended\n",
    "- Storage: 50GB+ free space\n",
    "- CUDA: 11.8+ and cuDNN 8.6+\n",
    "- Python: 3.10 or 3.11\n",
    "\n",
    "**Recommended:**\n",
    "- GPU: RTX 3090 (24GB) or RTX 4090 (24GB)\n",
    "- RAM: 32GB+\n",
    "- Storage: 100GB+ SSD\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ System Requirements Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üñ•Ô∏è  SYSTEM INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Python version\n",
    "print(f\"\\nüìå Python Version: {sys.version}\")\n",
    "print(f\"üìå Python Executable: {sys.executable}\")\n",
    "\n",
    "# Operating System\n",
    "print(f\"\\nüìå Operating System: {platform.system()} {platform.release()}\")\n",
    "print(f\"üìå Platform: {platform.platform()}\")\n",
    "print(f\"üìå Processor: {platform.processor()}\")\n",
    "\n",
    "# Project directory\n",
    "project_root = Path.cwd().parent\n",
    "print(f\"\\nüìå Project Root: {project_root}\")\n",
    "print(f\"üìå Current Directory: {Path.cwd()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Check GPU and CUDA Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if PyTorch is installed, if not, we'll install it later\n",
    "try:\n",
    "    import torch\n",
    "    torch_installed = True\n",
    "except ImportError:\n",
    "    torch_installed = False\n",
    "    print(\"‚ö†Ô∏è  PyTorch not installed yet. Will install in next steps.\")\n",
    "\n",
    "if torch_installed:\n",
    "    print(\"=\"*70)\n",
    "    print(\"üéÆ GPU & CUDA INFORMATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nüìå PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"üìå CUDA Available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üìå CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"üìå cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "        print(f\"üìå Number of GPUs: {torch.cuda.device_count()}\")\n",
    "        \n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"\\nüéÆ GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"   - Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB\")\n",
    "            print(f\"   - Compute Capability: {torch.cuda.get_device_properties(i).major}.{torch.cuda.get_device_properties(i).minor}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: CUDA not available! This project requires a CUDA-capable GPU.\")\n",
    "        print(\"Please ensure you have:\")\n",
    "        print(\"  1. NVIDIA GPU installed\")\n",
    "        print(\"  2. CUDA Toolkit installed (11.8+)\")\n",
    "        print(\"  3. Appropriate GPU drivers\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Install Core Dependencies\n",
    "\n",
    "We'll install all required packages from `requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check if requirements.txt exists\n",
    "requirements_path = project_root / 'requirements.txt'\n",
    "\n",
    "if requirements_path.exists():\n",
    "    print(\"‚úÖ Found requirements.txt\")\n",
    "    print(f\"üìç Location: {requirements_path}\")\n",
    "else:\n",
    "    print(\"‚ùå requirements.txt not found!\")\n",
    "    print(f\"Expected location: {requirements_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support first (most important)\n",
    "print(\"üîß Installing PyTorch with CUDA 11.8 support...\\n\")\n",
    "print(\"This may take several minutes...\\n\")\n",
    "\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PyTorch installation\n",
    "import torch\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚úÖ PyTorch Installation Verification\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nPyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(\"\\n‚úÖ GPU is ready for deep learning!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: CUDA not available!\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install remaining dependencies from requirements.txt\n",
    "print(\"üîß Installing remaining dependencies...\\n\")\n",
    "print(\"This may take 5-10 minutes...\\n\")\n",
    "\n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Verify All Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# List of critical packages to verify\n",
    "critical_packages = [\n",
    "    ('torch', 'PyTorch'),\n",
    "    ('torchvision', 'TorchVision'),\n",
    "    ('diffusers', 'Diffusers (Hugging Face)'),\n",
    "    ('transformers', 'Transformers (Hugging Face)'),\n",
    "    ('accelerate', 'Accelerate'),\n",
    "    ('cv2', 'OpenCV'),\n",
    "    ('PIL', 'Pillow'),\n",
    "    ('numpy', 'NumPy'),\n",
    "    ('pandas', 'Pandas'),\n",
    "    ('matplotlib', 'Matplotlib'),\n",
    "    ('gradio', 'Gradio'),\n",
    "    ('fastapi', 'FastAPI'),\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üì¶ PACKAGE VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_installed = True\n",
    "\n",
    "for package_name, display_name in critical_packages:\n",
    "    try:\n",
    "        module = importlib.import_module(package_name)\n",
    "        version = getattr(module, '__version__', 'unknown')\n",
    "        print(f\"‚úÖ {display_name:30s} - v{version}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {display_name:30s} - NOT INSTALLED\")\n",
    "        all_installed = False\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "if all_installed:\n",
    "    print(\"\\nüéâ All critical packages are installed successfully!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Some packages are missing. Please check the installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Directory Structure Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all required directories exist\n",
    "required_dirs = [\n",
    "    'data/raw',\n",
    "    'data/processed',\n",
    "    'data/train',\n",
    "    'data/test',\n",
    "    'data/validation',\n",
    "    'data/models',\n",
    "    'models/checkpoints',\n",
    "    'models/pretrained',\n",
    "    'models/configs',\n",
    "    'outputs/results',\n",
    "    'outputs/visualizations',\n",
    "    'outputs/comparisons',\n",
    "    'outputs/metrics',\n",
    "    'src/preprocessing',\n",
    "    'src/models',\n",
    "    'src/training',\n",
    "    'src/inference',\n",
    "    'src/utils',\n",
    "    'notebooks',\n",
    "    'scripts',\n",
    "    'examples',\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìÅ DIRECTORY STRUCTURE VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_dirs_exist = True\n",
    "\n",
    "for dir_path in required_dirs:\n",
    "    full_path = project_root / dir_path\n",
    "    if full_path.exists():\n",
    "        print(f\"‚úÖ {dir_path}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {dir_path} - MISSING\")\n",
    "        all_dirs_exist = False\n",
    "        # Create missing directory\n",
    "        full_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"   ‚û°Ô∏è  Created directory\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "if all_dirs_exist:\n",
    "    print(\"\\n‚úÖ All required directories exist!\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Missing directories have been created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Configure PyTorch and GPU Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚öôÔ∏è  PYTORCH CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nüìå Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Enable cuDNN benchmarking for better performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(\"‚úÖ cuDNN benchmark mode enabled\")\n",
    "    \n",
    "    # Enable TF32 for faster training on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(\"‚úÖ TF32 enabled for faster computation\")\n",
    "    \n",
    "    # Set memory allocation strategy\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "    print(\"‚úÖ CUDA memory allocation configured\")\n",
    "    \n",
    "    # Display current GPU memory\n",
    "    print(f\"\\nüìä GPU Memory Status:\")\n",
    "    print(f\"   - Total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"   - Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.4f} GB\")\n",
    "    print(f\"   - Cached: {torch.cuda.memory_reserved(0) / 1024**3:.4f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Test GPU with Simple Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"=\"*70)\n",
    "    print(\"üß™ GPU PERFORMANCE TEST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Test matrix multiplication on GPU\n",
    "    size = 5000\n",
    "    \n",
    "    # CPU test\n",
    "    print(f\"\\nüìä Testing {size}x{size} matrix multiplication...\\n\")\n",
    "    \n",
    "    a_cpu = torch.randn(size, size)\n",
    "    b_cpu = torch.randn(size, size)\n",
    "    \n",
    "    start = time.time()\n",
    "    c_cpu = torch.matmul(a_cpu, b_cpu)\n",
    "    cpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  CPU Time: {cpu_time:.4f} seconds\")\n",
    "    \n",
    "    # GPU test\n",
    "    a_gpu = torch.randn(size, size).cuda()\n",
    "    b_gpu = torch.randn(size, size).cuda()\n",
    "    \n",
    "    # Warm up\n",
    "    _ = torch.matmul(a_gpu, b_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    start = time.time()\n",
    "    c_gpu = torch.matmul(a_gpu, b_gpu)\n",
    "    torch.cuda.synchronize()\n",
    "    gpu_time = time.time() - start\n",
    "    \n",
    "    print(f\"‚è±Ô∏è  GPU Time: {gpu_time:.4f} seconds\")\n",
    "    print(f\"\\nüöÄ Speedup: {cpu_time/gpu_time:.2f}x faster on GPU\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  GPU not available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Download Pretrained Models\n",
    "\n",
    "We'll download the base models needed for the project:\n",
    "1. Stable Diffusion XL\n",
    "2. ControlNet (OpenPose)\n",
    "3. Other required models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "# Create pretrained models directory\n",
    "pretrained_dir = project_root / 'models' / 'pretrained'\n",
    "pretrained_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üì• DOWNLOADING PRETRAINED MODELS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚ö†Ô∏è  This will download ~10GB of data. It may take 15-30 minutes.\")\n",
    "print(\"You can skip this step and download models later if needed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Stable Diffusion XL (Optional - can be done later)\n",
    "# Uncomment to download\n",
    "\n",
    "# print(\"üì• Downloading Stable Diffusion XL Base...\")\n",
    "# sd_path = pretrained_dir / 'stable-diffusion-xl-base-1.0'\n",
    "# if not sd_path.exists():\n",
    "#     snapshot_download(\n",
    "#         repo_id=\"stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "#         local_dir=str(sd_path),\n",
    "#         local_dir_use_symlinks=False\n",
    "#     )\n",
    "#     print(\"‚úÖ Stable Diffusion XL downloaded successfully!\")\n",
    "# else:\n",
    "#     print(\"‚úÖ Stable Diffusion XL already exists\")\n",
    "\n",
    "print(\"\\nüí° TIP: Models will be downloaded automatically when needed.\")\n",
    "print(\"You can also download them manually later using the scripts/download_models.py script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Create Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Create default configuration\n",
    "config = {\n",
    "    'project': {\n",
    "        'name': 'AI-Virtual-TryOn',\n",
    "        'version': '1.0.0',\n",
    "        'description': 'Hybrid Generative AI Approach for Realistic Virtual Try-On'\n",
    "    },\n",
    "    'paths': {\n",
    "        'data_dir': 'data',\n",
    "        'models_dir': 'models',\n",
    "        'outputs_dir': 'outputs',\n",
    "        'checkpoints_dir': 'models/checkpoints',\n",
    "        'pretrained_dir': 'models/pretrained'\n",
    "    },\n",
    "    'model': {\n",
    "        'image_size': [512, 384],\n",
    "        'batch_size': 4,\n",
    "        'num_workers': 4,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    },\n",
    "    'training': {\n",
    "        'epochs': 100,\n",
    "        'learning_rate': 1e-5,\n",
    "        'weight_decay': 1e-4,\n",
    "        'save_interval': 10\n",
    "    },\n",
    "    'inference': {\n",
    "        'num_inference_steps': 50,\n",
    "        'guidance_scale': 7.5,\n",
    "        'controlnet_conditioning_scale': 1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = project_root / 'models' / 'configs' / 'default_config.yaml'\n",
    "config_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"‚öôÔ∏è  CONFIGURATION FILE CREATED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìç Location: {config_path}\")\n",
    "print(\"\\nüìÑ Configuration:\")\n",
    "print(yaml.dump(config, default_flow_style=False, sort_keys=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Create Helper Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple config loader utility\n",
    "config_loader_code = '''\n",
    "\"\"\"Configuration management utilities\"\"\"\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration loader and manager\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path=None):\n",
    "        if config_path is None:\n",
    "            # Default config path\n",
    "            project_root = Path(__file__).parent.parent.parent\n",
    "            config_path = project_root / 'models' / 'configs' / 'default_config.yaml'\n",
    "        \n",
    "        self.config_path = Path(config_path)\n",
    "        self.config = self.load_config()\n",
    "    \n",
    "    def load_config(self):\n",
    "        \"\"\"Load configuration from YAML file\"\"\"\n",
    "        with open(self.config_path, 'r') as f:\n",
    "            return yaml.safe_load(f)\n",
    "    \n",
    "    def get(self, key, default=None):\n",
    "        \"\"\"Get configuration value by key\"\"\"\n",
    "        keys = key.split('.')\n",
    "        value = self.config\n",
    "        \n",
    "        for k in keys:\n",
    "            if isinstance(value, dict) and k in value:\n",
    "                value = value[k]\n",
    "            else:\n",
    "                return default\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.get(key)\n",
    "'''\n",
    "\n",
    "# Save to src/utils/config.py\n",
    "config_utils_path = project_root / 'src' / 'utils' / 'config.py'\n",
    "with open(config_utils_path, 'w') as f:\n",
    "    f.write(config_loader_code)\n",
    "\n",
    "print(\"‚úÖ Created config utility: src/utils/config.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Final Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üéâ ENVIRONMENT SETUP COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ Completed Steps:\")\n",
    "print(\"   1. ‚úì System requirements verified\")\n",
    "print(\"   2. ‚úì GPU and CUDA configured\")\n",
    "print(\"   3. ‚úì All dependencies installed\")\n",
    "print(\"   4. ‚úì Directory structure verified\")\n",
    "print(\"   5. ‚úì PyTorch configured for GPU\")\n",
    "print(\"   6. ‚úì Configuration files created\")\n",
    "print(\"   7. ‚úì Utility functions created\")\n",
    "\n",
    "print(\"\\nüìä System Summary:\")\n",
    "print(f\"   - Python: {sys.version.split()[0]}\")\n",
    "print(f\"   - PyTorch: {torch.__version__}\")\n",
    "print(f\"   - Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   - GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   - VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"   1. Proceed to notebook 02_data_exploration.ipynb\")\n",
    "print(\"   2. Download datasets (VITON-HD, DeepFashion)\")\n",
    "print(\"   3. Start exploring the data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° TIP: Save this notebook and proceed to the next one!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Notes\n",
    "\n",
    "### Important Reminders:\n",
    "\n",
    "1. **GPU Memory**: Monitor GPU memory usage during training. If you encounter OOM errors:\n",
    "   - Reduce batch size\n",
    "   - Use gradient checkpointing\n",
    "   - Enable mixed precision training\n",
    "\n",
    "2. **Model Downloads**: Pretrained models are large (~10GB total). Download them when you have:\n",
    "   - Stable internet connection\n",
    "   - Sufficient storage space\n",
    "   - Time to wait (15-30 minutes)\n",
    "\n",
    "3. **Dependencies**: If you encounter package conflicts:\n",
    "   - Create a fresh virtual environment\n",
    "   - Install packages in the order specified\n",
    "   - Check for version compatibility\n",
    "\n",
    "4. **CUDA Issues**: If CUDA is not detected:\n",
    "   - Verify NVIDIA drivers are installed\n",
    "   - Check CUDA toolkit installation\n",
    "   - Ensure PyTorch CUDA version matches your CUDA toolkit\n",
    "\n",
    "### Troubleshooting:\n",
    "\n",
    "**Problem**: `RuntimeError: CUDA out of memory`\n",
    "- **Solution**: Reduce batch size or image resolution\n",
    "\n",
    "**Problem**: `ImportError: No module named 'xxx'`\n",
    "- **Solution**: Run `pip install xxx` or check requirements.txt\n",
    "\n",
    "**Problem**: Slow training\n",
    "- **Solution**: Enable cuDNN benchmark, use mixed precision, check GPU utilization\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Useful Resources\n",
    "\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [Hugging Face Diffusers](https://huggingface.co/docs/diffusers/index)\n",
    "- [CUDA Installation Guide](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/)\n",
    "- [Project GitHub Repository](https://github.com/Huzaifanasir95/AI-Virtual-TryOn)\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: Huzaifa Nasir  \n",
    "**Date**: December 2025  \n",
    "**Notebook**: 01_environment_setup.ipynb  \n",
    "**Status**: ‚úÖ Complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
