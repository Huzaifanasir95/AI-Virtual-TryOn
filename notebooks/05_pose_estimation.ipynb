{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fef79b63",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b42b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.collections import LineCollection\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Configuration\n",
    "import yaml\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba768c1",
   "metadata": {},
   "source": [
    "## 2. Setup Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f9fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project paths\n",
    "project_root = Path(r'd:\\Projects\\AI-Virtual-TryOn')\n",
    "dataset_root = project_root / 'data' / 'zalando-hd-resized'\n",
    "processed_dir = project_root / 'data' / 'processed'\n",
    "output_dir = project_root / 'outputs' / 'pose'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Project Root: {project_root}\")\n",
    "print(f\"üìÅ Dataset Root: {dataset_root}\")\n",
    "print(f\"üìÅ Processed Data: {processed_dir}\")\n",
    "print(f\"üìÅ Output Directory: {output_dir}\")\n",
    "\n",
    "# Load preprocessing configuration\n",
    "config_path = processed_dir / 'preprocessing_config.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    preprocess_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "print(\"\\n‚úÖ Loaded preprocessing configuration\")\n",
    "\n",
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nüñ•Ô∏è Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be54696",
   "metadata": {},
   "source": [
    "## 3. Define OpenPose Keypoint Structure\n",
    "\n",
    "OpenPose provides 25 body keypoints for each person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9aba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenPose 25 body keypoints (Body25 model)\n",
    "BODY25_KEYPOINTS = {\n",
    "    0: 'Nose',\n",
    "    1: 'Neck',\n",
    "    2: 'RShoulder',\n",
    "    3: 'RElbow',\n",
    "    4: 'RWrist',\n",
    "    5: 'LShoulder',\n",
    "    6: 'LElbow',\n",
    "    7: 'LWrist',\n",
    "    8: 'MidHip',\n",
    "    9: 'RHip',\n",
    "    10: 'RKnee',\n",
    "    11: 'RAnkle',\n",
    "    12: 'LHip',\n",
    "    13: 'LKnee',\n",
    "    14: 'LAnkle',\n",
    "    15: 'REye',\n",
    "    16: 'LEye',\n",
    "    17: 'REar',\n",
    "    18: 'LEar',\n",
    "    19: 'LBigToe',\n",
    "    20: 'LSmallToe',\n",
    "    21: 'LHeel',\n",
    "    22: 'RBigToe',\n",
    "    23: 'RSmallToe',\n",
    "    24: 'RHeel'\n",
    "}\n",
    "\n",
    "# Skeleton connections for visualization\n",
    "BODY25_SKELETON = [\n",
    "    # Head\n",
    "    (0, 1), (0, 15), (0, 16), (15, 17), (16, 18),\n",
    "    # Upper body\n",
    "    (1, 2), (1, 5), (1, 8),\n",
    "    # Right arm\n",
    "    (2, 3), (3, 4),\n",
    "    # Left arm\n",
    "    (5, 6), (6, 7),\n",
    "    # Lower body\n",
    "    (8, 9), (8, 12),\n",
    "    # Right leg\n",
    "    (9, 10), (10, 11),\n",
    "    # Left leg\n",
    "    (12, 13), (13, 14),\n",
    "    # Right foot\n",
    "    (11, 22), (11, 24), (22, 23),\n",
    "    # Left foot\n",
    "    (14, 19), (14, 21), (19, 20)\n",
    "]\n",
    "\n",
    "# Critical keypoints for virtual try-on (upper body focus)\n",
    "CRITICAL_KEYPOINTS = {\n",
    "    'face': [0, 15, 16, 17, 18],  # Nose, eyes, ears\n",
    "    'upper_body': [1, 2, 3, 4, 5, 6, 7],  # Neck, shoulders, elbows, wrists\n",
    "    'torso': [1, 8, 9, 12],  # Neck, hips\n",
    "    'lower_body': [9, 10, 11, 12, 13, 14]  # Hips, knees, ankles\n",
    "}\n",
    "\n",
    "print(\"‚úÖ OpenPose keypoint structure defined\")\n",
    "print(f\"\\nüìä Total keypoints: {len(BODY25_KEYPOINTS)}\")\n",
    "print(f\"üìä Skeleton connections: {len(BODY25_SKELETON)}\")\n",
    "print(f\"\\nüéØ Critical keypoint groups:\")\n",
    "for group, keypoints in CRITICAL_KEYPOINTS.items():\n",
    "    names = [BODY25_KEYPOINTS[k] for k in keypoints]\n",
    "    print(f\"   {group}: {names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9648dc",
   "metadata": {},
   "source": [
    "## 4. Load Dataset Catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb323b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load catalogs\n",
    "train_catalog = pd.read_csv(processed_dir / 'train_catalog.csv')\n",
    "val_catalog = pd.read_csv(processed_dir / 'val_catalog.csv')\n",
    "test_catalog = pd.read_csv(processed_dir / 'test_catalog.csv')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üìã DATASET CATALOGS LOADED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä Dataset sizes:\")\n",
    "print(f\"   Train: {len(train_catalog):,}\")\n",
    "print(f\"   Val: {len(val_catalog):,}\")\n",
    "print(f\"   Test: {len(test_catalog):,}\")\n",
    "print(f\"   Total: {len(train_catalog) + len(val_catalog) + len(test_catalog):,}\")\n",
    "\n",
    "print(\"\\n=\"*70)\n",
    "\n",
    "# Show sample paths\n",
    "sample_row = train_catalog.iloc[0]\n",
    "print(f\"\\nüìÑ Sample file paths:\")\n",
    "print(f\"   Person: {os.path.basename(sample_row['person_image'])}\")\n",
    "print(f\"   Pose image: {os.path.basename(sample_row['pose_image'])}\")\n",
    "print(f\"   Pose JSON: {os.path.basename(sample_row['pose_json'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7789a",
   "metadata": {},
   "source": [
    "## 5. Pose Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a05fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pose_keypoints(json_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load pose keypoints from OpenPose JSON file.\n",
    "    \n",
    "    Returns:\n",
    "        Array of shape [25, 3] with (x, y, confidence) for each keypoint\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if 'people' in data and len(data['people']) > 0:\n",
    "        # Get first person's keypoints\n",
    "        pose_keypoints = data['people'][0]['pose_keypoints_2d']\n",
    "        # Reshape from flat array to [25, 3]\n",
    "        keypoints = np.array(pose_keypoints).reshape(-1, 3)\n",
    "        return keypoints\n",
    "    else:\n",
    "        # Return zero array if no person detected\n",
    "        return np.zeros((25, 3))\n",
    "\n",
    "\n",
    "def draw_pose_keypoints(image: np.ndarray, keypoints: np.ndarray, \n",
    "                       threshold: float = 0.1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw pose keypoints and skeleton on image.\n",
    "    \n",
    "    Args:\n",
    "        image: RGB image\n",
    "        keypoints: [25, 3] array of (x, y, confidence)\n",
    "        threshold: Confidence threshold for drawing\n",
    "    \n",
    "    Returns:\n",
    "        Image with pose overlay\n",
    "    \"\"\"\n",
    "    img = image.copy()\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Draw skeleton connections\n",
    "    for connection in BODY25_SKELETON:\n",
    "        pt1_idx, pt2_idx = connection\n",
    "        pt1 = keypoints[pt1_idx]\n",
    "        pt2 = keypoints[pt2_idx]\n",
    "        \n",
    "        # Check if both points are valid\n",
    "        if pt1[2] > threshold and pt2[2] > threshold:\n",
    "            x1, y1 = int(pt1[0]), int(pt1[1])\n",
    "            x2, y2 = int(pt2[0]), int(pt2[1])\n",
    "            cv2.line(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    # Draw keypoints\n",
    "    for i, (x, y, conf) in enumerate(keypoints):\n",
    "        if conf > threshold:\n",
    "            x, y = int(x), int(y)\n",
    "            # Different colors for different body parts\n",
    "            if i in CRITICAL_KEYPOINTS['face']:\n",
    "                color = (255, 0, 0)  # Red for face\n",
    "            elif i in CRITICAL_KEYPOINTS['upper_body']:\n",
    "                color = (0, 0, 255)  # Blue for upper body\n",
    "            elif i in CRITICAL_KEYPOINTS['torso']:\n",
    "                color = (255, 255, 0)  # Yellow for torso\n",
    "            else:\n",
    "                color = (0, 255, 255)  # Cyan for lower body\n",
    "            \n",
    "            cv2.circle(img, (x, y), 4, color, -1)\n",
    "            cv2.circle(img, (x, y), 5, (255, 255, 255), 1)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize_pose_sample(person_img: np.ndarray, keypoints: np.ndarray,\n",
    "                         pose_map: Optional[np.ndarray] = None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create comprehensive pose visualization.\n",
    "    \"\"\"\n",
    "    if pose_map is not None:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    else:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axes = [axes[0], axes[1], None]\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(person_img)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Pose overlay\n",
    "    pose_overlay = draw_pose_keypoints(person_img, keypoints)\n",
    "    axes[1].imshow(pose_overlay)\n",
    "    axes[1].set_title('Pose Keypoints')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Pose map (if provided)\n",
    "    if pose_map is not None and axes[2] is not None:\n",
    "        # Show mean of all channels\n",
    "        pose_viz = np.mean(pose_map, axis=2)\n",
    "        axes[2].imshow(pose_viz, cmap='hot')\n",
    "        axes[2].set_title('Pose Map (Mean of 18 channels)')\n",
    "        axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"‚úÖ Pose visualization functions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2bd35b",
   "metadata": {},
   "source": [
    "## 6. Analyze Pose Keypoint Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b92248",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üîç POSE KEYPOINT DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Analyze first 500 samples\n",
    "n_samples = 500\n",
    "samples = train_catalog.iloc[:n_samples]\n",
    "\n",
    "keypoint_detection_count = {i: 0 for i in range(25)}\n",
    "keypoint_confidence_sum = {i: 0.0 for i in range(25)}\n",
    "samples_with_pose = 0\n",
    "\n",
    "print(f\"üìä Analyzing {n_samples} pose keypoint files...\\n\")\n",
    "\n",
    "for idx, row in tqdm(samples.iterrows(), total=len(samples), desc=\"Processing poses\"):\n",
    "    keypoints = load_pose_keypoints(row['pose_json'])\n",
    "    \n",
    "    if keypoints.sum() > 0:  # At least one keypoint detected\n",
    "        samples_with_pose += 1\n",
    "        \n",
    "        for i in range(25):\n",
    "            conf = keypoints[i, 2]\n",
    "            if conf > 0.1:  # Threshold for detection\n",
    "                keypoint_detection_count[i] += 1\n",
    "                keypoint_confidence_sum[i] += conf\n",
    "\n",
    "print(f\"\\nüìä Keypoint Detection Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(25):\n",
    "    count = keypoint_detection_count[i]\n",
    "    percentage = (count / samples_with_pose) * 100 if samples_with_pose > 0 else 0\n",
    "    avg_conf = keypoint_confidence_sum[i] / count if count > 0 else 0\n",
    "    name = BODY25_KEYPOINTS[i]\n",
    "    print(f\"   {i:2d}. {name:15s}: {count:3d} ({percentage:5.1f}%) | Avg conf: {avg_conf:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"\\n‚úÖ Samples with valid pose: {samples_with_pose}/{n_samples} ({samples_with_pose/n_samples*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902750eb",
   "metadata": {},
   "source": [
    "## 7. Visualize Sample Poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce253d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 6 sample poses\n",
    "fig, axes = plt.subplots(2, 6, figsize=(18, 6))\n",
    "\n",
    "for i in range(6):\n",
    "    row = train_catalog.iloc[i*10]\n",
    "    \n",
    "    # Load images\n",
    "    person_img = np.array(Image.open(row['person_image']))\n",
    "    keypoints = load_pose_keypoints(row['pose_json'])\n",
    "    \n",
    "    # Draw original and with pose\n",
    "    axes[0, i].imshow(person_img)\n",
    "    axes[0, i].set_title(f\"Person\\n{row['id']}\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    pose_overlay = draw_pose_keypoints(person_img, keypoints)\n",
    "    axes[1, i].imshow(pose_overlay)\n",
    "    axes[1, i].set_title(f\"Keypoints\\n{int(keypoints[:, 2].sum())} pts\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Pose Keypoint Visualization', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'pose_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Visualization saved to: {output_dir / 'pose_visualization.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187814d",
   "metadata": {},
   "source": [
    "## 8. Analyze Pose Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze pose map structure\n",
    "sample_row = train_catalog.iloc[0]\n",
    "pose_map = np.array(Image.open(sample_row['pose_image']))\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üîç POSE MAP ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä Pose Map Properties:\")\n",
    "print(f\"   Shape: {pose_map.shape}\")\n",
    "print(f\"   Dtype: {pose_map.dtype}\")\n",
    "print(f\"   Value range: [{pose_map.min()}, {pose_map.max()}]\")\n",
    "print(f\"   Mean: {pose_map.mean():.3f}\")\n",
    "print(f\"   Std: {pose_map.std():.3f}\")\n",
    "\n",
    "# Expected format: 18 channel pose map\n",
    "if len(pose_map.shape) == 3:\n",
    "    print(f\"\\nüìä Channels: {pose_map.shape[2]}\")\n",
    "    if pose_map.shape[2] == 3:\n",
    "        print(\"   ‚ö†Ô∏è Pose map is RGB - needs to be converted to 18-channel format\")\n",
    "    elif pose_map.shape[2] == 18:\n",
    "        print(\"   ‚úÖ Pose map has 18 channels (standard format)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c95f20",
   "metadata": {},
   "source": [
    "## 9. Visualize Pose Map Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7361c754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pose map\n",
    "sample_row = train_catalog.iloc[0]\n",
    "person_img = np.array(Image.open(sample_row['person_image']))\n",
    "pose_map_img = np.array(Image.open(sample_row['pose_image']))\n",
    "keypoints = load_pose_keypoints(sample_row['pose_json'])\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(person_img)\n",
    "axes[0, 0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Pose overlay\n",
    "pose_overlay = draw_pose_keypoints(person_img, keypoints)\n",
    "axes[0, 1].imshow(pose_overlay)\n",
    "axes[0, 1].set_title(f'Pose Keypoints ({int(keypoints[:, 2].sum())} detected)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Pose map (RGB visualization)\n",
    "axes[0, 2].imshow(pose_map_img)\n",
    "axes[0, 2].set_title('Pose Map (RGB)', fontsize=12, fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Show individual channels if multi-channel\n",
    "if len(pose_map_img.shape) == 3:\n",
    "    for i in range(3):\n",
    "        channel = pose_map_img[:, :, i] if pose_map_img.shape[2] > i else pose_map_img[:, :, 0]\n",
    "        axes[1, i].imshow(channel, cmap='hot')\n",
    "        axes[1, i].set_title(f'Channel {i}', fontsize=10)\n",
    "        axes[1, i].axis('off')\n",
    "else:\n",
    "    axes[1, 0].imshow(pose_map_img, cmap='hot')\n",
    "    axes[1, 0].set_title('Pose Map', fontsize=10)\n",
    "    axes[1, 0].axis('off')\n",
    "    axes[1, 1].axis('off')\n",
    "    axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('Pose Estimation - Detailed Analysis', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'pose_detailed_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Detailed analysis saved to: {output_dir / 'pose_detailed_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61bbf94",
   "metadata": {},
   "source": [
    "## 10. Pose Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475aef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_pose_quality(keypoints: np.ndarray, threshold: float = 0.1) -> Dict:\n",
    "    \"\"\"\n",
    "    Assess quality of pose detection.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with quality metrics\n",
    "    \"\"\"\n",
    "    quality = {\n",
    "        'total_keypoints': 0,\n",
    "        'avg_confidence': 0.0,\n",
    "        'face_complete': False,\n",
    "        'upper_body_complete': False,\n",
    "        'lower_body_complete': False,\n",
    "        'status': 'unknown'\n",
    "    }\n",
    "    \n",
    "    # Count detected keypoints\n",
    "    detected = keypoints[:, 2] > threshold\n",
    "    quality['total_keypoints'] = int(detected.sum())\n",
    "    \n",
    "    if quality['total_keypoints'] > 0:\n",
    "        quality['avg_confidence'] = float(keypoints[detected, 2].mean())\n",
    "    \n",
    "    # Check critical regions\n",
    "    face_detected = sum([keypoints[i, 2] > threshold for i in CRITICAL_KEYPOINTS['face']])\n",
    "    quality['face_complete'] = face_detected >= 3  # At least 3/5 face points\n",
    "    \n",
    "    upper_detected = sum([keypoints[i, 2] > threshold for i in CRITICAL_KEYPOINTS['upper_body']])\n",
    "    quality['upper_body_complete'] = upper_detected >= 5  # At least 5/7 upper body points\n",
    "    \n",
    "    lower_detected = sum([keypoints[i, 2] > threshold for i in CRITICAL_KEYPOINTS['lower_body']])\n",
    "    quality['lower_body_complete'] = lower_detected >= 3  # At least 3/6 lower body points\n",
    "    \n",
    "    # Overall status\n",
    "    if quality['face_complete'] and quality['upper_body_complete']:\n",
    "        quality['status'] = 'good'\n",
    "    elif quality['upper_body_complete']:\n",
    "        quality['status'] = 'acceptable'\n",
    "    else:\n",
    "        quality['status'] = 'poor'\n",
    "    \n",
    "    return quality\n",
    "\n",
    "\n",
    "# Assess quality on 200 samples\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ POSE QUALITY ASSESSMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_assess = 200\n",
    "samples = train_catalog.iloc[:n_assess]\n",
    "\n",
    "quality_stats = {\n",
    "    'good': 0,\n",
    "    'acceptable': 0,\n",
    "    'poor': 0,\n",
    "    'face_complete': 0,\n",
    "    'upper_body_complete': 0,\n",
    "    'lower_body_complete': 0\n",
    "}\n",
    "\n",
    "print(f\"üîç Assessing quality of {n_assess} pose detections...\\n\")\n",
    "\n",
    "for idx, row in tqdm(samples.iterrows(), total=len(samples), desc=\"Quality check\"):\n",
    "    keypoints = load_pose_keypoints(row['pose_json'])\n",
    "    quality = assess_pose_quality(keypoints)\n",
    "    \n",
    "    quality_stats[quality['status']] += 1\n",
    "    if quality['face_complete']:\n",
    "        quality_stats['face_complete'] += 1\n",
    "    if quality['upper_body_complete']:\n",
    "        quality_stats['upper_body_complete'] += 1\n",
    "    if quality['lower_body_complete']:\n",
    "        quality_stats['lower_body_complete'] += 1\n",
    "\n",
    "print(f\"\\nüìä Quality Report:\")\n",
    "print(\"-\" * 50)\n",
    "for status in ['good', 'acceptable', 'poor']:\n",
    "    count = quality_stats[status]\n",
    "    percentage = (count / n_assess) * 100\n",
    "    emoji = \"‚úÖ\" if status == 'good' else \"‚ö†Ô∏è\" if status == 'acceptable' else \"‚ùå\"\n",
    "    print(f\"{emoji} {status.title():20s}: {count:4d} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\nüìä Completeness:\")\n",
    "print(\"-\" * 50)\n",
    "for region in ['face_complete', 'upper_body_complete', 'lower_body_complete']:\n",
    "    count = quality_stats[region]\n",
    "    percentage = (count / n_assess) * 100\n",
    "    emoji = \"‚úÖ\" if percentage > 90 else \"‚ö†Ô∏è\" if percentage > 70 else \"‚ùå\"\n",
    "    name = region.replace('_', ' ').title()\n",
    "    print(f\"{emoji} {name:25s}: {count:4d} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# Overall assessment\n",
    "good_percentage = (quality_stats['good'] / n_assess) * 100\n",
    "if good_percentage > 80:\n",
    "    print(\"\\n‚úÖ Pose quality is excellent for virtual try-on application\")\n",
    "elif good_percentage > 60:\n",
    "    print(\"\\n‚úÖ Pose quality is good for virtual try-on application\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Pose quality may need improvement for optimal results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a6b8a",
   "metadata": {},
   "source": [
    "## 11. PoseProcessor Utility Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16baf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseProcessor:\n",
    "    \"\"\"\n",
    "    Utility class for processing pose data for virtual try-on models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_size: Tuple[int, int] = (512, 384)):\n",
    "        self.image_size = image_size  # (height, width)\n",
    "        self.num_keypoints = 25\n",
    "    \n",
    "    def normalize_keypoints(self, keypoints: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalize keypoint coordinates to [-1, 1].\n",
    "        \n",
    "        Args:\n",
    "            keypoints: [25, 3] array\n",
    "        \n",
    "        Returns:\n",
    "            Normalized keypoints [25, 3]\n",
    "        \"\"\"\n",
    "        normalized = keypoints.copy()\n",
    "        h, w = self.image_size\n",
    "        \n",
    "        # Normalize x, y to [-1, 1]\n",
    "        normalized[:, 0] = (keypoints[:, 0] / w) * 2 - 1\n",
    "        normalized[:, 1] = (keypoints[:, 1] / h) * 2 - 1\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def create_pose_heatmap(self, keypoints: np.ndarray, \n",
    "                           sigma: float = 3.0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create Gaussian heatmaps for each keypoint.\n",
    "        \n",
    "        Args:\n",
    "            keypoints: [25, 3] array\n",
    "            sigma: Gaussian kernel size\n",
    "        \n",
    "        Returns:\n",
    "            Heatmap array [25, H, W]\n",
    "        \"\"\"\n",
    "        h, w = self.image_size\n",
    "        heatmaps = np.zeros((self.num_keypoints, h, w), dtype=np.float32)\n",
    "        \n",
    "        for i, (x, y, conf) in enumerate(keypoints):\n",
    "            if conf > 0.1:  # Only create heatmap for detected keypoints\n",
    "                # Create Gaussian heatmap\n",
    "                x, y = int(x), int(y)\n",
    "                if 0 <= x < w and 0 <= y < h:\n",
    "                    # Create meshgrid\n",
    "                    xx, yy = np.meshgrid(np.arange(w), np.arange(h))\n",
    "                    # Gaussian formula\n",
    "                    heatmap = np.exp(-((xx - x)**2 + (yy - y)**2) / (2 * sigma**2))\n",
    "                    heatmaps[i] = heatmap * conf\n",
    "        \n",
    "        return heatmaps\n",
    "    \n",
    "    def get_pose_embedding(self, keypoints: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get flattened pose embedding for discriminator/loss.\n",
    "        \n",
    "        Returns:\n",
    "            Flattened array [75] (25 keypoints √ó 3)\n",
    "        \"\"\"\n",
    "        return keypoints.flatten()\n",
    "    \n",
    "    def visualize(self, image: np.ndarray, keypoints: np.ndarray) -> plt.Figure:\n",
    "        \"\"\"\n",
    "        Visualize pose with different representations.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Original with skeleton\n",
    "        pose_overlay = draw_pose_keypoints(image, keypoints)\n",
    "        axes[0].imshow(pose_overlay)\n",
    "        axes[0].set_title('Keypoint Overlay')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Heatmap visualization\n",
    "        heatmaps = self.create_pose_heatmap(keypoints)\n",
    "        heatmap_viz = np.max(heatmaps, axis=0)  # Max pooling across channels\n",
    "        axes[1].imshow(heatmap_viz, cmap='hot')\n",
    "        axes[1].set_title('Pose Heatmap')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Overlay heatmap on image\n",
    "        overlay = (image * 0.5 + \n",
    "                  (heatmap_viz[:, :, np.newaxis] * np.array([255, 0, 0])) * 0.5).astype(np.uint8)\n",
    "        axes[2].imshow(overlay)\n",
    "        axes[2].set_title('Combined Overlay')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "\n",
    "# Test the processor\n",
    "processor = PoseProcessor(image_size=(1024, 768))\n",
    "\n",
    "sample_row = train_catalog.iloc[5]\n",
    "person_img = np.array(Image.open(sample_row['person_image']))\n",
    "keypoints = load_pose_keypoints(sample_row['pose_json'])\n",
    "\n",
    "print(\"üîß Testing PoseProcessor...\\n\")\n",
    "fig = processor.visualize(person_img, keypoints)\n",
    "plt.savefig(output_dir / 'pose_processor_demo.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Test other methods\n",
    "normalized = processor.normalize_keypoints(keypoints)\n",
    "heatmaps = processor.create_pose_heatmap(keypoints)\n",
    "embedding = processor.get_pose_embedding(keypoints)\n",
    "\n",
    "print(\"\\n‚úÖ PoseProcessor created and tested\")\n",
    "print(f\"   Normalized keypoints shape: {normalized.shape}\")\n",
    "print(f\"   Heatmaps shape: {heatmaps.shape}\")\n",
    "print(f\"   Embedding shape: {embedding.shape}\")\n",
    "print(f\"   Saved demo to: {output_dir / 'pose_processor_demo.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d608ba",
   "metadata": {},
   "source": [
    "## 12. Save Pose Analysis Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258252f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive report\n",
    "pose_report = {\n",
    "    'analysis': {\n",
    "        'total_keypoints': 25,\n",
    "        'samples_analyzed': n_samples,\n",
    "        'samples_with_pose': samples_with_pose,\n",
    "        'detection_rate': samples_with_pose / n_samples\n",
    "    },\n",
    "    'keypoint_statistics': keypoint_detection_count,\n",
    "    'quality_assessment': quality_stats,\n",
    "    'critical_keypoints': CRITICAL_KEYPOINTS,\n",
    "    'skeleton_connections': len(BODY25_SKELETON)\n",
    "}\n",
    "\n",
    "report_path = output_dir / 'pose_analysis_report.json'\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(pose_report, f, indent=2)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üíæ POSE ANALYSIS REPORT SAVED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìÑ Report saved to: {report_path}\")\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   - Total keypoints: 25\")\n",
    "print(f\"   - Samples analyzed: {n_samples}\")\n",
    "print(f\"   - Quality checked: {n_assess}\")\n",
    "print(f\"   - Visualizations: 3\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be6151",
   "metadata": {},
   "source": [
    "## 13. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üéâ POSE ESTIMATION ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ Completed Tasks:\")\n",
    "print(\"   1. ‚úì Loaded and analyzed OpenPose keypoints (25 keypoints)\")\n",
    "print(\"   2. ‚úì Analyzed keypoint detection statistics\")\n",
    "print(\"   3. ‚úì Created pose visualization tools\")\n",
    "print(\"   4. ‚úì Analyzed pose map structure\")\n",
    "print(\"   5. ‚úì Assessed pose quality across dataset\")\n",
    "print(\"   6. ‚úì Implemented PoseProcessor utility class\")\n",
    "print(\"   7. ‚úì Generated comprehensive analysis report\")\n",
    "\n",
    "print(f\"\\nüìä Key Findings:\")\n",
    "print(f\"   - Total keypoints per pose: 25\")\n",
    "print(f\"   - Skeleton connections: {len(BODY25_SKELETON)}\")\n",
    "print(f\"   - Dataset samples: {len(train_catalog) + len(val_catalog) + len(test_catalog):,}\")\n",
    "print(f\"   - Good quality poses: {quality_stats['good']}/{n_assess} ({quality_stats['good']/n_assess*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "print(f\"   - pose_visualization: pose_visualization.png\")\n",
    "print(f\"   - detailed_analysis: pose_detailed_analysis.png\")\n",
    "print(f\"   - processor_demo: pose_processor_demo.png\")\n",
    "print(f\"   - pose_analysis_report.json\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for Next Steps:\")\n",
    "print(\"   1. Proceed to notebook 06_cloth_agnostic_representation.ipynb\")\n",
    "print(\"   2. Combine parsing and pose for cloth-agnostic person\")\n",
    "print(\"   3. Begin model architecture development\")\n",
    "print(\"   4. Integrate all components for training\")\n",
    "\n",
    "print(\"\\nüí° Key Utilities Created:\")\n",
    "print(\"   - load_pose_keypoints(): Load keypoints from JSON\")\n",
    "print(\"   - draw_pose_keypoints(): Draw skeleton on image\")\n",
    "print(\"   - assess_pose_quality(): Evaluate pose completeness\")\n",
    "print(\"   - PoseProcessor: Complete pose processing class\")\n",
    "print(\"     * normalize_keypoints()\")\n",
    "print(\"     * create_pose_heatmap()\")\n",
    "print(\"     * get_pose_embedding()\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ POSE ESTIMATION MODULE READY!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
