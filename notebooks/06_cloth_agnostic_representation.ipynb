{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a205ea",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b13f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "# Configuration\n",
    "import yaml\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a493f",
   "metadata": {},
   "source": [
    "## 2. Setup Paths and Load Previous Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f4118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project paths\n",
    "project_root = Path(r'd:\\Projects\\AI-Virtual-TryOn')\n",
    "dataset_root = project_root / 'data' / 'zalando-hd-resized'\n",
    "processed_dir = project_root / 'data' / 'processed'\n",
    "output_dir = project_root / 'outputs' / 'cloth_agnostic'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Project Root: {project_root}\")\n",
    "print(f\"üìÅ Dataset Root: {dataset_root}\")\n",
    "print(f\"üìÅ Processed Data: {processed_dir}\")\n",
    "print(f\"üìÅ Output Directory: {output_dir}\")\n",
    "\n",
    "# Load preprocessing configuration\n",
    "config_path = processed_dir / 'preprocessing_config.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    preprocess_config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "print(\"\\n‚úÖ Loaded preprocessing configuration\")\n",
    "\n",
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nüñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "# Load dataset catalogs\n",
    "train_catalog = pd.read_csv(processed_dir / 'train_catalog.csv')\n",
    "val_catalog = pd.read_csv(processed_dir / 'val_catalog.csv')\n",
    "test_catalog = pd.read_csv(processed_dir / 'test_catalog.csv')\n",
    "\n",
    "print(f\"\\nüìä Dataset loaded: {len(train_catalog)} train, {len(val_catalog)} val, {len(test_catalog)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64548d2",
   "metadata": {},
   "source": [
    "## 3. Load Parsing and Pose Utilities\n",
    "\n",
    "Import the utilities we created in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3784c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIP parsing classes (from notebook 04)\n",
    "LIP_CLASSES = {\n",
    "    0: 'Background',\n",
    "    1: 'Hat',\n",
    "    2: 'Hair',\n",
    "    3: 'Glove',\n",
    "    4: 'Sunglasses',\n",
    "    5: 'Upper-clothes',\n",
    "    6: 'Dress',\n",
    "    7: 'Coat',\n",
    "    8: 'Socks',\n",
    "    9: 'Pants',\n",
    "    10: 'Jumpsuits',\n",
    "    11: 'Scarf',\n",
    "    12: 'Skirt',\n",
    "    13: 'Face',\n",
    "    14: 'Left-arm',\n",
    "    15: 'Right-arm',\n",
    "    16: 'Left-leg',\n",
    "    17: 'Right-leg',\n",
    "    18: 'Left-shoe',\n",
    "    19: 'Right-shoe'\n",
    "}\n",
    "\n",
    "# Garment classes\n",
    "GARMENT_CLASSES = {\n",
    "    'upper_body': [5, 7],      # Upper-clothes, Coat\n",
    "    'lower_body': [9, 12],     # Pants, Skirt\n",
    "    'full_body': [6, 10],      # Dress, Jumpsuits\n",
    "    'accessories': [1, 3, 4, 8, 11, 18, 19]  # Hat, Glove, Sunglasses, Socks, Scarf, Shoes\n",
    "}\n",
    "\n",
    "# Body part classes for cloth-agnostic representation\n",
    "BODY_PARTS = {\n",
    "    'face': [13],\n",
    "    'hair': [2],\n",
    "    'arms': [14, 15],\n",
    "    'legs': [16, 17],\n",
    "    'background': [0]\n",
    "}\n",
    "\n",
    "# OpenPose keypoint structure (from notebook 05)\n",
    "BODY25_KEYPOINTS = {\n",
    "    0: 'Nose', 1: 'Neck', 2: 'RShoulder', 3: 'RElbow', 4: 'RWrist',\n",
    "    5: 'LShoulder', 6: 'LElbow', 7: 'LWrist', 8: 'MidHip',\n",
    "    9: 'RHip', 10: 'RKnee', 11: 'RAnkle',\n",
    "    12: 'LHip', 13: 'LKnee', 14: 'LAnkle',\n",
    "    15: 'REye', 16: 'LEye', 17: 'REar', 18: 'LEar',\n",
    "    19: 'LBigToe', 20: 'LSmallToe', 21: 'LHeel',\n",
    "    22: 'RBigToe', 23: 'RSmallToe', 24: 'RHeel'\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Parsing and pose structures loaded\")\n",
    "print(f\"   - LIP classes: {len(LIP_CLASSES)}\")\n",
    "print(f\"   - Garment categories: {len(GARMENT_CLASSES)}\")\n",
    "print(f\"   - Body parts: {len(BODY_PARTS)}\")\n",
    "print(f\"   - Pose keypoints: {len(BODY25_KEYPOINTS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b5aae",
   "metadata": {},
   "source": [
    "## 4. Cloth-Agnostic Mask Generation\n",
    "\n",
    "Create mask that represents the person with target garment removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cloth_agnostic_mask(parsing: np.ndarray, \n",
    "                               garment_type: str = 'upper_body') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create cloth-agnostic mask by removing target garment region.\n",
    "    \n",
    "    Args:\n",
    "        parsing: [H, W] parsing mask with class indices\n",
    "        garment_type: 'upper_body', 'lower_body', or 'full_body'\n",
    "    \n",
    "    Returns:\n",
    "        Binary mask [H, W] where 1 = keep, 0 = remove\n",
    "    \"\"\"\n",
    "    mask = np.ones_like(parsing, dtype=np.uint8)\n",
    "    \n",
    "    # Get garment class IDs to remove\n",
    "    classes_to_remove = GARMENT_CLASSES.get(garment_type, [])\n",
    "    \n",
    "    # Remove garment regions\n",
    "    for cls_id in classes_to_remove:\n",
    "        mask[parsing == cls_id] = 0\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_body_parts_mask(parsing: np.ndarray, include_parts: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract specific body parts from parsing mask.\n",
    "    \n",
    "    Args:\n",
    "        parsing: [H, W] parsing mask\n",
    "        include_parts: List of body part names to include\n",
    "    \n",
    "    Returns:\n",
    "        Binary mask [H, W]\n",
    "    \"\"\"\n",
    "    mask = np.zeros_like(parsing, dtype=np.uint8)\n",
    "    \n",
    "    for part_name in include_parts:\n",
    "        if part_name in BODY_PARTS:\n",
    "            class_ids = BODY_PARTS[part_name]\n",
    "            for cls_id in class_ids:\n",
    "                mask[parsing == cls_id] = 1\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_cloth_agnostic_rgb(person_img: np.ndarray, \n",
    "                              parsing: np.ndarray,\n",
    "                              garment_type: str = 'upper_body',\n",
    "                              fill_color: Tuple[int, int, int] = (128, 128, 128)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create cloth-agnostic RGB representation.\n",
    "    \n",
    "    Args:\n",
    "        person_img: [H, W, 3] RGB image\n",
    "        parsing: [H, W] parsing mask\n",
    "        garment_type: Type of garment to remove\n",
    "        fill_color: Color to fill removed regions\n",
    "    \n",
    "    Returns:\n",
    "        RGB image [H, W, 3] with garment region filled\n",
    "    \"\"\"\n",
    "    # Create cloth-agnostic mask\n",
    "    ca_mask = create_cloth_agnostic_mask(parsing, garment_type)\n",
    "    \n",
    "    # Apply mask to image\n",
    "    result = person_img.copy()\n",
    "    result[ca_mask == 0] = fill_color\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"‚úÖ Cloth-agnostic generation functions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed29117c",
   "metadata": {},
   "source": [
    "## 5. Pose Heatmap Generation\n",
    "\n",
    "Convert OpenPose keypoints to Gaussian heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pose_keypoints(json_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load pose keypoints from OpenPose JSON file.\n",
    "    \n",
    "    Returns:\n",
    "        Array of shape [25, 3] with (x, y, confidence)\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if 'people' in data and len(data['people']) > 0:\n",
    "        pose_keypoints = data['people'][0]['pose_keypoints_2d']\n",
    "        keypoints = np.array(pose_keypoints).reshape(-1, 3)\n",
    "        return keypoints\n",
    "    else:\n",
    "        return np.zeros((25, 3))\n",
    "\n",
    "\n",
    "def create_pose_heatmaps(keypoints: np.ndarray, \n",
    "                        image_size: Tuple[int, int],\n",
    "                        sigma: float = 3.0,\n",
    "                        threshold: float = 0.1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create Gaussian heatmaps for pose keypoints.\n",
    "    \n",
    "    Args:\n",
    "        keypoints: [25, 3] array of (x, y, confidence)\n",
    "        image_size: (height, width)\n",
    "        sigma: Gaussian kernel size\n",
    "        threshold: Confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        Heatmaps [25, H, W]\n",
    "    \"\"\"\n",
    "    h, w = image_size\n",
    "    heatmaps = np.zeros((25, h, w), dtype=np.float32)\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    y_grid, x_grid = np.ogrid[:h, :w]\n",
    "    \n",
    "    for i, (x, y, conf) in enumerate(keypoints):\n",
    "        if conf > threshold:\n",
    "            x, y = int(x), int(y)\n",
    "            if 0 <= x < w and 0 <= y < h:\n",
    "                # Gaussian heatmap\n",
    "                heatmap = np.exp(-((x_grid - x)**2 + (y_grid - y)**2) / (2 * sigma**2))\n",
    "                heatmaps[i] = heatmap * conf\n",
    "    \n",
    "    return heatmaps\n",
    "\n",
    "\n",
    "def visualize_pose_heatmap(heatmaps: np.ndarray, method: str = 'max') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Visualize pose heatmaps as single image.\n",
    "    \n",
    "    Args:\n",
    "        heatmaps: [25, H, W] or [18, H, W]\n",
    "        method: 'max' (max pooling) or 'mean' (average)\n",
    "    \n",
    "    Returns:\n",
    "        Single channel visualization [H, W]\n",
    "    \"\"\"\n",
    "    if method == 'max':\n",
    "        return np.max(heatmaps, axis=0)\n",
    "    elif method == 'mean':\n",
    "        return np.mean(heatmaps, axis=0)\n",
    "    else:\n",
    "        return np.sum(heatmaps, axis=0)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Pose heatmap generation functions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc926a",
   "metadata": {},
   "source": [
    "## 6. Visualize Single Sample - Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51badb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample\n",
    "sample_idx = 0\n",
    "sample_row = train_catalog.iloc[sample_idx]\n",
    "\n",
    "# Load all data\n",
    "person_img = np.array(Image.open(sample_row['person_image']))\n",
    "cloth_img = np.array(Image.open(sample_row['cloth_image']))\n",
    "parsing = np.array(Image.open(sample_row['parse_mask']))\n",
    "keypoints = load_pose_keypoints(sample_row['pose_json'])\n",
    "\n",
    "# Generate representations\n",
    "ca_mask = create_cloth_agnostic_mask(parsing, 'upper_body')\n",
    "ca_rgb = create_cloth_agnostic_rgb(person_img, parsing, 'upper_body')\n",
    "pose_heatmaps = create_pose_heatmaps(keypoints, person_img.shape[:2])\n",
    "pose_viz = visualize_pose_heatmap(pose_heatmaps, 'max')\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Row 1: Input data\n",
    "axes[0, 0].imshow(person_img)\n",
    "axes[0, 0].set_title('Original Person', fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(cloth_img)\n",
    "axes[0, 1].set_title('Target Garment', fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Color-coded parsing\n",
    "colored_parsing = np.zeros((*parsing.shape, 3), dtype=np.uint8)\n",
    "colored_parsing[parsing == 2] = [255, 0, 0]      # Hair - Red\n",
    "colored_parsing[parsing == 13] = [0, 0, 255]     # Face - Blue\n",
    "colored_parsing[parsing == 5] = [255, 165, 0]    # Upper-clothes - Orange\n",
    "colored_parsing[parsing == 14] = [0, 255, 255]   # Left-arm - Cyan\n",
    "colored_parsing[parsing == 15] = [0, 255, 255]   # Right-arm - Cyan\n",
    "colored_parsing[parsing == 9] = [0, 255, 0]      # Pants - Green\n",
    "\n",
    "axes[0, 2].imshow(colored_parsing)\n",
    "axes[0, 2].set_title('Parsing Mask', fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[0, 3].imshow(pose_viz, cmap='hot')\n",
    "axes[0, 3].set_title('Pose Heatmap', fontweight='bold')\n",
    "axes[0, 3].axis('off')\n",
    "\n",
    "# Row 2: Generated representations\n",
    "axes[1, 0].imshow(ca_mask, cmap='gray')\n",
    "axes[1, 0].set_title('Cloth-Agnostic Mask\\n(Remove Upper-clothes)', fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(ca_rgb)\n",
    "axes[1, 1].set_title('Cloth-Agnostic RGB\\n(Gray Fill)', fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Overlay pose on cloth-agnostic\n",
    "overlay = ca_rgb.copy()\n",
    "pose_colored = (pose_viz[:, :, np.newaxis] * np.array([255, 0, 0])).astype(np.uint8)\n",
    "overlay = np.clip(overlay.astype(float) + pose_colored.astype(float) * 0.5, 0, 255).astype(np.uint8)\n",
    "axes[1, 2].imshow(overlay)\n",
    "axes[1, 2].set_title('CA + Pose Overlay', fontweight='bold')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "# Show body parts mask\n",
    "body_mask = get_body_parts_mask(parsing, ['face', 'hair', 'arms'])\n",
    "axes[1, 3].imshow(body_mask, cmap='gray')\n",
    "axes[1, 3].set_title('Body Parts Mask\\n(Face+Hair+Arms)', fontweight='bold')\n",
    "axes[1, 3].axis('off')\n",
    "\n",
    "plt.suptitle(f'Cloth-Agnostic Representation Pipeline - Sample {sample_row[\"id\"]}', \n",
    "             fontsize=14, fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'cloth_agnostic_pipeline.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Pipeline visualization saved\")\n",
    "print(f\"\\nüìä Data shapes:\")\n",
    "print(f\"   Person image: {person_img.shape}\")\n",
    "print(f\"   Parsing mask: {parsing.shape}\")\n",
    "print(f\"   Pose heatmaps: {pose_heatmaps.shape}\")\n",
    "print(f\"   CA mask: {ca_mask.shape}\")\n",
    "print(f\"   CA RGB: {ca_rgb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93343c41",
   "metadata": {},
   "source": [
    "## 7. Generate Multiple Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c020dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 6 samples\n",
    "fig, axes = plt.subplots(3, 6, figsize=(18, 9))\n",
    "\n",
    "for i in range(6):\n",
    "    sample_row = train_catalog.iloc[i * 20]\n",
    "    \n",
    "    # Load data\n",
    "    person_img = np.array(Image.open(sample_row['person_image']))\n",
    "    parsing = np.array(Image.open(sample_row['parse_mask']))\n",
    "    keypoints = load_pose_keypoints(sample_row['pose_json'])\n",
    "    \n",
    "    # Generate representations\n",
    "    ca_rgb = create_cloth_agnostic_rgb(person_img, parsing, 'upper_body')\n",
    "    pose_heatmaps = create_pose_heatmaps(keypoints, person_img.shape[:2])\n",
    "    pose_viz = visualize_pose_heatmap(pose_heatmaps, 'max')\n",
    "    \n",
    "    # Plot\n",
    "    axes[0, i].imshow(person_img)\n",
    "    axes[0, i].set_title(f'{sample_row[\"id\"]}', fontsize=9)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(ca_rgb)\n",
    "    axes[1, i].set_title('Cloth-Agnostic', fontsize=9)\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    axes[2, i].imshow(pose_viz, cmap='hot')\n",
    "    axes[2, i].set_title('Pose Heatmap', fontsize=9)\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "# Add row labels\n",
    "axes[0, 0].text(-0.3, 0.5, 'Original', transform=axes[0, 0].transAxes,\n",
    "                fontsize=12, fontweight='bold', va='center', ha='right', rotation=90)\n",
    "axes[1, 0].text(-0.3, 0.5, 'Cloth-Agnostic', transform=axes[1, 0].transAxes,\n",
    "                fontsize=12, fontweight='bold', va='center', ha='right', rotation=90)\n",
    "axes[2, 0].text(-0.3, 0.5, 'Pose', transform=axes[2, 0].transAxes,\n",
    "                fontsize=12, fontweight='bold', va='center', ha='right', rotation=90)\n",
    "\n",
    "plt.suptitle('Cloth-Agnostic Representation - Multiple Samples', \n",
    "             fontsize=14, fontweight='bold', y=0.99)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'cloth_agnostic_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Multiple samples visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7468df",
   "metadata": {},
   "source": [
    "## 8. Multi-Channel Input Generation\n",
    "\n",
    "Combine all representations into final model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85d268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiChannelInputGenerator:\n",
    "    \"\"\"\n",
    "    Generate multi-channel input for virtual try-on model.\n",
    "    \n",
    "    Input format:\n",
    "    - Cloth-agnostic RGB: 3 channels\n",
    "    - Pose heatmaps: 18 channels (Body25 reduced to 18)\n",
    "    - Parsing one-hot: 20 channels\n",
    "    Total: 41 channels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_size: Tuple[int, int] = (1024, 768)):\n",
    "        self.image_size = image_size  # (H, W)\n",
    "        self.num_parsing_classes = 20\n",
    "        self.num_pose_channels = 18  # OpenPose BODY_18 format\n",
    "        \n",
    "        # Mapping from Body25 (25 keypoints) to Body18 (18 keypoints)\n",
    "        # We'll use the first 18 keypoints and skip foot details\n",
    "        self.body25_to_18 = list(range(18))  # Use keypoints 0-17\n",
    "    \n",
    "    def parsing_to_onehot(self, parsing: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert parsing mask to one-hot encoding.\n",
    "        \n",
    "        Args:\n",
    "            parsing: [H, W] with class indices\n",
    "        \n",
    "        Returns:\n",
    "            One-hot array [20, H, W]\n",
    "        \"\"\"\n",
    "        h, w = parsing.shape\n",
    "        onehot = np.zeros((self.num_parsing_classes, h, w), dtype=np.float32)\n",
    "        \n",
    "        for i in range(self.num_parsing_classes):\n",
    "            onehot[i][parsing == i] = 1.0\n",
    "        \n",
    "        return onehot\n",
    "    \n",
    "    def reduce_pose_to_18_channels(self, pose_25: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Reduce 25-channel pose to 18-channel (skip foot keypoints).\n",
    "        \n",
    "        Args:\n",
    "            pose_25: [25, H, W]\n",
    "        \n",
    "        Returns:\n",
    "            Reduced pose [18, H, W]\n",
    "        \"\"\"\n",
    "        return pose_25[:18]  # Take first 18 channels\n",
    "    \n",
    "    def generate_input(self, \n",
    "                      person_img: np.ndarray,\n",
    "                      parsing: np.ndarray,\n",
    "                      keypoints: np.ndarray,\n",
    "                      garment_type: str = 'upper_body',\n",
    "                      normalize: bool = True) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Generate all input representations.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with all representations\n",
    "        \"\"\"\n",
    "        # 1. Cloth-agnostic RGB (3 channels)\n",
    "        ca_rgb = create_cloth_agnostic_rgb(person_img, parsing, garment_type)\n",
    "        if normalize:\n",
    "            ca_rgb = (ca_rgb.astype(np.float32) / 255.0 - 0.5) / 0.5  # Normalize to [-1, 1]\n",
    "        ca_rgb = np.transpose(ca_rgb, (2, 0, 1))  # [3, H, W]\n",
    "        \n",
    "        # 2. Pose heatmaps (18 channels)\n",
    "        pose_25 = create_pose_heatmaps(keypoints, self.image_size)\n",
    "        pose_18 = self.reduce_pose_to_18_channels(pose_25)  # [18, H, W]\n",
    "        \n",
    "        # 3. Parsing one-hot (20 channels)\n",
    "        parsing_onehot = self.parsing_to_onehot(parsing)  # [20, H, W]\n",
    "        \n",
    "        # 4. Cloth-agnostic mask (1 channel)\n",
    "        ca_mask = create_cloth_agnostic_mask(parsing, garment_type)\n",
    "        ca_mask = ca_mask[np.newaxis, :, :]  # [1, H, W]\n",
    "        \n",
    "        # 5. Combined input (41 channels: 3 + 18 + 20)\n",
    "        combined = np.concatenate([ca_rgb, pose_18, parsing_onehot], axis=0)\n",
    "        \n",
    "        return {\n",
    "            'cloth_agnostic_rgb': ca_rgb,        # [3, H, W]\n",
    "            'pose_heatmaps': pose_18,             # [18, H, W]\n",
    "            'parsing_onehot': parsing_onehot,     # [20, H, W]\n",
    "            'cloth_agnostic_mask': ca_mask,       # [1, H, W]\n",
    "            'combined_input': combined             # [41, H, W]\n",
    "        }\n",
    "    \n",
    "    def to_tensor(self, data: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert numpy array to PyTorch tensor.\n",
    "        \"\"\"\n",
    "        return torch.from_numpy(data).float()\n",
    "\n",
    "\n",
    "# Test the generator\n",
    "generator = MultiChannelInputGenerator(image_size=(1024, 768))\n",
    "\n",
    "sample_row = train_catalog.iloc[0]\n",
    "person_img = np.array(Image.open(sample_row['person_image']))\n",
    "parsing = np.array(Image.open(sample_row['parse_mask']))\n",
    "keypoints = load_pose_keypoints(sample_row['pose_json'])\n",
    "\n",
    "inputs = generator.generate_input(person_img, parsing, keypoints)\n",
    "\n",
    "print(\"‚úÖ Multi-channel input generator created and tested\")\n",
    "print(f\"\\nüìä Generated representations:\")\n",
    "for key, value in inputs.items():\n",
    "    print(f\"   {key:25s}: {value.shape}\")\n",
    "\n",
    "print(f\"\\nüí° Total input channels: {inputs['combined_input'].shape[0]}\")\n",
    "print(f\"   - Cloth-agnostic RGB: 3\")\n",
    "print(f\"   - Pose heatmaps: 18\")\n",
    "print(f\"   - Parsing one-hot: 20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccb1809",
   "metadata": {},
   "source": [
    "## 9. Visualize Multi-Channel Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc40c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different channels\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = fig.add_gridspec(3, 6, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Row 1: RGB channels\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ca_rgb_vis = np.transpose(inputs['cloth_agnostic_rgb'], (1, 2, 0))  # [H, W, 3]\n",
    "ca_rgb_vis = ((ca_rgb_vis * 0.5 + 0.5) * 255).astype(np.uint8)  # Denormalize\n",
    "ax1.imshow(ca_rgb_vis)\n",
    "ax1.set_title('Cloth-Agnostic RGB', fontweight='bold')\n",
    "ax1.axis('off')\n",
    "\n",
    "for i in range(3):\n",
    "    ax = fig.add_subplot(gs[0, i+1])\n",
    "    ax.imshow(inputs['cloth_agnostic_rgb'][i], cmap='gray')\n",
    "    ax.set_title(f'RGB Channel {i}', fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Show CA mask\n",
    "ax = fig.add_subplot(gs[0, 4])\n",
    "ax.imshow(inputs['cloth_agnostic_mask'][0], cmap='gray')\n",
    "ax.set_title('CA Mask', fontsize=10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Original for reference\n",
    "ax = fig.add_subplot(gs[0, 5])\n",
    "ax.imshow(person_img)\n",
    "ax.set_title('Original', fontsize=10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Row 2: Some pose channels\n",
    "pose_channels_to_show = [0, 1, 2, 3, 4, 5]  # Nose, Neck, Shoulders, Elbows\n",
    "for i, ch_idx in enumerate(pose_channels_to_show):\n",
    "    ax = fig.add_subplot(gs[1, i])\n",
    "    ax.imshow(inputs['pose_heatmaps'][ch_idx], cmap='hot')\n",
    "    keypoint_name = BODY25_KEYPOINTS.get(ch_idx, f'KP-{ch_idx}')\n",
    "    ax.set_title(f'Pose: {keypoint_name}', fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Row 3: Some parsing channels\n",
    "parsing_channels = [2, 13, 5, 14, 15, 9]  # Hair, Face, Upper-clothes, Arms, Pants\n",
    "for i, ch_idx in enumerate(parsing_channels):\n",
    "    ax = fig.add_subplot(gs[2, i])\n",
    "    ax.imshow(inputs['parsing_onehot'][ch_idx], cmap='gray')\n",
    "    class_name = LIP_CLASSES.get(ch_idx, f'Class-{ch_idx}')\n",
    "    ax.set_title(f'Parse: {class_name}', fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Multi-Channel Input Visualization (41 total channels)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.savefig(output_dir / 'multi_channel_input.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Multi-channel visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4194bb",
   "metadata": {},
   "source": [
    "## 10. Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9511327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_cloth_agnostic_quality(ca_mask: np.ndarray, parsing: np.ndarray) -> Dict:\n",
    "    \"\"\"\n",
    "    Assess quality of cloth-agnostic generation.\n",
    "    \"\"\"\n",
    "    quality = {\n",
    "        'removed_pixels': int((ca_mask == 0).sum()),\n",
    "        'removed_percentage': float((ca_mask == 0).sum() / ca_mask.size * 100),\n",
    "        'kept_pixels': int((ca_mask == 1).sum()),\n",
    "        'kept_percentage': float((ca_mask == 1).sum() / ca_mask.size * 100),\n",
    "        'has_face': bool((parsing == 13).any()),\n",
    "        'has_arms': bool((parsing == 14).any() or (parsing == 15).any()),\n",
    "        'status': 'good'\n",
    "    }\n",
    "    \n",
    "    # Check if reasonable amount removed (5-30%)\n",
    "    if quality['removed_percentage'] < 5 or quality['removed_percentage'] > 50:\n",
    "        quality['status'] = 'warning'\n",
    "    \n",
    "    return quality\n",
    "\n",
    "\n",
    "# Assess 200 samples\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ CLOTH-AGNOSTIC QUALITY ASSESSMENT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_assess = 200\n",
    "samples = train_catalog.iloc[:n_assess]\n",
    "\n",
    "stats = {\n",
    "    'removed_percentages': [],\n",
    "    'has_face': 0,\n",
    "    'has_arms': 0,\n",
    "    'good': 0,\n",
    "    'warning': 0\n",
    "}\n",
    "\n",
    "print(f\"üîç Assessing {n_assess} cloth-agnostic generations...\\n\")\n",
    "\n",
    "for idx, row in tqdm(samples.iterrows(), total=len(samples), desc=\"Quality check\"):\n",
    "    parsing = np.array(Image.open(row['parse_mask']))\n",
    "    ca_mask = create_cloth_agnostic_mask(parsing, 'upper_body')\n",
    "    \n",
    "    quality = assess_cloth_agnostic_quality(ca_mask, parsing)\n",
    "    \n",
    "    stats['removed_percentages'].append(quality['removed_percentage'])\n",
    "    if quality['has_face']:\n",
    "        stats['has_face'] += 1\n",
    "    if quality['has_arms']:\n",
    "        stats['has_arms'] += 1\n",
    "    stats[quality['status']] += 1\n",
    "\n",
    "# Calculate statistics\n",
    "removed_arr = np.array(stats['removed_percentages'])\n",
    "\n",
    "print(f\"\\nüìä Quality Report:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"‚úÖ Good generations        : {stats['good']:4d} ({stats['good']/n_assess*100:5.1f}%)\")\n",
    "print(f\"‚ö†Ô∏è Warnings                : {stats['warning']:4d} ({stats['warning']/n_assess*100:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä Removed Region Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"   Mean removed: {removed_arr.mean():.1f}%\")\n",
    "print(f\"   Std removed: {removed_arr.std():.1f}%\")\n",
    "print(f\"   Min removed: {removed_arr.min():.1f}%\")\n",
    "print(f\"   Max removed: {removed_arr.max():.1f}%\")\n",
    "\n",
    "print(f\"\\nüìä Body Parts Preserved:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"‚úÖ Face preserved          : {stats['has_face']:4d} ({stats['has_face']/n_assess*100:5.1f}%)\")\n",
    "print(f\"‚úÖ Arms preserved          : {stats['has_arms']:4d} ({stats['has_arms']/n_assess*100:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "if stats['good'] / n_assess > 0.9:\n",
    "    print(\"\\n‚úÖ Cloth-agnostic generation quality is excellent!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some samples may need manual review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc49231",
   "metadata": {},
   "source": [
    "## 11. Save Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cloth-agnostic configuration\n",
    "ca_config = {\n",
    "    'image_size': list(generator.image_size),\n",
    "    'input_channels': {\n",
    "        'cloth_agnostic_rgb': 3,\n",
    "        'pose_heatmaps': 18,\n",
    "        'parsing_onehot': 20,\n",
    "        'total': 41\n",
    "    },\n",
    "    'garment_types': list(GARMENT_CLASSES.keys()),\n",
    "    'body_parts': list(BODY_PARTS.keys()),\n",
    "    'fill_color': [128, 128, 128],\n",
    "    'pose_sigma': 3.0,\n",
    "    'pose_threshold': 0.1,\n",
    "    'normalization': {\n",
    "        'rgb_mean': 0.5,\n",
    "        'rgb_std': 0.5,\n",
    "        'range': [-1, 1]\n",
    "    },\n",
    "    'quality_stats': {\n",
    "        'samples_assessed': n_assess,\n",
    "        'good_percentage': stats['good'] / n_assess * 100,\n",
    "        'mean_removed': float(removed_arr.mean()),\n",
    "        'std_removed': float(removed_arr.std())\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = output_dir / 'cloth_agnostic_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(ca_config, f, indent=2)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üíæ CLOTH-AGNOSTIC CONFIGURATION SAVED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìÑ Config saved to: {config_path}\")\n",
    "\n",
    "print(f\"\\nüìä Configuration Summary:\")\n",
    "print(f\"   - Image size: {ca_config['image_size']}\")\n",
    "print(f\"   - Total input channels: {ca_config['input_channels']['total']}\")\n",
    "print(f\"   - Garment types: {len(ca_config['garment_types'])}\")\n",
    "print(f\"   - Quality: {ca_config['quality_stats']['good_percentage']:.1f}% good\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d34525",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54127dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üéâ CLOTH-AGNOSTIC REPRESENTATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ Completed Tasks:\")\n",
    "print(\"   1. ‚úì Created cloth-agnostic mask generation\")\n",
    "print(\"   2. ‚úì Implemented RGB cloth-agnostic representation\")\n",
    "print(\"   3. ‚úì Generated pose heatmaps (18 channels)\")\n",
    "print(\"   4. ‚úì Created parsing one-hot encoding (20 channels)\")\n",
    "print(\"   5. ‚úì Built multi-channel input generator (41 channels total)\")\n",
    "print(\"   6. ‚úì Assessed generation quality\")\n",
    "print(\"   7. ‚úì Saved configuration\")\n",
    "\n",
    "print(f\"\\nüìä Key Findings:\")\n",
    "print(f\"   - Input channels: 3 (RGB) + 18 (Pose) + 20 (Parsing) = 41\")\n",
    "print(f\"   - Good quality: {stats['good']}/{n_assess} ({stats['good']/n_assess*100:.1f}%)\")\n",
    "print(f\"   - Mean garment removal: {removed_arr.mean():.1f}%\")\n",
    "print(f\"   - Face preserved: {stats['has_face']/n_assess*100:.1f}%\")\n",
    "print(f\"   - Arms preserved: {stats['has_arms']/n_assess*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "print(f\"   - cloth_agnostic_pipeline.png\")\n",
    "print(f\"   - cloth_agnostic_samples.png\")\n",
    "print(f\"   - multi_channel_input.png\")\n",
    "print(f\"   - cloth_agnostic_config.json\")\n",
    "\n",
    "print(\"\\nüöÄ Ready for Next Steps:\")\n",
    "print(\"   1. Create PyTorch Dataset class with all preprocessing\")\n",
    "print(\"   2. Build DataLoader pipeline for training\")\n",
    "print(\"   3. Begin model architecture development (Generator, Discriminator)\")\n",
    "print(\"   4. Implement training loop\")\n",
    "\n",
    "print(\"\\nüí° Key Components Created:\")\n",
    "print(\"   - create_cloth_agnostic_mask(): Remove garment from mask\")\n",
    "print(\"   - create_cloth_agnostic_rgb(): Generate RGB representation\")\n",
    "print(\"   - create_pose_heatmaps(): Gaussian heatmaps from keypoints\")\n",
    "print(\"   - MultiChannelInputGenerator: Complete preprocessing pipeline\")\n",
    "print(\"     * parsing_to_onehot()\")\n",
    "print(\"     * reduce_pose_to_18_channels()\")\n",
    "print(\"     * generate_input()\")\n",
    "print(\"     * to_tensor()\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"\\n‚úÖ CLOTH-AGNOSTIC MODULE READY!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
