{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e619c09",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7926a407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce MX450\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models import VGG19_Weights\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7867fbe",
   "metadata": {},
   "source": [
    "## 2. Setup Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "412c2c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Project Root: d:\\Projects\\AI-Virtual-TryOn\n",
      "ðŸ“ Output Directory: d:\\Projects\\AI-Virtual-TryOn\\outputs\\loss_functions\n",
      "\n",
      "âœ… Loaded model configuration\n",
      "\n",
      "ðŸ–¥ï¸ Using device: cuda\n",
      "\n",
      "ðŸ“Š Loss Configuration:\n",
      "   lambda_gan: 1.0\n",
      "   lambda_perceptual: 10.0\n",
      "   lambda_l1: 10.0\n",
      "   lambda_fm: 10.0\n",
      "   vgg_layers: ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1']\n"
     ]
    }
   ],
   "source": [
    "# Project paths\n",
    "project_root = Path(r'd:\\Projects\\AI-Virtual-TryOn')\n",
    "output_dir = project_root / 'outputs' / 'loss_functions'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load model configuration\n",
    "with open(project_root / 'outputs' / 'model_architecture' / 'model_architecture_config.json', 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "print(f\"ðŸ“ Project Root: {project_root}\")\n",
    "print(f\"ðŸ“ Output Directory: {output_dir}\")\n",
    "print(f\"\\nâœ… Loaded model configuration\")\n",
    "\n",
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "# Loss weights configuration\n",
    "loss_config = {\n",
    "    'lambda_gan': 1.0,        # GAN loss weight\n",
    "    'lambda_perceptual': 10.0, # Perceptual loss weight\n",
    "    'lambda_l1': 10.0,        # L1 reconstruction weight\n",
    "    'lambda_fm': 10.0,        # Feature matching weight\n",
    "    'vgg_layers': ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1']\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸ“Š Loss Configuration:\")\n",
    "for key, value in loss_config.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b7df2",
   "metadata": {},
   "source": [
    "## 3. VGG19 Perceptual Loss Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f3ad741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VGGPerceptualLoss defined\n",
      "   - Extracts features from VGG19\n",
      "   - Computes L1 distance across multiple layers\n",
      "   - Uses ImageNet pretrained weights\n"
     ]
    }
   ],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Perceptual loss using VGG19 features.\n",
    "    Computes L1 distance between features from multiple layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 layers: List[str] = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1'],\n",
    "                 weights: Optional[List[float]] = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained VGG19\n",
    "        vgg = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1).features\n",
    "        \n",
    "        # Freeze VGG parameters\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Layer mapping\n",
    "        self.layer_name_mapping = {\n",
    "            'relu1_1': '1',\n",
    "            'relu1_2': '3',\n",
    "            'relu2_1': '6',\n",
    "            'relu2_2': '8',\n",
    "            'relu3_1': '11',\n",
    "            'relu3_2': '13',\n",
    "            'relu3_3': '15',\n",
    "            'relu3_4': '17',\n",
    "            'relu4_1': '20',\n",
    "            'relu4_2': '22',\n",
    "            'relu4_3': '24',\n",
    "            'relu4_4': '26',\n",
    "            'relu5_1': '29',\n",
    "            'relu5_2': '31',\n",
    "            'relu5_3': '33',\n",
    "            'relu5_4': '35',\n",
    "        }\n",
    "        \n",
    "        # Build feature extractors\n",
    "        self.features = nn.ModuleDict()\n",
    "        for layer_name in layers:\n",
    "            layer_idx = int(self.layer_name_mapping[layer_name])\n",
    "            self.features[layer_name] = nn.Sequential(*[vgg[i] for i in range(layer_idx + 1)])\n",
    "        \n",
    "        # Layer weights (equal if not specified)\n",
    "        self.weights = weights if weights is not None else [1.0] * len(layers)\n",
    "        \n",
    "        # Normalization for ImageNet\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "    \n",
    "    def normalize(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize from [-1, 1] to ImageNet normalization.\n",
    "        \"\"\"\n",
    "        # Convert from [-1, 1] to [0, 1]\n",
    "        x = (x + 1) / 2\n",
    "        # Apply ImageNet normalization\n",
    "        x = (x - self.mean) / self.std\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute perceptual loss between x and y.\n",
    "        \n",
    "        Args:\n",
    "            x: Generated image [B, 3, H, W]\n",
    "            y: Target image [B, 3, H, W]\n",
    "        \n",
    "        Returns:\n",
    "            Perceptual loss (scalar)\n",
    "        \"\"\"\n",
    "        # Normalize inputs\n",
    "        x = self.normalize(x)\n",
    "        y = self.normalize(y)\n",
    "        \n",
    "        loss = 0.0\n",
    "        \n",
    "        # Extract features and compute loss\n",
    "        for (layer_name, feature_extractor), weight in zip(self.features.items(), self.weights):\n",
    "            x_feat = feature_extractor(x)\n",
    "            y_feat = feature_extractor(y)\n",
    "            \n",
    "            # L1 distance between features\n",
    "            loss += weight * F.l1_loss(x_feat, y_feat)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "print(\"âœ… VGGPerceptualLoss defined\")\n",
    "print(\"   - Extracts features from VGG19\")\n",
    "print(\"   - Computes L1 distance across multiple layers\")\n",
    "print(\"   - Uses ImageNet pretrained weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe9ca9b",
   "metadata": {},
   "source": [
    "## 4. GAN Loss (Adversarial Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4fd066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GANLoss defined\n",
      "   - Supports vanilla, LSGAN, and hinge loss\n",
      "   - Handles both real and fake predictions\n"
     ]
    }
   ],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    GAN loss (adversarial loss).\n",
    "    Supports multiple GAN objectives: vanilla, lsgan, hinge.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gan_mode: str = 'lsgan', target_real_label: float = 1.0, \n",
    "                 target_fake_label: float = 0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gan_mode: Type of GAN loss ('vanilla', 'lsgan', 'hinge')\n",
    "            target_real_label: Label for real images\n",
    "            target_fake_label: Label for fake images\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gan_mode = gan_mode\n",
    "        self.real_label = target_real_label\n",
    "        self.fake_label = target_fake_label\n",
    "        \n",
    "        if gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'hinge':\n",
    "            # Hinge loss computed manually\n",
    "            self.loss = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported GAN mode: {gan_mode}\")\n",
    "    \n",
    "    def get_target_tensor(self, prediction: torch.Tensor, target_is_real: bool) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create target tensor with same size as prediction.\n",
    "        \"\"\"\n",
    "        if target_is_real:\n",
    "            target = torch.ones_like(prediction) * self.real_label\n",
    "        else:\n",
    "            target = torch.ones_like(prediction) * self.fake_label\n",
    "        return target\n",
    "    \n",
    "    def forward(self, prediction: torch.Tensor, target_is_real: bool) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute GAN loss.\n",
    "        \n",
    "        Args:\n",
    "            prediction: Discriminator output [B, 1, H, W]\n",
    "            target_is_real: Whether target should be real (True) or fake (False)\n",
    "        \n",
    "        Returns:\n",
    "            GAN loss (scalar)\n",
    "        \"\"\"\n",
    "        if self.gan_mode == 'hinge':\n",
    "            if target_is_real:\n",
    "                # Discriminator loss for real: -min(0, -1 + D(x))\n",
    "                loss = F.relu(1.0 - prediction).mean()\n",
    "            else:\n",
    "                # Discriminator loss for fake: -min(0, -1 - D(G(z)))\n",
    "                loss = F.relu(1.0 + prediction).mean()\n",
    "        else:\n",
    "            target = self.get_target_tensor(prediction, target_is_real)\n",
    "            loss = self.loss(prediction, target)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "print(\"âœ… GANLoss defined\")\n",
    "print(\"   - Supports vanilla, LSGAN, and hinge loss\")\n",
    "print(\"   - Handles both real and fake predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b07c651",
   "metadata": {},
   "source": [
    "## 5. Feature Matching Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4a236dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FeatureMatchingLoss defined\n",
      "   - Matches discriminator intermediate features\n",
      "   - Helps stabilize GAN training\n"
     ]
    }
   ],
   "source": [
    "class FeatureMatchingLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature matching loss.\n",
    "    Matches intermediate discriminator features between real and fake images.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, fake_features: List[torch.Tensor], \n",
    "                real_features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute feature matching loss.\n",
    "        \n",
    "        Args:\n",
    "            fake_features: List of feature maps from discriminator (fake images)\n",
    "            real_features: List of feature maps from discriminator (real images)\n",
    "        \n",
    "        Returns:\n",
    "            Feature matching loss (scalar)\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "        \n",
    "        for fake_feat, real_feat in zip(fake_features, real_features):\n",
    "            loss += F.l1_loss(fake_feat, real_feat.detach())\n",
    "        \n",
    "        return loss / len(fake_features)\n",
    "\n",
    "\n",
    "print(\"âœ… FeatureMatchingLoss defined\")\n",
    "print(\"   - Matches discriminator intermediate features\")\n",
    "print(\"   - Helps stabilize GAN training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c523fc1d",
   "metadata": {},
   "source": [
    "## 6. Combined Loss Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ab262f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VITONLoss defined\n",
      "   - Combines GAN, perceptual, L1, and feature matching losses\n",
      "   - Separate methods for generator and discriminator\n",
      "   - Returns detailed loss breakdown\n"
     ]
    }
   ],
   "source": [
    "class VITONLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined loss for Virtual Try-On.\n",
    "    Combines GAN, perceptual, L1, and feature matching losses.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 lambda_gan: float = 1.0,\n",
    "                 lambda_perceptual: float = 10.0,\n",
    "                 lambda_l1: float = 10.0,\n",
    "                 lambda_fm: float = 10.0,\n",
    "                 vgg_layers: List[str] = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'relu5_1'],\n",
    "                 gan_mode: str = 'lsgan'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Loss weights\n",
    "        self.lambda_gan = lambda_gan\n",
    "        self.lambda_perceptual = lambda_perceptual\n",
    "        self.lambda_l1 = lambda_l1\n",
    "        self.lambda_fm = lambda_fm\n",
    "        \n",
    "        # Loss functions\n",
    "        self.gan_loss = GANLoss(gan_mode=gan_mode)\n",
    "        self.perceptual_loss = VGGPerceptualLoss(layers=vgg_layers)\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.fm_loss = FeatureMatchingLoss()\n",
    "    \n",
    "    def compute_generator_loss(self,\n",
    "                              fake_image: torch.Tensor,\n",
    "                              real_image: torch.Tensor,\n",
    "                              disc_fake: torch.Tensor,\n",
    "                              fake_features: Optional[List[torch.Tensor]] = None,\n",
    "                              real_features: Optional[List[torch.Tensor]] = None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute generator losses.\n",
    "        \n",
    "        Args:\n",
    "            fake_image: Generated image [B, 3, H, W]\n",
    "            real_image: Target real image [B, 3, H, W]\n",
    "            disc_fake: Discriminator output for fake image [B, 1, H', W']\n",
    "            fake_features: Discriminator features for fake (optional)\n",
    "            real_features: Discriminator features for real (optional)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with individual and total losses\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        \n",
    "        # GAN loss (fool discriminator)\n",
    "        losses['gan'] = self.gan_loss(disc_fake, target_is_real=True) * self.lambda_gan\n",
    "        \n",
    "        # Perceptual loss\n",
    "        losses['perceptual'] = self.perceptual_loss(fake_image, real_image) * self.lambda_perceptual\n",
    "        \n",
    "        # L1 reconstruction loss\n",
    "        losses['l1'] = self.l1_loss(fake_image, real_image) * self.lambda_l1\n",
    "        \n",
    "        # Feature matching loss (if features provided)\n",
    "        if fake_features is not None and real_features is not None:\n",
    "            losses['fm'] = self.fm_loss(fake_features, real_features) * self.lambda_fm\n",
    "        else:\n",
    "            losses['fm'] = torch.tensor(0.0, device=fake_image.device)\n",
    "        \n",
    "        # Total loss\n",
    "        losses['total'] = losses['gan'] + losses['perceptual'] + losses['l1'] + losses['fm']\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def compute_discriminator_loss(self,\n",
    "                                   disc_real: torch.Tensor,\n",
    "                                   disc_fake: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Compute discriminator losses.\n",
    "        \n",
    "        Args:\n",
    "            disc_real: Discriminator output for real image [B, 1, H', W']\n",
    "            disc_fake: Discriminator output for fake image [B, 1, H', W']\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with individual and total losses\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        \n",
    "        # Real loss\n",
    "        losses['real'] = self.gan_loss(disc_real, target_is_real=True)\n",
    "        \n",
    "        # Fake loss\n",
    "        losses['fake'] = self.gan_loss(disc_fake, target_is_real=False)\n",
    "        \n",
    "        # Total loss (average of real and fake)\n",
    "        losses['total'] = (losses['real'] + losses['fake']) * 0.5\n",
    "        \n",
    "        return losses\n",
    "\n",
    "\n",
    "print(\"âœ… VITONLoss defined\")\n",
    "print(\"   - Combines GAN, perceptual, L1, and feature matching losses\")\n",
    "print(\"   - Separate methods for generator and discriminator\")\n",
    "print(\"   - Returns detailed loss breakdown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec2a8c",
   "metadata": {},
   "source": [
    "## 7. Initialize Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db34e5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\nasir/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548M/548M [06:03<00:00, 1.58MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸŽ¯ LOSS FUNCTIONS INITIALIZED\n",
      "======================================================================\n",
      "\n",
      "âœ… VITONLoss created and moved to cuda\n",
      "\n",
      "ðŸ“Š Loss weights:\n",
      "   - GAN: 1.0\n",
      "   - Perceptual: 10.0\n",
      "   - L1: 10.0\n",
      "   - Feature Matching: 10.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize combined loss\n",
    "criterion = VITONLoss(\n",
    "    lambda_gan=loss_config['lambda_gan'],\n",
    "    lambda_perceptual=loss_config['lambda_perceptual'],\n",
    "    lambda_l1=loss_config['lambda_l1'],\n",
    "    lambda_fm=loss_config['lambda_fm'],\n",
    "    vgg_layers=loss_config['vgg_layers'],\n",
    "    gan_mode='lsgan'\n",
    ").to(device)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸŽ¯ LOSS FUNCTIONS INITIALIZED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nâœ… VITONLoss created and moved to {device}\")\n",
    "print(f\"\\nðŸ“Š Loss weights:\")\n",
    "print(f\"   - GAN: {loss_config['lambda_gan']}\")\n",
    "print(f\"   - Perceptual: {loss_config['lambda_perceptual']}\")\n",
    "print(f\"   - L1: {loss_config['lambda_l1']}\")\n",
    "print(f\"   - Feature Matching: {loss_config['lambda_fm']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41d95c",
   "metadata": {},
   "source": [
    "## 8. Test Loss Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "781c4818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ§ª TESTING LOSS COMPUTATIONS\n",
      "======================================================================\n",
      "\n",
      "ðŸ“¥ Input shapes:\n",
      "   Fake image: (2, 3, 1024, 768)\n",
      "   Real image: (2, 3, 1024, 768)\n",
      "   Disc fake: (2, 1, 126, 94)\n",
      "   Disc real: (2, 1, 126, 94)\n",
      "\n",
      "ðŸ”· Testing Generator Loss...\n",
      "\n",
      "ðŸ“Š Generator Losses:\n",
      "   gan            : 2.0081\n",
      "   perceptual     : 116.0492\n",
      "   l1             : 11.2854\n",
      "   fm             : 0.0000\n",
      "   total          : 129.3427\n",
      "\n",
      "ðŸ”¶ Testing Discriminator Loss...\n",
      "\n",
      "ðŸ“Š Discriminator Losses:\n",
      "   real           : 2.0030\n",
      "   fake           : 1.0033\n",
      "   total          : 1.5032\n",
      "\n",
      "======================================================================\n",
      "\n",
      "âœ… Loss computation test successful!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ§ª TESTING LOSS COMPUTATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create dummy data\n",
    "batch_size = 2\n",
    "fake_image = torch.randn(batch_size, 3, 1024, 768).to(device)\n",
    "real_image = torch.randn(batch_size, 3, 1024, 768).to(device)\n",
    "disc_fake = torch.randn(batch_size, 1, 126, 94).to(device)\n",
    "disc_real = torch.randn(batch_size, 1, 126, 94).to(device)\n",
    "\n",
    "print(f\"\\nðŸ“¥ Input shapes:\")\n",
    "print(f\"   Fake image: {tuple(fake_image.shape)}\")\n",
    "print(f\"   Real image: {tuple(real_image.shape)}\")\n",
    "print(f\"   Disc fake: {tuple(disc_fake.shape)}\")\n",
    "print(f\"   Disc real: {tuple(disc_real.shape)}\")\n",
    "\n",
    "# Test generator loss\n",
    "print(f\"\\nðŸ”· Testing Generator Loss...\")\n",
    "gen_losses = criterion.compute_generator_loss(\n",
    "    fake_image=fake_image,\n",
    "    real_image=real_image,\n",
    "    disc_fake=disc_fake\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Generator Losses:\")\n",
    "for name, loss in gen_losses.items():\n",
    "    print(f\"   {name:15s}: {loss.item():.4f}\")\n",
    "\n",
    "# Test discriminator loss\n",
    "print(f\"\\nðŸ”¶ Testing Discriminator Loss...\")\n",
    "disc_losses = criterion.compute_discriminator_loss(\n",
    "    disc_real=disc_real,\n",
    "    disc_fake=disc_fake\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š Discriminator Losses:\")\n",
    "for name, loss in disc_losses.items():\n",
    "    print(f\"   {name:15s}: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nâœ… Loss computation test successful!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ffe913",
   "metadata": {},
   "source": [
    "## 9. Test Individual Loss Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77121bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ”¬ TESTING INDIVIDUAL LOSS COMPONENTS\n",
      "======================================================================\n",
      "\n",
      "ðŸ”· GAN Loss:\n",
      "   Real: 2.0030\n",
      "   Fake: 1.0033\n",
      "\n",
      "ðŸ”· Perceptual Loss:\n",
      "   Loss: 11.6049\n",
      "\n",
      "ðŸ”· L1 Loss:\n",
      "   Loss: 1.1285\n",
      "\n",
      "ðŸ”¬ Testing with different image similarities:\n",
      "\n",
      "   Identical images:\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸ”¬ TESTING INDIVIDUAL LOSS COMPONENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test GAN loss\n",
    "print(\"\\nðŸ”· GAN Loss:\")\n",
    "gan_loss_real = criterion.gan_loss(disc_real, target_is_real=True)\n",
    "gan_loss_fake = criterion.gan_loss(disc_fake, target_is_real=False)\n",
    "print(f\"   Real: {gan_loss_real.item():.4f}\")\n",
    "print(f\"   Fake: {gan_loss_fake.item():.4f}\")\n",
    "\n",
    "# Test perceptual loss\n",
    "print(\"\\nðŸ”· Perceptual Loss:\")\n",
    "perceptual_loss = criterion.perceptual_loss(fake_image, real_image)\n",
    "print(f\"   Loss: {perceptual_loss.item():.4f}\")\n",
    "\n",
    "# Test L1 loss\n",
    "print(\"\\nðŸ”· L1 Loss:\")\n",
    "l1_loss = criterion.l1_loss(fake_image, real_image)\n",
    "print(f\"   Loss: {l1_loss.item():.4f}\")\n",
    "\n",
    "# Test with different similarity levels\n",
    "print(\"\\nðŸ”¬ Testing with different image similarities:\")\n",
    "identical = real_image.clone()\n",
    "similar = real_image + torch.randn_like(real_image) * 0.1\n",
    "different = torch.randn_like(real_image)\n",
    "\n",
    "print(\"\\n   Identical images:\")\n",
    "print(f\"      Perceptual: {criterion.perceptual_loss(identical, real_image).item():.4f}\")\n",
    "print(f\"      L1: {criterion.l1_loss(identical, real_image).item():.4f}\")\n",
    "\n",
    "print(\"\\n   Similar images (noise=0.1):\")\n",
    "print(f\"      Perceptual: {criterion.perceptual_loss(similar, real_image).item():.4f}\")\n",
    "print(f\"      L1: {criterion.l1_loss(similar, real_image).item():.4f}\")\n",
    "\n",
    "print(\"\\n   Different images (random):\")\n",
    "print(f\"      Perceptual: {criterion.perceptual_loss(different, real_image).item():.4f}\")\n",
    "print(f\"      L1: {criterion.l1_loss(different, real_image).item():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nâœ… Individual loss tests successful!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298c7584",
   "metadata": {},
   "source": [
    "## 10. Visualize Loss Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5aa676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss_comparison():\n",
    "    \"\"\"\n",
    "    Visualize how different losses respond to image similarity.\n",
    "    \"\"\"\n",
    "    # Create images with varying levels of similarity\n",
    "    base_image = torch.randn(1, 3, 256, 256).to(device)\n",
    "    noise_levels = np.linspace(0, 2, 20)\n",
    "    \n",
    "    perceptual_losses = []\n",
    "    l1_losses = []\n",
    "    \n",
    "    for noise in noise_levels:\n",
    "        noisy_image = base_image + torch.randn_like(base_image) * noise\n",
    "        noisy_image = torch.clamp(noisy_image, -1, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            perceptual = criterion.perceptual_loss(noisy_image, base_image).item()\n",
    "            l1 = criterion.l1_loss(noisy_image, base_image).item()\n",
    "        \n",
    "        perceptual_losses.append(perceptual)\n",
    "        l1_losses.append(l1)\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(noise_levels, perceptual_losses, 'b-', linewidth=2, label='Perceptual Loss')\n",
    "    ax1.plot(noise_levels, l1_losses, 'r-', linewidth=2, label='L1 Loss')\n",
    "    ax1.set_xlabel('Noise Level', fontsize=12)\n",
    "    ax1.set_ylabel('Loss Value', fontsize=12)\n",
    "    ax1.set_title('Loss vs Image Similarity', fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Normalized comparison\n",
    "    perceptual_norm = np.array(perceptual_losses) / max(perceptual_losses)\n",
    "    l1_norm = np.array(l1_losses) / max(l1_losses)\n",
    "    \n",
    "    ax2.plot(noise_levels, perceptual_norm, 'b-', linewidth=2, label='Perceptual (normalized)')\n",
    "    ax2.plot(noise_levels, l1_norm, 'r-', linewidth=2, label='L1 (normalized)')\n",
    "    ax2.set_xlabel('Noise Level', fontsize=12)\n",
    "    ax2.set_ylabel('Normalized Loss', fontsize=12)\n",
    "    ax2.set_title('Normalized Loss Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'loss_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Loss comparison visualization saved\")\n",
    "\n",
    "\n",
    "visualize_loss_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50119f",
   "metadata": {},
   "source": [
    "## 11. Loss Weights Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391e4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_loss_weights():\n",
    "    \"\"\"\n",
    "    Analyze the impact of different loss weights.\n",
    "    \"\"\"\n",
    "    # Sample data\n",
    "    fake = torch.randn(1, 3, 256, 256).to(device)\n",
    "    real = torch.randn(1, 3, 256, 256).to(device)\n",
    "    disc = torch.randn(1, 1, 31, 23).to(device)\n",
    "    \n",
    "    # Test different weight configurations\n",
    "    weight_configs = [\n",
    "        {'lambda_gan': 1, 'lambda_perceptual': 0, 'lambda_l1': 0, 'lambda_fm': 0},\n",
    "        {'lambda_gan': 0, 'lambda_perceptual': 1, 'lambda_l1': 0, 'lambda_fm': 0},\n",
    "        {'lambda_gan': 0, 'lambda_perceptual': 0, 'lambda_l1': 1, 'lambda_fm': 0},\n",
    "        {'lambda_gan': 1, 'lambda_perceptual': 10, 'lambda_l1': 10, 'lambda_fm': 10},\n",
    "    ]\n",
    "    \n",
    "    config_names = ['GAN only', 'Perceptual only', 'L1 only', 'Combined (balanced)']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for config, name in zip(weight_configs, config_names):\n",
    "        test_criterion = VITONLoss(**config, vgg_layers=loss_config['vgg_layers']).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            losses = test_criterion.compute_generator_loss(fake, real, disc)\n",
    "        \n",
    "        results.append({\n",
    "            'config': name,\n",
    "            'total': losses['total'].item(),\n",
    "            'gan': losses['gan'].item(),\n",
    "            'perceptual': losses['perceptual'].item(),\n",
    "            'l1': losses['l1'].item(),\n",
    "            'fm': losses['fm'].item()\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ“Š LOSS WEIGHTS SENSITIVITY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n{df.to_string(index=False)}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(config_names))\n",
    "    width = 0.15\n",
    "    \n",
    "    ax.bar(x - 2*width, df['gan'], width, label='GAN', color='#ff9999')\n",
    "    ax.bar(x - width, df['perceptual'], width, label='Perceptual', color='#66b3ff')\n",
    "    ax.bar(x, df['l1'], width, label='L1', color='#99ff99')\n",
    "    ax.bar(x + width, df['fm'], width, label='Feature Matching', color='#ffcc99')\n",
    "    ax.bar(x + 2*width, df['total'], width, label='Total', color='#ff99cc', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Configuration', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Loss Value', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Loss Components Across Different Weight Configurations', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(config_names, rotation=15, ha='right')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'loss_weights_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… Loss weights analysis complete\")\n",
    "\n",
    "\n",
    "analyze_loss_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c5a589",
   "metadata": {},
   "source": [
    "## 12. Save Loss Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3e0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed loss configuration\n",
    "loss_full_config = {\n",
    "    'loss_weights': {\n",
    "        'lambda_gan': loss_config['lambda_gan'],\n",
    "        'lambda_perceptual': loss_config['lambda_perceptual'],\n",
    "        'lambda_l1': loss_config['lambda_l1'],\n",
    "        'lambda_fm': loss_config['lambda_fm']\n",
    "    },\n",
    "    'gan_loss': {\n",
    "        'type': 'lsgan',\n",
    "        'target_real': 1.0,\n",
    "        'target_fake': 0.0\n",
    "    },\n",
    "    'perceptual_loss': {\n",
    "        'network': 'VGG19',\n",
    "        'layers': loss_config['vgg_layers'],\n",
    "        'pretrained': 'ImageNet'\n",
    "    },\n",
    "    'reconstruction_loss': {\n",
    "        'type': 'L1',\n",
    "        'pixel_wise': True\n",
    "    },\n",
    "    'feature_matching': {\n",
    "        'enabled': True,\n",
    "        'discriminator_features': True\n",
    "    },\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "config_path = output_dir / 'loss_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(loss_full_config, f, indent=2)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ’¾ LOSS CONFIGURATION SAVED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸ“„ Config saved to: {config_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"   - GAN weight: {loss_config['lambda_gan']}\")\n",
    "print(f\"   - Perceptual weight: {loss_config['lambda_perceptual']}\")\n",
    "print(f\"   - L1 weight: {loss_config['lambda_l1']}\")\n",
    "print(f\"   - Feature matching weight: {loss_config['lambda_fm']}\")\n",
    "print(f\"   - VGG layers: {len(loss_config['vgg_layers'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a622c0e8",
   "metadata": {},
   "source": [
    "## 13. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ðŸŽ‰ LOSS FUNCTIONS COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nâœ… Completed Tasks:\")\n",
    "print(\"   1. âœ“ Implemented VGG19 perceptual loss\")\n",
    "print(\"   2. âœ“ Implemented GAN loss (LSGAN)\")\n",
    "print(\"   3. âœ“ Implemented feature matching loss\")\n",
    "print(\"   4. âœ“ Created combined VITONLoss module\")\n",
    "print(\"   5. âœ“ Tested all loss computations\")\n",
    "print(\"   6. âœ“ Tested individual loss components\")\n",
    "print(\"   7. âœ“ Visualized loss comparisons\")\n",
    "print(\"   8. âœ“ Analyzed loss weight sensitivity\")\n",
    "print(\"   9. âœ“ Saved loss configuration\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Loss Components:\")\n",
    "print(f\"   ðŸ”· GAN Loss:\")\n",
    "print(f\"      - Type: LSGAN (Mean Squared Error)\")\n",
    "print(f\"      - Weight: {loss_config['lambda_gan']}\")\n",
    "print(f\"      - Purpose: Adversarial training for realistic generation\")\n",
    "print(f\"   ðŸ”· Perceptual Loss:\")\n",
    "print(f\"      - Network: VGG19 (ImageNet pretrained)\")\n",
    "print(f\"      - Layers: {len(loss_config['vgg_layers'])} layers\")\n",
    "print(f\"      - Weight: {loss_config['lambda_perceptual']}\")\n",
    "print(f\"      - Purpose: Feature-level similarity\")\n",
    "print(f\"   ðŸ”· L1 Loss:\")\n",
    "print(f\"      - Type: Mean Absolute Error\")\n",
    "print(f\"      - Weight: {loss_config['lambda_l1']}\")\n",
    "print(f\"      - Purpose: Pixel-wise reconstruction\")\n",
    "print(f\"   ðŸ”· Feature Matching:\")\n",
    "print(f\"      - Type: Discriminator feature matching\")\n",
    "print(f\"      - Weight: {loss_config['lambda_fm']}\")\n",
    "print(f\"      - Purpose: Training stability\")\n",
    "\n",
    "print(f\"\\nðŸ“ Generated Files:\")\n",
    "print(f\"   - loss_comparison.png\")\n",
    "print(f\"   - loss_weights_analysis.png\")\n",
    "print(f\"   - loss_config.json\")\n",
    "\n",
    "print(\"\\nðŸš€ Ready for Next Steps:\")\n",
    "print(\"   1. Implement training loop\")\n",
    "print(\"   2. Add checkpointing and logging\")\n",
    "print(\"   3. Implement learning rate scheduling\")\n",
    "print(\"   4. Start model training\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Features:\")\n",
    "print(\"   - Multi-scale perceptual loss (5 VGG layers)\")\n",
    "print(\"   - Stable GAN training with LSGAN\")\n",
    "print(\"   - Feature matching for better convergence\")\n",
    "print(\"   - Balanced loss weights for quality\")\n",
    "print(\"   - Comprehensive loss breakdown for monitoring\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "print(\"\\nâœ… LOSS FUNCTIONS READY FOR TRAINING!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
